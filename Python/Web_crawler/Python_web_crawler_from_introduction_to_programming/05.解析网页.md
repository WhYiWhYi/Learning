# 5 解析网页
## 5.1 使用正则表达式解析网页
在提取网页中的数据时，我们可以先把源代码变成字符串，然后使用正则表达式匹配想要的数据。正则表达式相关的内容请移步“python-2020.1.27-自动化任务：模式匹配与正则表达式”
### 5.1.1 re.match方法
* re.match(pattern, string, flags=0)
	* 从字符串**起始位置**匹配一个模式，如果从起始位置匹配不了，就会返回None
	* pattern：正则表达式
	* string：要匹配的字符串
	* flags：控制正则表达式的匹配方式，如是否区分大小写、多行匹配等
	```pyhon
	line = "Fat cats are smarter than dogs, is it right?"
	m = re.match(r'(.*) are (.*?) dogs', line)  # m是正则表达式的匹配对象
	print('匹配的整句话', m.group(0))
	>>>Fat cats are smarter than dogs
	print('匹配的第一个结果', m.group(1))
	>>>Fat cats
	print('匹配的第二个结果', m.group(2))
	>>>smarter than
	print('匹配的结果列表', m.groups())
	>>>('Fat cats', 'smarter than')
	```

	* r'：raw string，使用了r即代表纯粹的字符串而不会对其中的反斜杠“\”行特殊处理
	* 正则表达式中的()代表了分组，整个匹配为第一组（索引号0），第一个括号内（.\*）的匹配为第二组（索引号1），第二个括号内（.\*?）的匹配为第三组（索引号2）
	* 贪婪模式与非贪婪模式：“.\*”会尽量匹配最多的字符，因此匹配了“Fat cats”，而“.\*?”则尽量匹配尽量少的字符，因此匹配了“smarter”
### 5.1.2 re.search方法
re.match()只能从字符串起始位置进行匹配，而re.search则是扫描整个字符串并返回第一个成功的匹配，其余参数二者相同

### 5.1.3 re.findall方法
re.match()和re.search()只能找到与所写模式的一个匹配，而re.findall()可以找到所有的匹配，并将其储存在**列表**中
举个栗子
```python
# 抓取博客的所有文章标题

import requests
import re

link = "a_link"
headers = {}
r = requests.get(link, headers=headers)
html = r.text

gettitle = re.findall('<h1 class="post-title"><a href=.*?>(.*?)</a></h1>', html)
print(gettitle)
```
## 5.2 使用BeautifulSoup解析网页
### 5.2.1 BeautifulSoup解析器
* python标准库：```BeautifulSoup(markup, "html.parser")``` python的内置标准库，执行速度适中，文档容错能力强
* **lxml HTML解析器**：```BeautifulSoup(markup, "lxml")``` 速度快，文档容错能力强，需安装C语言库
* lxml XML解析器：```BeautifulSoup(markup, "xml")``` 或 ```BeautifulSoup(markup, ["lxml", "xml"])``` 速度快，文档容错能力强，需安装C语言库
* html5lib：```BeautifulSoup(markup, "html5lib")```
### 5.2.2 使用BeautifulSoup获取博客标题
```python
import re
import requests
from bs4 import BeautifulSoup

link = "http://www.santostang.com/"
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36"}
r = requests.get(link, headers=headers)  # 获取网页

soup = BeautifulSoup(r.text, "lxml")  # 使用“lxml”解析器
first_title = soup.find("h1", class_="post-title").a.text  # 找到第一个指定h1下的a的文本内容
print("第一章的标题是 " + first_title)
all_title = soup.findAll("h1", class_="post-title")  # 找到所有指定和h1下的a的文本内容，findAll返回的对象可迭代
for i in range(len(all_title)):
    title = all_title[i].a.text
    print("第%s章的标题是：%s" % (i+1, title))  # 使用双重填充
```
### 5.2.3 BeautifulSSoup的其他功能
BeautifulSoup对象是一个复杂的树形结构，每一个节点都是一个Python对象，获取网页内容就是一个提取对象内容的过程，提取方法主要分为以下三种
* 遍历文档树（使用较少）
	* 获取header中的h3标签内的所有内容：```soup.header.h3``` 输出结果包括标签+内容
	* 获取某个标签的所有子节点：```soup.header.div.contents``` 输出结果可用索引检索的
	* 获取某个节点下一级节点的所有子标签：```soup.header.div.children```
	* 获取某个节点下面所有子子孙孙的节点的标签：```soup.header.div.descendants```
	* 获取某个节点的父节点：```soup.header.div.parent``` 输出父节点所包含的所有子节点信息
* 搜索文档树
	* find()和findAll()方法
	* find()和findAll()方法还可和re正则表达式结合使用
		```python
		for tag in soup.findAll(re.compile("^h"))
		```
* CSS选择器
CSS选择器方法既可以作为遍历文档树的方法提取数据，也可以作为搜索文档树的方法提取数据
	* 遍历文档树
		* 通过tag标签逐层查找
			```python
			soup.select("header h3")
			```
		* 通过某个tag标签下直接子标签遍历
			```python
			soup.select("header > h3")
			soup.select("div > a")
			```
	* 搜索文档树
		```python
		soup.select('a[herf^="http://www.santostang.com/"]')  # 找到所有链接以“”开头的<a>标签
		```
## 5.3 使用lxml解析网页
与BeautifulSoup相比，lxml多了Xpath选择器方法
元素的XPath可通过F12选中目标内容后复制时选择“Copy Xpath”获得
```python
# 使用Xpath的方法获取播客主页所有文章标题

import requests
from lxml import etree

link = "http://www.santostang.com/"
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36"}
r = requests.get(link, headers=headers)  # 获取网页

html = etree.HTML(r.text)  # 使用lxml的etree解析为lxml格式
title_list = html.xpath('//h1[@class="post-title"]/a/text()')  # 使用Xpath读取内容
	# “//h1”：选取所有<h1>子元素
	# “[@class="post-title"]”选取<h1>中class为“post-title”的元素
	# “/a”：选取<h1>子元素的<a>元素
	# “/text()”：提取<a>元素中的所有文本
print(title_list)
```