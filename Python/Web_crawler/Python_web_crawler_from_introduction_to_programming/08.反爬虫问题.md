# 8 反爬虫问题

## 8.2 反爬虫的方式有哪些
### 8.2.1 不返回网页
爬虫发送请求给相应网站地址后，网站返回404页面，表示服务器无法正常提供信息或服务器无响应；长时间不返回数据，代表对爬虫进行了封杀
* 通过IP访问量：网站对会话访问进行统计，如果单个IP的访问量超过了某个阈值，就会进行封杀或要求输入验证码
* 通过session：session的意思是“会话控制”，session对象储存特定用户会话所需的属性和配置信息，如果一个session的访问量过大，就会进行封杀或要求输入验证码
* 通过User-Agent：User-Agent表示浏览器在发送请求时，附带当前浏览器和当前系统环境的参数给服务器。当服务器判断User-Agent不是真正的浏览器，或某个User-Agent的访问超过阈值时予以封锁
### 8.2.2 返回非目标网页
在爬虫返回网页时返回非目标网页，即返回假数据，如返回空白页或爬取多页时返回同一页
### 8.2.3 获取数据变难
通过增加获取数据的难度反爬虫，如登陆后才可查看数据，或设置验证码
## 8.3 如何“反反爬虫”
让爬虫顺利运行下去的中心思想是让爬虫程序看起来更像正常用户
### 8.3.1 修改请求头
* 将请求头改为浏览器的格式 
	```python
	headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36"}
	```
* 可以建一个User-Agent池，在其中随机切换User-Agent（但实际爬虫中针对User-Agent访问量进行封锁的网站较少，所以一般用不到）
	```python
	class HtmlDownloader(object):
    	def __init__(self):
        	self.url_manager = UrlManager()
        	# 使用random.choice随机选择列表中的一个headers作为请求头
        	USER_AGENT = random.choice([
  "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1", "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
  "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
  "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6", "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1"
  ])
        	self.headers = {'User-Agent':USER_AGENT}
		
		def donwloader(self, url):
			response = requests.get(url,headers=self.headers)
			response.encoding = "utf-8"
			if response.status_code in [int("20" + str(x)) for x in range(10)]:
				return response
	```
* 除了User-Agent，还可在headers中写入Host和Referer
### 8.3.2 修改爬虫的间隔时间
在两次爬虫之间添加一定的时间间隔
* 使用 ```time.sleep()``` 在爬虫访问之间设置一定的间隔时间
* 使用randomm库将间隔时间进行随机设置
	```python
	import time
	import random
	
	# random.randint(0,2)的结果是0，1，2，而random.random的结果是一个0~1之间的随机数
	sleep_time = random.randint(0,2) + random.random()
	time.sleep(sleep_time)
	```
* 每次爬取都间隔0~3s也不太像真实访问，因此可以设置在爬取一定页数后休息更长时间，如爬取5次数据休息10秒
	```python
	scrap_times = 0
	for eachone in title_list:
		url = eachone.a['herf']
		soup_article = scrap(url)
		title = soup_article.find("h1", class_="view-title").text.strip()
		# 在for循环遍历中加入休息时间的判定
		scrap_times += 1
		if scrap_times % 5 == 0:  # 如果时间能被5整除
			sleep_time = 10 + random.random()
		else:
			sleep_time = random.randint(0,2) + random.random()
		time.sleep(sleep_time)
	```
### 8.3.3 使用代理
可以维护一个代理IP池，从而让爬虫程序隐藏自己的真实IP。网上有很多代理IP，但是良莠不齐，可以通过筛选找到能用的，但是代理IP池维护起来很麻烦，且十分不稳定
```python
link = "somelink.com"
proxies = ("http": "http://xxx.xxx.xxx.xxx:xxxx")
response = requests.get(link, proxies=proxies)
```
不推荐使用代理IP的方法，一方面网络上免费的代理IP都很不稳定，可能一两分钟就失效了，另一方面，通过代理IP的服务器请求爬取速度很慢