# 1.初见网络爬虫

## 1.1 网络连接
* 网络连接过程
	* A的电脑将packet【请求头（A的路由器MAC地址，B的IP地址）+消息体（A对B服务器的请求）】发送至A的路由器
	* A的路由器将packet从A的MAC地址由A的IP地址寄到B的IP地址，期间经历了一些中介服务器
	* B的服务器在B的IP地址收到了packet，由服务器解读请求头中的目标端口，然后将其传输到对应的网络服务器应用上
	* 网络服务器应用从服务器接收到了信息【这是一个GET请求，请求文件index.html】
	* 网络服务器应用找到对应的HTML文件，将其打包为packet1以同样的过程发送给A
* python实现
	```python
	from urllib.request import urlopen  # 打开url的函数
	html = urlopen("http://pythonscraping.com/pages/page1.html")  # 将url中html的内容储存到变量中
	print(html.read())  # 将url中的html内容打印出来
	```

## 1.2 BeautifulSoup
### 1.2.2 运行BeautifulSoup
```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen("http://pythonscraping.com/pages/page1.html")
bso = BeautifulSoup(html)  # 使用url中的所有html内容创建一个BeautifulSoup对象
print(bso.h1)  # 打印bso对象中的h1标签中的内容
```
* BS对象也可被直接调用
	```python
	bso.html.body.h1 = bso.body.h1 = bso.html.h1
	```
### 1.2.3 创建可靠的网络连接
```python
from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup

def getTitle(url)  # 定义函数用于检测链接的有效性；try可以写入函数中用于检查
	try:  # 先检查链接是否存在，若不存在则返回None，存在则进行下一步
		html = urlopen(url)
	except HTTPError as e:
		return None
	try:  # 检查要访问的子标签（h1）是否存在，如果不存在则返回None，存在则进行下一步（return title）
		bso = BeautifulSoup(html)
		title = bso.body.h1
	except AttributeError as e:
		return None
	return title

title = getTitle("http://www.pythonscraping.com/pages/page1.html")
if title == None:
	print("Title could not be found!")
else:
	print(title)
```
### 1.2.4 其他
在写爬虫的时候多半需要考虑类似getSiteHTML、getTitle这样的函数（具有周密的异常处理功能），可让数据采集过程稳定



# 2.复杂HTML解析
## 2.2 BeautifulSoup
网站多半具有层叠样式表（Cascading Style Sheet, CSS），其可以让HTML元素呈现差异化，使具有完全相同修饰的元素呈现出不同的样式
```
<span class="green"></span>
span：span标签
class：span标签下的class属性
"green"：class属性内的“green”标签
/span：代表标签的结束
```
### 2.2.1 find()和findAll()
```
findAll(tag, attributes, recursive, text, limit, keywords)  # findAll和find函数的返回值标签+内容
find(tag, attributes, recursive, text, keywords)

tag：标签。可以是标签的字符串或一个包含标签名的列表
	.findAll({'h1', 'h2', 'h3'})
attributes：属性。使用属性匹配属性内标签。一个由字典封装的标签的若干属性和对应属性值
	.findAll("span", {"class": {"green", "red"}})
		span：标签
		class：span标签下的class属性，属性和属性值封装在一个字典中
		green/red：class属性下的属性值，当一个属性有多个属性值时，所有属性值封装在字典中；注意属性下的属性值一定要完整，不可用".jpg"匹配图片，必须使用模式
recursive：布尔值。True（默认值）：findAll会依据要求查找标签参数的所有子标签，以及子标签的子标签；False：findAll只查找一级标签
text：使用属性内标签的文本内容匹配标签
	.findAll(text = "the prince")
limit：范围限制参数。只输出网页上按顺序排列的n项
keyword：关键词参数。选择那些具有指定属性的标签，即无视标签，依据属性匹配属性内标签
	bso.findAll(id="text") = bso.findAll("", {"id": "text"})

find和findAll函数可以连续使用，以找到特定子标题下的内容
```
```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen("http://www.pythonscraping.com/pages/warandpeace.html")
bso = BeautifulSoup(html)
namelist = bso.findALL("span", {"class": "green"})
for name in namelist:
	print(name.get_text())
```
### 2.2.2 BeautifulSoup中的对象
* BeautifulSoup对象
	由BeautifulSoup()返回的对象，即经过格式化和组织过的html文档对象（XML结构信息）。
* Tag对象
	由BeautifulSoup对象使用findAll或find函数返回的对象，即符合条件的html文档标签+内容
* NavigableString对象
	表示标签里的文字而非标签
* Comment对象
	用于查找HTML文档的注释标签

### 2.2.3 导航树

* 子标签和后代标签
	* 子标签：一个父标签的下一级标签；children()
		```python
		.find("table",{"id":"giftList"}).children  # 仅打印giftList下一级的标签
		```
	* 后代标签：一个父标签下面所有级别的标签；descendants()
		```python
		.find("table",{"id":"giftList"}).descendants  # 打印giftList下所有的标签
		```
* 兄弟标签
	* 兄弟标签：除对象标签自己以外、和对象标签处在同一个父标签下的同级标签；next_siblings()、previous_siblings()
		```python
		.find("table",{"id":"giftList"}).tr.next_siblings:  # 在table标签下与tr标签同级的、位于tr标签后的所有标签
		```
	* next_siblings()：对象标签以后的同级标签
	* previous_siblings()：对象标签以前的同级标签
* 父标签：对象标签的上一级标签；parent()、parents()
## 2.3 正则表达式
Regex：如果你给我的字符串符合规则，我就返回它
* *：匹配前面的字符、子表达式或括号里的字符0次或多次
* +：匹配前面的字符、子表达式或括号里的字符1次或多次
* []：匹配任意自创字符，如[A-Za-z0-9]
* ()：表达式编组，正则表达式中，编组优先运行
* {m,n}：匹配前面的字符、子表达式或括号里的字符m到n次（m和n均包含）
* [^]：匹配任意一个不在中括号内的字符，即排除中括号内的字符，如\[^A-Z]
* |:匹配任意一个由竖线分割的字符、子表达式
* .：匹配任意单个字符（包括符号、数字和空格等）
* ^：指字符串开始位置的字符或子表达式，如^a
* $：指字符串末端位置的字符或子表达式，如a$，如果不用它，每个正则表达式实际都带着“.*”模式且只从字符串开头进行匹配
* \：转义字符，把有特殊含义的字符转换成字面形式
* ?!：“不包含”，表示字符不能出现在目标字符串里
## 2.4 获取属性
对于一个标签对象，可以使用以下代码获取它的全部属性
```python
myTag.attrs  # 返回一个python字典，可以获取和操作这些属性
```
比如要获取图片的资源位置src，即指定要获取的属性名，获取该属性的属性值
```python
myImgTag.attrs["src"]
```
举个栗子
```python
html = urlopen("https://baike.baidu.com/item/%E5%87%AF%E6%96%87%C2%B7%E8%B4%9D%E8%82%AF/2728128")
bso = BeautifulSoup(html)  # 将百度百科的界面创建为BS对象
for link in bso.findAll("a", href=re.compile("^(/item/)(.*)$")):
	# print(link)
	# ("a", href=re.compile("^(/item/)(.*)$"))指的是：在"a"标签下，满足href属性内属性值为该正则表达式的所有“标签A+标签A下的内容”。在使用正则时，属性不用引号括起来
	# bso.findAll("a", href=re.compile("^(/item/)(.*)$"))即为bso中的上述所有“标签A+标签A下的内容”
	# link为遍历这些内容时的循环变量
	if 'href' in link.attrs:  # 如果“href”属性是标签A的下属属性之一
		print(link.attrs['href'])  # 则输出该标签A中href属性内的属性值
```



# 3.开始采集
## 3.1 遍历单个域名
```python
# 从一个网站上随机地从一个链接跳到另一个链接
from urllib.request import urlopen
from bs4 import BeautifulSoup
import datetime
import random
import re

random.seed(datetime.datetime.now())  # 以系统时间作为随机数序列生成的起点，可使程序运行时间更具随机性
def getLinks(articleUrl):
	html = urlopen("http://en.wikipedia.org" + articleUrl)  # 链接可以为固定字符串+输入字符的组合形式（从不同页面爬取）
	bso = BeautifulSoup(html)
	return bso.find("div", {"id": "bodyContent"}).findAll("a". herf=re.compile("^(/wiki/)((?!:).)*$"))
	# 返回bso中在<div id="bodyContent">标签下所有符合<a herf=re.compile>的“标签+标签下的内容”，在此处为wiki中该词条下进入下一个

links = getLink("/wiki/Kevin_Bacon")  # links是个列表
while len(links) > 0:  # 当词条通向别的词条的链接数大于0时（即可继续从上一步生成的词条进入下一词条）继续循环
	newArticle = links[random.randint(0, len(links)-1)].attrs["herf"]  # 从links中随机选出下一个链接
	print(newArticle)
	links = getLinks(newArticle)  # 重复以上步骤
```
## 3.2 采集整个网站
需要采集整个网站的情况
* 生成网站地图
* 全面、深度收集数据
常用的网站采集方法一般从顶级页面开始（即主页），然后搜索页面上所有的链接，形成列表。再去采集这些链接的每一个页面，然后把在每个页面上找到的链接形成新的列表，重复执行下一轮采集。但由于很多链接都是重复的内链，因此链接去重十分重要
```python
# 建立一个爬虫和数据收集（打印）的组合程序
from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

pages = set()  #创建一个空的集合，用于收集已经爬过的界面链接，用于链接去重
def getLinks(pageUrl):
	globle pages  # 将pages设为全局变量
	html = urlopen("http://en.wikipedia.org" + pageUrl)  # 使用字符串+输入字符的形式创建链接
	bso = BeautifulSoup(html)
	try:  # try部分用于获取信息
		# 不可能确保每一页上有所有类型的数据，所以每个打印语句都是按照数据在页面上出现的可能性从高到低排列的
		print(bso.h1.get_text())  
		print(bso.find(id="mw-content-text").findAll("p")[0])
		print(bso.find(id="ca-edit").find("span").attrs['herf'])
	except AttributeError:
		print("页面缺少一些属性，但不用担心")
	
	for link in bso.findAll("a", herf=re.compile("^(/wiki/)")):
		if 'herf' in link.attrs:
			if link.attrs['herf'] not in pages:  # 对于所有符合正则要求的链接，检测其是否已在集合中
				newPage = link.attrs['herf']  # 提取link中'herf'属性的属性值
				print("-----------------------\n" + newPage)
				pages.add(newPage)  # 将新界面加入到去重集合中
				getLinks(newPage)  # 进入新界面获取信息

getLinks("")
```
## 3.3 通过互联网采集
在不同网站上的数据采集要难得多，因为不同网站的布局迥然不同
* 我要收集哪些数据？这些数据可以通过采集几个已经确定的网站（永远是最简单的做法）完成吗？或者我的爬虫需要发现那些我可能不知道的网站吗？
* 当我的爬虫到了某个网站，它是立即顺着下一个出站链接跳到一个新网站，还是在网站上呆一会儿，深入采集网站的内容？
* 有没有我不想采集的一类网站？我对非英文网站的内容感兴趣吗？
* 如果我的网络爬虫引起了某个网站网管的怀疑，我如何避免法律责任？（关于这个问题的更多信息请参考附录C）
举个栗子
```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
import re
import datetime
import random

pages = set()
random.seed(datetime.datetime.now())

# 获取页面所有内链的列表
def getInternalLinks(bsObj, includeUrl):
	internalLinks = []
	# 找出所有以"/"开头的链接，即内链
	for link in bsObj.findAll("a", href=re.compile("^(/|.*"+includeUrl+")")): 
		if link.attrs['href'] is not None:
			if link.attrs['href'] not in internalLinks:
				internalLinks.append(link.attrs['href'])
	return internalLinks
	
# 获取页面所有外链的列表
def getExternalLinks(bsObj, excludeUrl):
	externalLinks = []
	# 找出所有以"http"或"www"开头且不包含当前URL的链接
	for link in bsObj.findAll("a",href=re.compile("^(http|www)((?!"+excludeUrl+").)*$")):
		if link.attrs['href'] is not None:
			if link.attrs['href'] not in externalLinks:
				externalLinks.append(link.attrs['href'])
	return externalLinks

# 解析内链含有的特定域名：起始域名去掉“http://”
def splitAddress(address):
	addressParts = address.replace("http://", "").split("/")
	return addressParts

# 如果外链为0，则随机进入一个内链，再次寻找外链（即再次运行函数本身）；外链数目不为0，则随机进入一个外链
def getRandomExternalLink(startingPage):
	html = urlopen(startingPage)
	bsObj = BeautifulSoup(html)
	externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])  # 取分割后的第一项
	if len(externalLinks) == 0:
		internalLinks = getInternalLinks(startingPage)
		return getNextExternalLink(internalLinks[random.randint(0, len(internalLinks)-1)])
	else:
		return externalLinks[random.randint(0, len(externalLinks)-1)]

# 定义主函数，调用其他函数
def followExternalOnly(startingSite):
	externalLink = getRandomExternalLink("http://oreilly.com")
	print("随机外链是："+externalLink)
	followExternalOnly(externalLink)

followExternalOnly("http://oreilly.com")
```
## 3.4 用Scrapy采集
