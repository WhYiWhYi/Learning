# 7 提升爬虫速度

## 7.1 并发和并行，同步和异步
### 7.1.1 并发和并行
并发：在一个时间段内发生若干事件的情况
* 举例：在使用单核CPU时，多个工作任务即以并发的方式进行。因为只有一个CPU，因此各个任务会分别占用CPU一段时间依次执行，如果在对应的时间段没有分得任务，就会切换到另一个任务，并在下一次得到CPU使用权的时候再继续执行直至完成。在这种情况下，因为各个任务的时间段很短经常切换，所以看上去像是“同时”运行

并行：在同一时刻发生若干事件的情况
* 举例：在使用多核CPU时，各个核的任务能够真正地同时运行，即并行
### 7.1.2 同步和异步
同步：并发或并行的各个任务不是独自运行的，任务之间有一定的交替顺序，可能在运行完一个任务得到结果后，另一个任务才会开始运行
异步：并发或并行的各个任务可以独立运行，一个任务的运行不受另一个任务的影响
## 7.2 多线程爬虫
### 7.2.1 多线程
threading模块的Thread类
```python
import threading
import time

class myThread (threading.Thread):  # 定义创建线程的类
	def __init__(self, name, delay):
		threading.Thread.__init__(self)
		self.name = name
		self.delay = delay
	
	def run(self):  # 定义运行线程后要在执行的方法
		print("Strating " + self.name)
		print_time(self.name, self.delay)  # 在构建实例时传入的“delay”参数将传递至print_time函数
		print("Exiting " + self.name)

def print_time(threadName, delay):
	counter = 0
	while counter < 3:
		time.sleep(delay)
		print(threadName, time.ctime())
		counter += 1

threads = []
# 创建新线程：对myThread类传入参数，创建thread1和thread2两个实例
thread1 = myThread("Thread-1", 1)
thread2 = myThread("Thread-2", 2)
# 开启新线程
thread1.start()
thread2.strat()
# 添加新线程到线程列表
threads.append(thread1)
threads.append(thread2)
# 等待所有的线程完成
for t in threads:
	t.join()  # 等待所有子线程完成后才会继续执行主线程

print("Exiting Main Thread")
```
### 7.2.2 简单的多线程爬虫
举个栗子
```python
# 开启五个线程获取1000个网页
import threading
import requests

link_list = []
with open ("alexa.txt", "r") as file:
	file_list = file.readline()
	for eachone in file_list:
		link = eachone.split("\t")[1]
		link = link.replace("\n", "")
		link_list.append(link)

class myThread(threading, Thread):  # 定义创建线程的类（包括线程要进行的操作）
	def __init__(self, name, link_range):
        threading.Thread.__init__(self)
        self.name = name
        self.link_range = link_range
	# 在创建线程的类中就要定义好线程需要做的事，每个线程在运行其内的函数（crawler）时都是单独进行的
	def run(self):  # 定义每个线程要做的事
		print("Starting " + self.name)  # 创建线程
		crawler(self.name, self.link_range)  # 线程创建后要运行的函数
		print("Exiting " + self.name)  # 结束线程

def crawler(threadName, link_range):
	# 一个骚气的分组遍历的例子
	for i in range(link_range[0], link_range[1]+1)  # 依据link_range_list的编号设置遍历
		try:
			r = requests.get(link_list[i], timeout=20)
			print(threadName, r.status_code, link_list[i])
		except Exception as e:
			print(threadName, "Error: ", e)

thread_list = []
link_range_list = [(0,200), (201,400), (401,600),(601,800),(801,1000)]
# 创建新线程
for i in range(1,6):  # 创建五个线程，注意range从1开始，故到6结束
	thread = myThread("Thread-" + str(i), link_range_list[i-1])  # 每个线程分配对应的linklist
	thread.start()  # 创建新线程后直接运行
	thread_list.append(thread)  # 将线程加入到线程列表中
# 等待所有线程完成
for thread in thread_list:
	thread.join()

print("Exiting Main Thread")
```
在这一代码中，1000个链接被分为了五等分，所以当某个线程先完成200条网页的爬虫后会退出线程，即只剩下四个线程在运行，此时相对五个线程来说速度会下降，而只有最后一个线程时，就会变回单线程运行
### 7.2.4 使用Queue的多线程爬虫
使用Queue可以在完成1000个网页的抓取前都使用5个线程的全速爬虫，将这1000个网页放入Queue的队列中，各个线程都是从这个队列中获取链接，直到完成所有的网页抓取为止
Queue的队列类：
* FIFO（先入先出）队列Queue
* LIFO（后入先出）队列LifoQueue
* 优先级队列PriorityQueue
举个栗子
```python
import threading
import requests
import queue as Queue

link_list = []
with open ("alexa.txt", "r") as file:
	file_list = file.readline()
	for eachone in file_list:
		link = eachone.split("\t")[1]
		link = link.replace("\n", "")
		link_list.append(link)

class myThread(threading.Thread):
	def __init__(self, name, q):  # 创建线程时的简单参数：线程名字、爬取的链接列表
		threading.Thread.__init__(self)
		self.name = name
		self.q = q
	def run(self):
		print("Strating " + self.name)
		while True:
			try:
				crawler(self.name, self.q)
			except:
				break
		print("Exiting " + self.name)

def crawler(threadName, q):
	url = q.get(timeout=2)  # 从填充的workQueue队列中逐步获取链接
	try:
		r = requests.get(url, timeout=20)
		print(q.qsize(), threadName, r.status_code, url)  # q.qsize()输出队列剩余量
	except Exception as e:
		print(q.qszie(), threadName, url, "Error: ", e)

threadList = ["Thread-1", "Thread-2", "Thread-3", "Thread-4", "Thread-5"]
workQueue = Queue.Queue(1000)  # 使用FOFOQueue的方法建立队列对象workQueue
threads = []
# 创建新线程
for name in threadList:
	thread = myThread(name, workQueue)  # 向myThread中传入参数，创建线程实例
	thread.start()
	threads.append(thread)
# 填充队列
for url in link_list:
	workQueue.put(url)  # 向队列对象中添加链接填充队列，建立一个1000链接的长队
# 等待所有线程完成
for t in threads:
	t.join()

print("Exiting Main Thread")
```
## 7.3 多进程爬虫
7.2中介绍的多线程爬虫的多线程是在单核上以并发的方法异步运行，而多进程爬虫则可利用CPU的多核，进程数取决于计算机CPU的处理器个数，各个核上进程的运行是并行的。当进程数量大于CPU的内核数量时，等待运行的进程会等到其他进程运行完毕让出内核
python中使用多进程需使用multiprocessing库，有两种方法：Process+Queue、Pool+Queue方法
### 7.3.1 使用Process+Queue的多进程爬虫

了解计算机核心数量
```python
from multiprocessing import cpu_count
print(cpu_count())
```
举个栗子
```python
# 使用三个进程
from multiprocessing import Process, Queue
import requests

link_list = []
with open ("alexa.txt", "r") as file:
	file_list = file.readline()
	for eachone in file_list:
		link = eachone.split("\t")[1]
		link = link.replace("\n", "")
		link_list.append(link)

class myProcess(Process):
# myProcess子类使用了Process这个父类，因此在定义子类的构造函数(即__init__)时需要调用父类Process的构造函数
	def __init__(self, q):
		Process.__init__(self)
		self.q = q
	def run(self):
		print("Strating ", self.pid)
		while not self.q.empty()
			crawler(self.q)
		print("Exiting ", self.pid)

def crawler(q):
	url = q.get(timeout=2)
	try:
		r = requests.get(url, timeout=20)
		print(q.qsize(), r.status_code, url)
	except Exception as e:
		print(q.qsize(), url, "Error: ", e)

if __name__ == '__main__':
	ProcessNames = ["Process-1", "Process-2", "Process-3"]
	workQueue = Queue(1000)
	# 填充队列
	for url in link_list:
		workQueue.put(url)  # 向队列对象中添加链接填充队列，建立一个1000链接的长队
	
	for i in range(0,3):
		p = Myprocess(WorkQueue)
		p.daemon = True  # 在多进程中每个进程都可以单独设置其属性，当设置为True时，父进程结束后子进程即自动终止
		p.start()
		p.join()
	
	print("Main process Ended!")
```
### 7.3.2 使用Pool+Queue的多进程爬虫
当被操作对象数目不大时，可直接使用Process动态生成多个进程，但若是上百个、上千个目标，手动地限制进程数量就太过于烦琐，此时可使用Pool发挥进程池的功效
Pool可以提供指定数量的进程供用户调用，当有新的请求提交到pool中时，如果池还没满，则可创建一个新的进程来执行该请求，若池中进程数已经达到规定的最大值，则请求就会继续等待，直到池中的进程结束方可创建新的进程
阻塞和非阻塞：
阻塞和非阻塞关注的是程序在等待调用结果（消息、返回值等）时的状态

* 阻塞：等到回调结果出来，在有结果前，当前进程会被挂起
* 非阻塞：添加进程后，不一定非要等到结果出来就可以添加其他进程运行（当然是非阻塞式更好啦！）

举个栗子
```python
from multiprocessing import Pool, Manager
import requests

link_list = []
with open ("alexa.txt", "r") as file:
	file_list = file.readline()
	for eachone in file_list:
		link = eachone.split("\t")[1]
		link = link.replace("\n", "")
		link_list.append(link)

def crawler(q, index):
	process_id = "Process-" + str(index)
	while not q.empty()
		url = q.get(timeout=2)
		try:
			r = requests.get(url, timeout=20)
			print(Process_id, q.qsize(), r.status_code, url)
		except Exception as e:
			print(Process_id, q.qsize(), url, "Error: ", e)

if __name__ == '__main__'
	manager = Manager()  # 创建Manager类的实例
	workQueue = manager.Queue(1000)  # 使用manager实例来创建队列，该队列可以在父进程与子进程间通信
	# 填充队列
	for url in link_list:
		workQueue.put(url)  # 向队列对象中添加链接填充队列，建立一个1000链接的长队
	
	poor = Pool(processes=3)  # 设置进程池为3
	for i in range(4):
		# 使用Pool创建子进程时，使用的方法为：pool.apply_async(target=func, args=(args))
		# func使用的是函数名而非调用函数，故无需加括号
		# args使用元组传入参数至func函数：队列、进程；此处传入workQueue队列和第i个进程到函数中
		# pool.apply_async是非阻塞方法，pool.apply为阻塞方法
		pool.apply_async(crawler, args=(workQueue, i))
	
	print("Strated processes")
	pool.close()
	pool.join()
	
	print("Main process Ended!")
```
## 7.4 多协程爬虫

协程的优势：
* 协程像一种在程序级别模拟系统级别的进程，由于其是单线程且少了上下文切换，因此相对来说系统消耗少，速度快
* 协程方便切换控制流，可简化编程模型。协程可保留上一次调用时的状态（所有局部状态的一个特定组合），每次过程重入时，就相当于进入了上一次调用的状态
* 协程具有高扩展性和高并发性，一个CPU甚至可支持上万协程

协程的缺点
* 协程本质是一个单线程，不能同时使用单个CPU的多核，需要和进程配合才能运行在多CPU上
* 有长时间阻塞的IO操作时，可能会阻塞整个程序，此时最好不要使用协程
举个栗子
```python
import gevent  # 协程库
from gevent.queue import Queue, Empty
import requests

from gevent import monkey
monkey.patch_all()  # 将下面可能有IO操作的单独做上标记，并将IO转为异步执行的函数

link_list = []
with open ("alexa.txt", "r") as file:
	file_list = file.readline()
	for eachone in file_list:
		link = eachone.split("\t")[1]
		link = link.replace("\n", "")
		link_list.append(link)
	
def crawler(index):
	process_id = "Process-" + str(index)
	while not workQueue.empty()
		url = workQueue.get(timeout=2)
		try:
			r = requests.get(url, timeout=20)
			print(Process_id, workQueue.qsize(), r.status_code, url)
		except Exception as e:
			print(Process_id, workQueue.qsize(), url, "Error: ", e)

def boss():
	for url in link_list:
		workQueue.put_nowait(url)  # 填充队列

if __name__ == '__main__':
	workQueue = Queue(1000)  # 仍使用Queue创建队列
	
	gevent.spawn(boss).join()  # 将Queue创建的队列中加入的内容整合到gevent中
	# 创建多协程爬虫
	jobs = []
	for i in range(10):  # 开了10个协程
		jobs.append(gevent.spawn(crawler, i))
	gevent.joinall(jobs)
	
	print("Main ended!")
```
## 7.5 总结
多线程、多进程、多协程所需的时间明显少于串行
当线程、进程、协程数均设置为3时，三者所花时间基本上是串行的1/3
新增线程可以增加下载速度，但由于Python在多线程中的GIL锁机制，由于某个进程需要在更多的线程之间进行切换，所以会浪费很多时间，因此多增加的线程效果或越来越不明显
而多进程是调集CPU的多个进程进行工作，10个进程的性能是串行的10多倍
多协程由于是在单线程上模拟的并发，因此在性能上并没有多线程和多进程那么好，且由于带宽的限制，新添加的线程并不会带来更快的速度