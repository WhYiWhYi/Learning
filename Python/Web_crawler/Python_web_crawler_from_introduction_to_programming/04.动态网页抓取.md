# 4 动态网页抓取

## 4.1 动态抓取的实例
异步更新技术：Asynchronous JavaScript And XML, AJAX。可通过在后台与服务器进行少量数据交换就可以使网页实现异步更新，即可以在不重新加载整个网页的情况下对网页的某部分进行更新
动态网页：当用JavaScript加载时，放置评论的代码里并没有评论数据，而是通过JavaScript提取数据加载到源代码进行呈现的
动态网页的例子
* 博客的评论区
* 天猫的评论区
使用AJAX加载的动态网页的爬取方法
* 通过浏览器审查元素解析地址
* 通过Selenum模拟浏览器抓取
## 4.2 解析真实地址抓取（以知乎的关注者为例）
1. 在需要动态抓取的网页打开F12
2. 点击Network选项，刷新网页。此时Network会显示浏览器从网页服务器中得到的所有文件，这一过程称“抓包”，所需要的关注者数据就在其中
3. 一般这些数据都已json文件格式获取，因此选择Network中的XHR选项，找到followers文件，并在Preview里找到真实地址
4. 通过requests请求地址数据
5. 使用json库中的json.loads解析json数据结构，找到自己需要的内容后提取即可
```python
# 爬取知乎关注者的名称
import time
import requests
import json

# 共有693位关注者，一个页面显示20人，首先要尝试获取所有界面：需要解析网址的结构
def getLinklist(followernum):  # 输入值为粉丝数目
    follower_num = followernum
    a = follower_num // 20 + 2  # range()取前不取后，故此处需+2而不是+1
    linklist = []
    # linkhead依据不同账号进行更改
    linkhead = '''https://www.zhihu.com/api/v4/members/haoyu-52/followers?include=data%5B%2A%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&limit=20&offset='''
    for i in range(0, a):
        link = linkhead + str(i * 20)
        linklist.append(link)
    return linklist  # 返回值为一个列表，保存有所有要爬界面的网址

# 获取关注者的名字
def getFollowers(linklist):
    getFollowers = []
    headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36'}
    for link in linklist:
        r = requests.get(link, headers=headers, timeout=10)
        time.sleep(0.75)  # 设置间隔时间
        json_data = json.loads(r.text)
        # print(json_data)  # 可打印出来看一下json解析后的格式，找到自己所需内容的位置
        follower_list = json_data['data']  # json解析后，使用['data']索引到对应的key
        for follower in follower_list:  # 保存follower信息的是一个列表，固遍历获取
            follower_name = follower['name']  # 每个人的信息都是字典，故使用['name']索引到对应的key
            getFollowers.append(follower_name)
    return getFollowers

list1 = getFollowers(getLinklist(693))
print(list1)
```
## 4.3 通过Selenium模拟浏览器抓取
在复杂的网站，调用的网页很难直接通过F12找到，此外，有些数据真实地址的URL也十分冗长和复杂，甚至为了规避抓取而进行加密，使变量十分复杂。对于这些情况，可以使用浏览器渲染引擎，直接用浏览器在显示网页时解析HTML、应用CSS样式并执行JavaScript语句
此方法在爬虫过程中会打开一个浏览器加载该网页，自动操作浏览器浏览各个网页并把数据抓下来。即使用浏览器渲染方法将爬取动态网页变成爬取静态网页
举个栗子
### 4.3.1 爬取博客单条评论
```python
# 爬取博客的单条评论
import time
from selenium import webdriver

driver = webdriver.Chrome()  # 使用Chrome作为模拟浏览器
driver.get("http://www.santostang.com/2018/07/04/hello-world/")
time.sleep(5)  # 网速较慢时，网页加载不全，下述代码检索不到，需要暂停一下
# 代码中的JavaScript解析成了一个iframe，所有的评论都装在这个框架中，里面的评论并未解析出来，故若不对iframe解析则无法找到'div.reply-content'元素
driver.switch_to.frame(driver.find_element_by_css_selector("iframe[title='livere']"))
# 找到div标签下class属性为“reply-content”的元素，即<div class="reply-content">
# “.”表示class属性，“#”表示id属性
comment = driver.find_element_by_css_selector('div.reply-content')
content = comment.find_element_by_tag_name('p')  # 找到comment中标签为“p”的内容
print(content.text)
```
## 4.3.2 # 爬取博客所有评论
```python
# 爬取博客的所有评论
from selenium import webdriver
import time

driver = webdriver.Chrome()  # 使用Chrome作为模拟浏览器
# 隐性等待：设置了一个最长等待时间，如果在规定时间内网页加载完成，则执行下一步，否则一直等到时间截止，然后执行下一步。隐性等待对整个driver的周期都起作用，所以只要设置一次即可
driver.implicitly_wait(20)  # 隐性等待，最长等20秒
driver.get("http://www.santostang.com/2018/07/04/hello-world/")
time.sleep(5)
for i in range(0,3):
    # 下滑到页面底部
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    # 转换iframe，再找到查看更多，点击
	driver.switch_to.frame(driver.find_element_by_css_selector("iframe[title='livere']"))
    load_more = driver.find_element_by_css_selector('button.more-btn')  # 找到“查看更多”
    load_more.click()  # 模拟单击
    # 解析iframe后，下滑的代码就用不了了，所以又要用driver.switch_to.default_content()转回去成本来的未解析iframe
    driver.switch_to.default_content()
    time.sleep(2)  # 程序等待2秒以加载评论

# 再次转换iframe以用于寻找评论
driver.switch_to.frame(driver.find_element_by_css_selector("iframe[title='livere']"))
comments = driver.find_elements_by_css_selector('div.reply-content')
for comment in comments:
    content = comment.find_element_by_tag_name('p')
    print (content.text)
```
### 4.3.3 selenium其他
selenium选择元素的方法
* find_element_by_id：通过元素的id选择
	```driver.find_element_by_id(‘loginForm’)```
* find_element_by_name：通过元素的name选择
	```driver.find_element_by_name(‘password’)```
* find_element_by_xpath：通过xpath选择
* find_element_by_link_text：通过链接地址选择
* find_element_by_partial_link_text：通过链接的部分地址选择
* find_element_by_tag_name：通过元素的名称选择
	```comment.find_element_by_tag_name('p')```
* find_element_by_class_name：通过元素的id选择
* find_element_by_css_selector：通过css选择器选择
	```driver.find_elements_by_css_selector('div.reply-content')```
* 当要选择多个元素时，使用“elements”即可

selenium的其他操作
```python
user = driver.find_element_by_name("username")
user.clear  # clear：清除元素内容，用于在模拟输入时
user.send_keys("1234567")  # send_keys：模拟按键输入，用于模拟输入时
pwd = driver.find_element_by_name("password")
pwd.clear
pwd.send_keys("******")
driver.find_element_by_id("loginBtn").click()  # click：模拟点击
```
selenium的官方文档：http://selenium-python.readthedocs.io/index.html
### 4.3.4 selenium高级操作
由于Selenium在整个网页加载出来后才开始爬取内容，速度往往较慢，因此，在实际使用时，可以用selenium控制浏览器加载的内容，从而加快Selenium的爬取速度
* 控制CSS加载
	CSS样式文件是用于控制页面的外观和元素放置位置，对内容并无影响，故网页的CSS可以限制
* 控制图片文件的显示
	如果不需要抓取网页的图片，最好可以禁止图片加载以极大提高网络爬虫效率
* 控制JavaScript的运行
	```python
	prefs = {
		'profile.default_content_setting_values': {
			'images': 2,  # 限制图片加载
			'permissions.default.stylesheet':2,  # 限制CSS加载
			'javascript': 2  # 限制Java加载
		}
	}
	chrom_opt = webdriver.ChromeOptions()
	chrom_opt.aadd_experimental_option("prefs", prefs)
	
	browser = webdrive.Chrome(chrome_options=chrom_opt)
	browser.get()
	```