# 6 使用神经网络拟合数据
## 6.1 人工神经元
深度学习的核心是神经网络：**一种能够通过简单函数的组合来表示复杂函数的数学实体**

这些复杂函数的基本构件是**神经元**。单个神经元的核心就是对输入的**线性变换**，如将输入乘以一个数字（权重），再加上一个常数（偏置），然后应用一个**固定的非线性函数，即激活函数**，可以表示为下列形式：
$$
o = f(wx\,+\,b)
$$
* $o$ 是输出
* $x$ 是输入
* $w$ 是权重或比例因子
* $b$ 是偏置或偏移量
* $f()$ 是激活函数

输入（$x$）和输出（$o$）可以是简单的标量或向量值（保留许多标量值），权重（$w$）可以是单个标量或矩阵，偏置（$b$）是标量或向量（输入的维度和权重必须匹配）。当表达式使用多维权重和偏置表示许多神经元时，可称为一层神经元
### 6.1.1 组成一个多层网络
一个多层神经网络可以有刚刚讨论过的函数组成。其中前一层神经元的输出被用作后一层神经元的输入
$$
x_1 = f(w_0 * x_0 + b_0)
$$
$$
x_2 = f(w_1 * x_1 + b_1)
$$
$$
...
$$
$$
y = f(w_n * x_n + b_n)
$$
$$
y = f(w_n * f(...f(w_1 * f(w_0 * x_0 + b_0) + b_1)...) + b_n)
$$
### 6.1.2 理解误差函数
上一章的线性模型和将实际用于深度学习的模型之间的一个重要区别是误差函数的形状。线性模型和误差平方损失函数有一条**凸误差曲线**，其具有一个明确定义的最小值，自动、确定地求出使误差函数值最小的参数是完全可能的

对于神经网络而言，试图近似的每一个参数都没有唯一的正确答案，取而代之的是试图让所有参数在协同作用的时候产生一个有用的输出。这个输出只是接近事实，因此会有一定程度的不完美。这种不完美在哪里以及如何表现是有些随意的，由此可见，控制输出的参数（以及因此而产生的不完美）也是有些随意的

神经网络具有**非凸误差曲面**的很大一部分原因是激活函数。一组神经元能够近似广泛有用函数的能力取决于每个神经元固有的线性和非线性行为的组合
### 6.1.3 我们需要的只是激活函数 & 6.1.4 更多激活函数
激活函数的重要作用
* 它允许模型内部的输出函数在不同值上有不同的斜率。通过巧妙地为许多输出设置不同的斜率，神经网络可以近似任意函数
* 它可以在网络的最后一层将前面的线性运算的输出集中到给定范围内
	* 限制输出范围：如对输出值设置上限，```torch.nn.Hardtanh()```
	* 压缩输出范围：如 ```torch.nn.Sigmoid()```，其在线性函数输出的中间有一块区域，神经元在这个区域十分敏感（这种敏感可以理解为pH值的酸碱变化），而其它所有的东西都集中在边界值的旁边。敏感范围内的样本的微小改变将导致结果的显著变化，而在边界值的样本即使有剧烈的变化，对输出值的影响也不大

其他输出函数：如ReLU，是目前（2017年？）性能最好的通用激活函数之一
### 6.1.5 选择最佳激活函数
一般而言（下列陈述也不总是正确的），激活函数有如下特性
* 非线性。非线性可以使整个网络能够逼近更复杂的函数
* 可微。因此可以通过它们计算梯度
* 至少有一个敏感范围，此范围内对输入的变化会导致输出产生相应的变化，这是训练所需要的；包含许多不敏感（或饱和）的范围，即输入的变化导致输出的变化很小或没有变化
* 通常情况下，激活函数至少有以下一种特征
	* 当输入到负无穷大时，接近（或满足）一个下限
	* 正无穷时相似但上界相反
综合上述特性就组成了一个非常强大的机制：在一个由线性+激活单元构成的网络中，当不同的输入呈现给网络时，不同的单元会对相同的输入在不同范围内响应。与这些输入相关的错误将主要影响在敏感区域工作的神经元，使其他单元不受学习过程的影响。这些单元的参数相对容易通过梯度下降来优化，因为在输出饱和之前学习将表现得很像线性函数
### 6.1.6 学习对于神经网络意味着什么
深度神经网络其中一个吸引人的地方在于，它使我们不必过于担心代表数据的确切函数，不管它具体是什么函数。对于深度神经网络模型，我们有一个通用的逼近器和一种估参方法。这个逼近器可以依据具体需求进行定制，就模型容量和建模复杂输入输出关系的能力而言，只需组合简单的构建块

一个成功训练的网络，能通过它的权重和偏置捕捉数据的内在结构，以有意义的数值表示的形式正确地处理以前未见过的数据

深度神经网络能使我们在没有明确模型的情况下近似处理高度非线性的问题。相反，从一个通用的、未经训练的模型开始，我们通过为它提供一组输入和输出以及一个可以反向传播的损失函数，使它专门处理某项任务。**使用样本将一个通用模型专门化为一个任务就是我们所说的学习**。**因为模型并不是为了特定的任务而构建的，即模型并未描述该任务的工作规则**

在某种程度上，我们放弃解释，以换取解决日益复杂的问题的可能性。换句话说，我们有时缺乏能力、信息或计算资源来为我们所呈现的事物建立一个明确的模型，所以数据驱动是我们前进的唯一途径
## 6.2 PyTorch nn模块
```torch.nn``` 模块包含创建各种神经网络结构所需的构建块（即模块，在其他框架中被称为”层”）。PyTorch模块派生自基类 ```nn.Module```，一个模块可以有一个或多个参数实例作为属性，这些参数实例是张量，它们的值在训练过程中得到了优化。一个模块还可以有一个或多个子模块（```nn.Module``` 的子类）作为属性，且它还能跟踪它们的参数
* 子模块必须是顶级属性，不能包含在列表或字典实例中。对于需要一列或一组子模块的情况，PyTorch提供了 ```nn.ModuleList``` 和 ```nn.ModuleDict```
### 6.2.1 使用__call__而非forward()
PyTorch提供的所有nn.Module的子类都定义了它们的 ```__call__``` 方法，这允许我们实例化一个```nn.Linear```（```nn.Module```的一个子类），并像调用函数一样调用它
* ```forward()``` 方法执行前向计算。可以直接调用
* ```__call__()``` 方法在调用 ```forward()``` 之前和之后执行其他重要的事情。推荐使用
```python
import torch.nn as nn

linear_model = nn.Linear(1, 1)
linear_model(t_un_val)  # 推荐使用y = model(x)，而不是y = model.forward(x)
```
### 6.2.2 回到线性模型
1. 批量输入

	```nn``` 中的所有模块都被编写为可以同时为多个输入产生输出。假设需要在10个样本上运行```nn.Linear```，可以创建一个大小为B（即10） × Nin的输入张量，Nin为输入特征的数量；输出也为B × Nout的张量，Nout为输出特征的数量
	
	* 以批处理图像为例，输入维度为 $B \times C \times H \times W$，包含3个批次（$B$），3个通道（$C$）和高度、宽度
2. 优化批次
	
	批处理的原因
	* 确保我们要求的计算足够大以饱和我们用于执行计算的计算资源，使批量的结果返回速度与单个结果的返回速度一样快
	* 一些高级模型使用来自整个批处理的统计信息，且随着批处理大小的增加，这些统计信息会变得更好
3. 使用 ```nn``` 提供的损失

	```nn``` 实际包含了几个常见的损失函数，如 ```nn.MSELoss()```。这些损失函数也是 ```nn.Module``` 的子类，因此也可以创建一个实例并将其作为函数调用
	```python 
	loss_fn = nn.MSELoss()
	```
## 6.3 最终完成一个神经网络（以神经网络替代线性模型）
### 6.3.1 替换线性模型
本节将构建一个最简单的神经网络：一个线性模块，一个激活函数；输入到另一个线性模块
```nn``` 提供了一种通过 ```nn.Sequential``` 容器来连接模型的方式

最终构建的模型：将第1个模块所期望的输入指定为 ```nn.Sequenial``` 的一个参数，将中间输出传递给后续模块，并产生最后一个模块返回的输出
```python
from torch import nn

# 从1个输入特征得到13个隐藏特征，通过tanh激活函数传递，并将得到的13个数字线性组合成1个输出特征
seq_model = nn.Sequential(nn.Linear(1, 13),  # 线性模块，in=1，out=13
						  nn.Tanh(),  # 激活函数
						  nn.Linear(13, 1))  # 另一个线性模块，in=13，out=1

seq_model
>>> Sequential(
			   (0): Linear(in_features=1, out_features=13, bias=True),
			   (1): Tanh(),
			   (2): Linear(in_features=13, out_features=1, bias=True))
```
### 6.3.2 检查参数
对Sequential容器调用 ```model.parameters()``` 将从第1个和第2个线性模块手机权重和偏置。这种情况下可通过输出参数的形状来检查参数具有指导意义
```python
[param.shape for param in seq_model.parameters()]
>>> [torch.Size([13, 1]),
	 torch.Size([13]),
	 torch.Size([1, 13]),
	 torch.Size([1])]  # 优化器将得到的各个张量
```
在调用 ```model.backward()``` 之后，所有参数都填充了它们的梯度，然后优化器在调用 ```optimizer.step()``` 期间相应地更新它们的值，这和此前的线性模型并无不同

在检查由几个子模块组成的模型的参数时，也可通过名称识别参数。每个模块的名称就是模块在参数中出现的序号。此外Sequential也接受OrderedDict为每个模块命名，这允许我们为子模块配置更多解释性名称
```python
for name, param in seq_model.named_parameters():
	print(name, param.shape)

>>> 0.weight torch.Size([13, 1])
	0.bias torch.Size([13])
	2.weight torch.Size([1, 13])
	2.bias torch.Size([1])

# 对模块命名
from collection import OrderedDict

seq_model = nn.Sequential(OrderedDict([
									   ('hidden_linear', nn.Linear(1, 8)),
									   ('hidden_activation', nn.Tanh(),
									   ('output_linear', nn.Linear(8, 1)))]))
```
此外，我们也可通过将子模块作为属性来访问一个特定的参数，这对于检查参数或它们的梯度非常有用，例如在训练期间监控梯度
```python
seq_model.output_linear.bias
>>> Parameter containing: tensor([-0.0173], requires_grad=True)
```