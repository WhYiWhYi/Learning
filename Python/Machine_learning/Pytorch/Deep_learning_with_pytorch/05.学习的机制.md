# 5 学习的机制
机器学习：提出 一种输入数据与期望输出配对的学习算法。一旦学习发生，当向算法输入与训练时的输入数据**足够相似**的新数据时，该算法将能够产生正确的输出

深度学习：即使输入数据与期望的输出数据**相差甚远**时，学习算法也能工作

本章主要讲解如何自动化通用拟合函数（以最小化损失），以下是本章将实现内容的高度概括
* 给定输入数据和相应期望输出（实际数据）以及权重（参数）的初始值 
* 向模型输入数据（正向传播），并通过比较输出结果与实际数据来**评估误差**
* 优化模型参数：使用复合函数的导数的链式法则计算（反向传播，以更新权重值）**权重单位变化后的误差变化**（误差相对参数的梯度）
* 在**使误差减小**的方向上**更新权重值**
* 重复上述过程2~4，直至根据未知的数据评估的误差降到可接受的范围内
## 5.1 永恒的建模经验
我们将从数据中学习一些东西，即使算法从数据中学习。同样地，也可以表述为我们将**拟合数据**。在这个过程中总是涉及一个具有许多参数的函数，这些参数的值是从数据中估计出来的：简而言之，这就是模型。可以认为从数据中学习的假定基础模型并非解决特定问题，而是能够近似**更广泛的函数族**，其可以自动适应处理相似输入和输出对的任何任务
## 5.2 学习就是参数估计
示例：建立一个数据集，以我们选择的单位来表示刻度值和相应的温度值，选择一个模型，迭代地调整其权重，直到误差的度量足够低，最终能够以我们选择的单位来解释新的刻度值
### 5.2.2 数据准备
```python
# 基础数据
# t_c为摄氏度为单位的温度，t_u为我们未知的单位
t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])
t_u = torch.sensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])
```
当有了一组数据，可以将其可视化来看其大致的规律（低维数据可以，复杂的高维数据可能不行）
### 5.2.4 选择线性模型首试
在缺乏进一步了解的情况下，我们猜测这两组数据集可能是线性相关的，即 $t\_u$ 乘以一个因子（权重 $w$），加一个常数（偏置 $b$），我们就可以得到摄氏温度（忽略一定的误差）
* 权重：告诉我们给定的输入对输出的影响有多大
* 偏置：所有输入为零时的输出
$$
t\_c = w * t\_u + b
$$
现在我们将基于现有的数据来评估模型中的 $w$ 和 $b$ 参数，这样我们通过运行模型得到的位置温度 t_u 就会接近我们实际测量的摄氏温度（听起来就像是通过一组测量值来**拟合**一条直线）。

总结：我们有一个带有一些未知参数的模型，我们需要估计这些参数以使输出的预测值和测量值之间的**误差尽可能小**。神经网络的本质上是使用几个或一些参数将一个模型变换为更加复杂的模型
### 5.3 减少损失是我们想要的
我们需要精确定义误差的程度，这种程度通过**损失函数**来定义
* 损失函数（或代价函数）是一个计算单个数值的函数，其通常涉及获取一些训练样本的期望输出与输入这些样本是模型实际产生的输出之间的差值
* 如果误差很大，则说明损失函数的值大了，理想情况下应该尽可能使损失函数的值较小，这样预测值和测量值二者可以完美匹配。我们的优化过程应该以找到 $w$ 和 $b$ 为目标，使损失函数处于最小值。
* 当优化目标是最小化（minimize）时，我们自然地希望能最小化收敛到某个常数值，比如常见的0，于是一般来说需确保损失函数计算得到的损失为正
* 从概念上说，损失函数是一种对训练样本中要修正的错误进行优先处理的方法，因此参数更新会导致对高权重样本的输出进行调整，而不是对损失较小的其他样本的输出进行调整。依据所选择数学表达式的不同，可强调或忽略某些误差

在本节的例子中，最直接的损失函数为 $|t\_p - t\_c|$ 和 $(t\_p-t\_c)^2$。基于一些理由（详见原文），最终选择了 $(t\_p-t\_c)^2$ 作为本例的损失函数
```python
# 定义模型
# t_u：输入张量；w：权重参数；b：偏置参数
def model(t_u, w, b):
	return w * t_u + b

# 定义损失
# 构建了一个差分张量，先对其平方元素进行处理，最后通过对得到的张量中的所有元素求平均值得到一个标量损失函数，即均方损失函数
def loss_fn(t_p, t_c):
	squared_diffs = (t_p - t_c) ** 2
	return squared_diffs.mean
```
【拓展】广播
Pytorch改写了在NumPy中流行的广播机制（只能对**相同形状**的参数使用基于元素的二元运算（加减乘除），每个张量中匹配位置的项将被用来计算与结果张量中相应的项），对大多数二元运算放宽了一些假定条件
* 向后向前迭代每个索引维度，如果该维度中有一个操作数的维度大小1，那么PyTorch将使用该维度上的单个项与另一个张量沿该维度上的每一项进行运算
* 如果两个维度大小都大于1，则它们的维度大小必须相同，并使用自然匹配
* 如果一个张量的维度大于另一个张量的维度，那么另一个张量上的所有项将和这些维度上的每一项进行运算
```python
x: torch.Size([])
y: torch.Size([3, 1])
z: torch.Size([1, 3])
a: torch.Size([2, 1, 1])

x * y
>>> torch.Size([3, 1])
y * z
>>> torch.Size([3, 3])
y * z * a
>>> torch.Size([2, 3, 3])
```
## 5.4 沿梯度下降
我们将根据参数，使用**梯度下降法**来优化损失函数。梯度下降是一个非常简单的概念，它可以很好地扩展到具有数百万个参数的大型神经网络模型
### 5.4.1 减小损失
梯度下降的思想是计算各参数的损失变化率，并在**减小损失变化率的方向上**修改各参数。在本节的例子中，可以通过在 $w$ 和 $b$ 上添加（或减少）一个小数字来估计损失变化率，即损失在这附近的变化有多大
```python
delta = 0.1  # “小数字”

loss_rate_of_change_w = \
(loss_fn(model(t_u, w+delta, b), t_c) - 
loss_fn(model(t_u, w-delta, b), t_c)) / (2.0 * delta)  #  b参数也可应用相同的
```
在 $w$ 和 $b$ 的当前值附近，$w$ 的增加会导致损失的一些变化。如果变化是负的，那我们需要增加 $w$ 来最小化损失；若变化是正的，我们需要减小 $w$ 的值

具体需要增加或减少可以靠对 $w$ 应用一个与损失的变化率成比例的变化，特别当损失有多个参数时：我们将一个变化应用于那些可以使损失产生重大变化的参数。另一种简单的方式使通过缓慢地改变参数，因为在距离当前 $w$ 值的邻域很远的地方，改变的速率可能会有显著不同。因此，我们通常应该用一个小的**比例因子**来衡量变化率。这个比例因子有很多名称，在机器学习中称为学习率（learning_rate）
```python
learning_rate = 1e-2
w = w - learning_rate * loss_rate_of_change_w
```
通过重复以上评估步骤（当选择一个足够小的学习率），我们将收敛到在给定数据上使损失最小的参数的最优值
### 5.4.2 进行分析









