# 3 从张量开始
如第2章的例子，深度学习支持的一些应用程序总是以某种形式获取数据（图像或文本），并以另一种形式生成数据（如标签、数字或更多的图像或文本）。从这个角度来说，深度学习实际上需要构建一个能够将数据从一种表示转换为另一种表示的系统
## 3.1 实际数据转为浮点数
深度神经网络通常在不同阶段学习将数据从一种形式转换为另一种形式，意味着每个阶段转换的数据可以被认为是一个**中间表征序列**。一般来说，这些中间表征是**浮点数**的集合，它们描述输入的特征，并以一种有助于描述输入映射到神经网络输出的方式捕获数据的结构。这些描述是针对当前任务的，是从相关例子中学习到的。这些中间表征是将**输入**与**前一层神经元的权重**结合的结果，每个中间表征对之前的输入都是唯一的
## 3.2 张量：多维数组
张量：也可称多维数组，是PyTorch引入的一种基本数据结构，其可以将向量（一维）和矩阵（二维）推广到任意维度。它储存了一组数字，这些数字可以用一个索引单独访问，也可以用多个索引访问

更有效的张量数据结构能表示多种类型的数据，如图像、时间序列，甚至句子。通过定义张量上的操作，甚至可以使用Python这样速度不是特别快的高级语言来同时高效对数据进行切片和操作
### 3.2.2 构造第一个张量
```python
import torch

a = torch.ones(3)  # 创建一个大小为3的一维张量，用1.0填充
b = torch.tensor([1.0, 1.0, 1.0])  # 将大小为3的列表转化为张量
```
### 3.2.3 张量的本质
Python列表/数字元组：在内存中**单独分配**的**Python对象的集合**

PyTorch张量/NumPy数组：**连续内存块**中**未封装的C数字类型**而非Python对象
```python
import torch

points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])

points.shape  # 获取张量每个维度上的大小（靠前的数字为更低维）
>>> torch.Size([3,2])

points[0]  # 使用单个索引定位二维张量中的一维张量
>>> tensor([4., 1.])

points[0, 1]  # 使用两个索引来定位二维张量中的单个元素
>>> tensor(1.)
```
## 3.3 索引张量
与NumPy一样，可以为**张量的每个维度**使用范围索引（高级索引见第4章）
```python
points[1:]  # 第一行之后的所有行，隐含所有列（列信息也会一同输出，但无需索引）

# ','前索引行，','后索引列
# a[axis1_x: axis1_y, axis2_x: axis2_y, ...]，n个维度就可以使用n-1个逗号进行分别索引
points[1:, :]  # 第一行之后的所有行、所有列（同时提供了列的索引）
>>> tensor([[5., 3.],
			[2., 1.]])
points[1:, 0]  # 第一行之后的所有行，第一列
>>> tensor([5., 2.])

points[None]  # 在外层增加大小为1的维度(通俗来说就是在最外面套上一层括号)
>>> tensor([[[4., 1.],
	 		 [5., 3.],
			 [2., 1.]]])
```
### 3.4 命名张量（直至2.0.0版本仍为试验性特性）
**当前此特性仍为试验性特性，重要的代码请勿使用**
张量的维度或坐标轴往往需要表示一些信息，这意味着当把张量作为索引时，我们需要记住维度的顺序并按此顺序编写索引。在通过多个张量转换数据时，跟踪哪个维度包含哪些数据可能容易出错，因此有人建议给维度指定名称

PyTorch的工厂函数，如tensor()、rand()等有一个names参数，其为一个字符串序列
```python
weights_named = torch.tensor([0.21, 0.71, 0.08], names=['channels'])
```
当已有一个张量，且想为其添加名称但又不改变现有名称时可使用refine_names()方法，使用rename()兄弟方法还可以覆盖或删除现有名称
```python
# 与索引类似，省略号（...）可允许省略任意数量的维度（在只想重命名后几个维度时可使用）
img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')

# 使用names方法查看维度名称
img_name = img_named.names
>>> ('channels', 'rows', 'columns')

# rename时可通过传入None来删除名称
ime_named = img_t.rename(..., None, 'rows', 'columns')
```
对于设置了张量名称的张量（即有两个输入的操作），除了常规维度检查，即检查张量维度是否相同外，PyTorch也将检查张量的名称，但目前为止其还没有维度自动对齐的功能，

不同名称的维度组合会出现报错，此外，如果想对命名的张量进行**操作的函数之外**使用，需要将这些张量重命名为None来删除他们的名称
```python
# align_as()可将张量按照输入的张量的维度补齐缺失的维度，并返回一个张量。补齐后的维度也将按照输入张量的“正确”的顺序排列
# 将weights_named张量按img_named张量补齐了维度
weights_aligned = weights_named.align_as(img_named)

weights_aligned.shape
>>> torch.Size([3, 1, 1])  # 后两个索引即为补齐的维度，也即'rows'和'columns'
weights_aligned.names
>>> ('channels', 'rows', 'columns')
```
## 3.5 张量的元素类型
在进行大规模数据处理和计算时，使用标准的Python数字类型可能不是最优的，原因如下：
* Python中的数字是对象。在存储少量数值时，这种封装操作并不是问题，但如果我们需要存储数百万的数据，采用封装操作会非常低效
* Python列表：为对象的顺序集合，没有为有效获取两个向量的点积或将向量求和而定义的操作；无法优化其内容在内存中的排列（因为它们是指向Python对象的指针的可索引集合）；尽管可以使用元素为列表的列表来生成高维数据，但这同样是十分低效的
* Python解释器与优化后的已编译代码相比速度很慢，在大型数字类型的数据集合上执行数学运算，使用用编译过的更低级的语言（如C）编写的优化代码可以快很多

鉴于上述原因，NumPy或PyTorch张量提供了高效的数值数据结构的底层实现和相关操作，并将这些封装在一个方便的高级API中。要实现这一点，张量内中的对象必须是相同类型的数字，PyTorch必须跟踪这个数字类型
### 3.5.1 使用dtype指定数字类型
张量构造函数通过dtype参数指定包含在张量中的数字数据类型
```python
points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], dtype=torch.float64)
```
dtype可能的取值包括：
* **torch.float32或torch.float**：32位浮点数（张量默认类型，占4bytes）
* torch.float64或torch.double：64位双精度浮点数
* torch.float16或torch.half：16位半精度浮点数
* torch.int8：8位有符号整数
* torch.unit8：8位无符号整数
* torch.int16或torch.short：16位有符号整数
* torch.int32或torch.int：32位有符号整数
* torch.int64或torch.long：64位有符号整数
* torch.bool：布尔值
### 3.5.2 适合任何场合的dtype
神经网络中的计算通常使用**32位浮点精度**执行，采用更高精度（64位）并不会提高模型精度，反而需要需要更多更多内存和计算时间。16位半精度浮点数的数据类型是由现代GPU提供的，在标准CPU中并不存在

张量可以作为其他张量的索引，此时PyTorch期望索引张量为**64位的整数**。创建一个将整数作为参数的张量（如torch.sensor([2,2])），默认会创建一个64位的整数张量

### 3.5.3 管理张量的dtype属性
可以将适当的dtype作为构造函数的参数已给张量分配正确的数字类型，也可以通过dtype来了解一个张量的dtype值
```python
double_points = torch.ones(10, 2, dtype=torch.double)

double_points.dtype
>>> torch.int16
```
我们也可以将张量创建的输出转换为正确的类型
* to()方法会检查转换是否是必要的，如必要则执行转换
```python
double_points = torch.zeros(10, 2).double()
double_points = torch.ones(10, 2).to(torch.double)
```
在操作中输入多种类型时，输入时会自动向较大类型转换。因此，如果我们想要进行32位计算，我们需要确保所有的输入最多是32位的
```python
points_32 = torch.rand(5, dtype.float)
points_64 = torch.rand(5, dtype.double)

points_32 * points_64
>>> dtype=torch.float64
```
## 3.6 张量的API
关于张量及张量之间的绝大多数操作都可以在torch模块中找到，此处列举一些操作的类型：
* 创建操作：用于构造张量的函数，如ones()、from_numpy()
* 索引、切片、连接、转换操作：用于改变张量的形状、步长或内容的函数，如transpose()
* 数学操作：通过运算操作张量内容的函数
	* 逐点操作：通过将函数分别应用于每个元素来得到一个新的张量，如abs()、cos()
	* 归约操作：通过遍历张量来计算聚合值的函数，如mean()、std()、norm()
	* 比较操作：在张量上计算数字谓词的函数，如equal()、max()
	* 频谱操作：在频域中进行变换和操作的函数
	* 其他操作：作用于向量的特定函数，或作用于矩阵的特定函数
	* BLAS和LAPACK操作：符合基本线性代数子程序规范的函数，用于标量、向量-向量、矩阵-向量和矩阵-矩阵等操作
* 随机采样：从概率分布中随机生成值的函数，如randn()、normal()
* 序列化：保存和加载张量的函数，如load()、save()
* 并行化：用于控制并行CPU执行的线程数的函数，如set_num_threads()
## 3.7 张量的存储视图
张量的底层实现：张量中的值被分配到由torch.Storage实例所管理的**连续内存块**中。










