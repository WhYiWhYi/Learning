# 3 从张量开始
如第2章的例子，深度学习支持的一些应用程序总是以某种形式获取数据（图像或文本），并以另一种形式生成数据（如标签、数字或更多的图像或文本）。从这个角度来说，深度学习实际上需要构建一个能够将数据从一种表示转换为另一种表示的系统
## 3.1 实际数据转为浮点数
深度神经网络通常在不同阶段学习将数据从一种形式转换为另一种形式，意味着每个阶段转换的数据可以被认为是一个**中间表征序列**。一般来说，这些中间表征是**浮点数**的集合，它们描述输入的特征，并以一种有助于描述输入映射到神经网络输出的方式捕获数据的结构。这些描述是针对当前任务的，是从相关例子中学习到的。这些中间表征是将**输入**与**前一层神经元的权重**结合的结果，每个中间表征对之前的输入都是唯一的
## 3.2 张量：多维数组
张量：也可称多维数组，是PyTorch引入的一种基本数据结构，其可以将向量（一维）和矩阵（二维）推广到任意维度。它储存了一组数字，这些数字可以用一个索引单独访问，也可以用多个索引访问

更有效的张量数据结构能表示多种类型的数据，如图像、时间序列，甚至句子。通过定义张量上的操作，甚至可以使用Python这样速度不是特别快的高级语言来同时高效对数据进行切片和操作
### 3.2.2 构造第一个张量
```python
import torch

a = torch.ones(3)  # 创建一个大小为3的一维张量，用1.0填充
b = torch.tensor([1.0, 1.0, 1.0])  # 将大小为3的列表转化为张量
```
### 3.2.3 张量的本质
Python列表/数字元组：在内存中**单独分配**的**Python对象的集合**

PyTorch张量/NumPy数组：**连续内存块**中**未封装的C数字类型**而非Python对象
```python
import torch

points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])

points.shape  # 获取张量每个维度上的大小（靠前的数字为更低维）
>>> torch.Size([3,2])

points[0]  # 使用单个索引定位二维张量中的一维张量
>>> tensor([4., 1.])

points[0, 1]  # 使用两个索引来定位二维张量中的单个元素
>>> tensor(1.)
```
## 3.3 索引张量
与NumPy一样，可以为**张量的每个维度**使用范围索引（高级索引见第4章）
```python
points[1:]  # 第一行之后的所有行，隐含所有列（列信息也会一同输出，但无需索引）

# ','前索引行，','后索引列
# a[axis1_x: axis1_y, axis2_x: axis2_y, ...]，n个维度就可以使用n-1个逗号进行分别索引
points[1:, :]  # 第一行之后的所有行、所有列（同时提供了列的索引）
>>> tensor([[5., 3.],
			[2., 1.]])
points[1:, 0]  # 第一行之后的所有行，第一列
>>> tensor([5., 2.])

points[None]  # 在外层增加大小为1的维度(通俗来说就是在最外面套上一层括号)
>>> tensor([[[4., 1.],
	 		 [5., 3.],
			 [2., 1.]]])
```
### 3.4 命名张量（直至2.0.0版本仍为试验性特性）
**当前此特性仍为试验性特性，重要的代码请勿使用**
张量的维度或坐标轴往往需要表示一些信息，这意味着当把张量作为索引时，我们需要记住维度的顺序并按此顺序编写索引。在通过多个张量转换数据时，跟踪哪个维度包含哪些数据可能容易出错，因此有人建议给维度指定名称

PyTorch的工厂函数，如tensor()、rand()等有一个names参数，其为一个字符串序列
```python
weights_named = torch.tensor([0.21, 0.71, 0.08], names=['channels'])
```
当已有一个张量，且想为其添加名称但又不改变现有名称时可使用refine_names()方法，使用rename()兄弟方法还可以覆盖或删除现有名称
```python
# 与索引类似，省略号（...）可允许省略任意数量的维度（在只想重命名后几个维度时可使用）
img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')

# 使用names方法查看维度名称
img_name = img_named.names
>>> ('channels', 'rows', 'columns')

# rename时可通过传入None来删除名称
ime_named = img_t.rename(..., None, 'rows', 'columns')
```
对于设置了张量名称的张量（即有两个输入的操作），除了常规维度检查，即检查张量维度是否相同外，PyTorch也将检查张量的名称，但目前为止其还没有维度自动对齐的功能，

不同名称的维度组合会出现报错，此外，如果想对命名的张量进行**操作的函数之外**使用，需要将这些张量重命名为None来删除他们的名称
```python
# align_as()可将张量按照输入的张量的维度补齐缺失的维度，并返回一个张量。补齐后的维度也将按照输入张量的“正确”的顺序排列
# 将weights_named张量按img_named张量补齐了维度
weights_aligned = weights_named.align_as(img_named)

weights_aligned.shape
>>> torch.Size([3, 1, 1])  # 后两个索引即为补齐的维度，也即'rows'和'columns'
weights_aligned.names
>>> ('channels', 'rows', 'columns')
```
## 3.5 张量的元素类型
在进行大规模数据处理和计算时，使用标准的Python数字类型可能不是最优的，原因如下：
* Python中的数字是对象。在存储少量数值时，这种封装操作并不是问题，但如果我们需要存储数百万的数据，采用封装操作会非常低效
* Python列表：为对象的顺序集合，没有为有效获取两个向量的点积或将向量求和而定义的操作；无法优化其内容在内存中的排列（因为它们是指向Python对象的指针的可索引集合）；尽管可以使用元素为列表的列表来生成高维数据，但这同样是十分低效的
* Python解释器与优化后的已编译代码相比速度很慢，在大型数字类型的数据集合上执行数学运算，使用用编译过的更低级的语言（如C）编写的优化代码可以快很多

鉴于上述原因，NumPy或PyTorch张量提供了高效的数值数据结构的底层实现和相关操作，并将这些封装在一个方便的高级API中。要实现这一点，张量内中的对象必须是相同类型的数字，PyTorch必须跟踪这个数字类型
### 3.5.1 使用dtype指定数字类型
张量构造函数通过dtype参数指定包含在张量中的数字数据类型
```python
points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], dtype=torch.float64)
```
dtype可能的取值包括：
* **torch.float32或torch.float**：32位浮点数（张量默认类型，占4bytes）
* torch.float64或torch.double：64位双精度浮点数
* torch.float16或torch.half：16位半精度浮点数
* torch.int8：8位有符号整数
* torch.unit8：8位无符号整数
* torch.int16或torch.short：16位有符号整数
* torch.int32或torch.int：32位有符号整数
* torch.int64或torch.long：64位有符号整数
* torch.bool：布尔值
### 3.5.2 适合任何场合的dtype
神经网络中的计算通常使用**32位浮点精度**执行，采用更高精度（64位）并不会提高模型精度，反而需要需要更多更多内存和计算时间。16位半精度浮点数的数据类型是由现代GPU提供的，在标准CPU中并不存在

张量可以作为其他张量的索引，此时PyTorch期望索引张量为**64位的整数**。创建一个将整数作为参数的张量（如torch.sensor([2,2])），默认会创建一个64位的整数张量

### 3.5.3 管理张量的dtype属性
可以将适当的dtype作为构造函数的参数已给张量分配正确的数字类型，也可以通过dtype来了解一个张量的dtype值
```python
double_points = torch.ones(10, 2, dtype=torch.double)

double_points.dtype
>>> torch.int16
```
我们也可以将张量创建的输出转换为正确的类型
* to()方法会检查转换是否是必要的，如必要则执行转换
```python
double_points = torch.zeros(10, 2).double()
double_points = torch.ones(10, 2).to(torch.double)
```
在操作中输入多种类型时，输入时会自动向较大类型转换。因此，如果我们想要进行32位计算，我们需要确保所有的输入最多是32位的
```python
points_32 = torch.rand(5, dtype.float)
points_64 = torch.rand(5, dtype.double)

points_32 * points_64
>>> dtype=torch.float64
```
## 3.6 张量的API
关于张量及张量之间的绝大多数操作都可以在torch模块中找到，此处列举一些操作的类型：
* 创建操作：用于构造张量的函数，如ones()、from_numpy()
* 索引、切片、连接、转换操作：用于改变张量的形状、步长或内容的函数，如transpose()
* 数学操作：通过运算操作张量内容的函数
	* 逐点操作：通过将函数分别应用于每个元素来得到一个新的张量，如abs()、cos()
	* 归约操作：通过遍历张量来计算聚合值的函数，如mean()、std()、norm()
	* 比较操作：在张量上计算数字谓词的函数，如equal()、max()
	* 频谱操作：在频域中进行变换和操作的函数
	* 其他操作：作用于向量的特定函数，或作用于矩阵的特定函数
	* BLAS和LAPACK操作：符合基本线性代数子程序规范的函数，用于标量、向量-向量、矩阵-向量和矩阵-矩阵等操作
* 随机采样：从概率分布中随机生成值的函数，如randn()、normal()
* 序列化：保存和加载张量的函数，如load()、save()
* 并行化：用于控制并行CPU执行的线程数的函数，如set_num_threads()
## 3.7 张量的存储视图
**目前tensor.storage()已经被移除而转变为tensor.untyped_storage()，因此不再进行具体代码展示**
张量的底层实现：张量中的值被分配到由torch.Storage实例所管理的**连续内存块**中。**存储区**是由数字数据组成的**一维数组**，即包含给定类型的数字（如float、int64）的连续内存块。一个PyTorch的Tensor实例就是这样一个Storage实例的视图，该实例能使用**偏移量**和每个维度的步长对该存储区进行索引。多个张量可以索引同一个存储区，即便它们索引到的数据不同。底层内存只分配一次，所以无论Storage实例管理的数据大小如何，都可以快速地创建不同的数据张量视图
## 3.8 张量元数据：大小、偏移量和步长
为了在存储区建立索引，张量依赖于下列三个信息：
* 大小（NumPy中称为shape）：一个元组，表示张量在每个维度上有多少元素
* 偏移量：存储区中某元素相对张量中的第1个元素的索引
* 步长：也是一个元组，存储区中为了获得下一个元素需要跳过的元素数量
	* 对于一个大小为(3,3)的数组，其步长为(3,1)，3代表隔3个元素才能索引到下一行，1代表隔1个元素
## 3.9 将张量存储到GPU
除了dtype，PyTorch张量还具有设备（device）的概念，即张量数据在计算机上的位置，可以通过更改将张量的存储和相关操作移动到GPU进行
```python
points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device="cuda")  # 在GPU创建一个张量
points_gpu = points.to(device='cuda')  # 通过to()将CPU上的张量复制到GPU上
points_gpu = points.to(device='cuda:0')  # 多个GPU时还可以指定使用的GPU
points_cpu = points_gpu.to(device='cpu')  # 将GPU上的张量返回CPU

# 上述方法也可等效为
pints_gpu = points.cuda()
points_gpu = points.cuda(0)
points_cpu = points_gpu.cpu()
```
## 3.10 NumPy互操作性
Pytorch张量可以非常有效地（零拷贝互操作性）与NumPy数组互换
```python
points_np = points.numpy()  # 转化为NumPy数组
points = torch.from_numpy(points_np)  # 转化为PyTorch张量
```
返回的NumPy数组大小、形状、数字类型都与代码相对应，且返回的数组与张量存储共享相同的底层缓冲区
* 如果张量此前储存于CPU中，可以有效执行numpy()方法且基本上不需要任何开销。**修改Numpy数组也将同时导致原始张量的变化**
* 如果张量此前储存在GPU中，PyTorch将把张量的内容**复制**到CPU上分配的NumPy数组中
## 3.11 广义张量
除了PyTorch张量外，还有一些其他种类的张量。通过分派机制可以将面向用户的API连接到恰当的后端函数来满足其他类型张量的需要（包括PyTorch本身使用的张量类型）
## 3.12 序列化张量（张量存储）
用户可能不希望每次运行程序时都要从头开始对模型进行训练，因此可以序列化张良对象进行保存
* PyTorch内部存储
	
	如果只使用PyTorch加载张量，可以使用这种方法快速地保存张量，但文件格式本身**不具有互操作性**，即无法使用PyTorch以外的软件读取张量。这对于一些用例来说可能是一种限制
	```python
	# 直接存储
	torch.save(points, 'save_path/points.t')
	
	# 使用with open方法
	with open('save_path/points.t', 'wb') as f:
		torch.save(points, f)
		
	# 加载张量
	points = torch.load('save_path/points.t')
	
	# 使用with open方法
	with open('save_path/points.t', 'rb') as f:
		torch.load(points, f)
	```
* h5py
	
	如果是将PyTorch引入已经依赖于不同库的现有系统时，或许需要使用互操作性的方式存储张量，此时可以使用HDF5格式和h5py库
	* HDF5是一种可移植的、被广泛支持的格式，其用于将序列化的多维数组组织在一个嵌套的键值对字典中
	* HDF5可以在磁盘上索引数据集并只访问我们感兴趣的元素。即在打开文件或请求数据集时，数据将一直保存在磁盘上。在访问后要将返回的对象传递给torch.from_numpy()函数直接获得张量（数据会被复制到张量所在的存储中）并关闭HDF5文件。HDF5文件关闭后，将会使数据集失效而无法通过键进行访问
	```python
	# Python通过h5py库支持HDF5，该库接收和返回NumPy数组格式的数据
	# 'coords'是保存到HDF5文件的一个键，用于读取时索引数组，键名可以自定义，甚至使用嵌套键
	
	import h5py
	
	# 存储
	f = h5py.File('save_path/points.hdf5', 'w')
	dset = f.create_dataset('coords', data=points.numpy())  # 将张量转化为数组
	f.close()
	
	# 加载
	f = h5py.File('save_path/points.hdf5', 'r')
	dset = f['coords']  # 通过键访问hdf5文件中特定dataset（此时数据仍在硬盘上）
	>>> dset <class: 'h5py._hl.dataset.Dataset'>
	
	last_points = dset[-2:]  # 只访问最后两个点（在硬盘上切片后进入内存）
	>>> last_points <class 'numpy.ndarray>
	
	last_points = torch.from_numpy(dset[-2:])
	>>> last_points <class 'torch.Tensor'>
	
	f.close()
	```

【拓展】HDF5文件类型
* HDF5是一种常见的跨平台数据储存文件，可以存储不同类型的图像和数码数据（图像、PDF甚至Excel等），常见的使用方法包括将预处理后的训练、测试数据进行储存
* HDF5有两种主要对象：
	* Groups：文件夹，每个HDF5文件实际就是根目录
	* Datasets：类似NumPy中的数组Array，每个Datasets又包含两类数据
		* Rawdata：原始数据
		* Metadata：元数据
			* Dataspace：原始数据的秩（Rank）和维度（dimension）
			* Datatype：数据类型
			* Properties：如Chuncked、Compressed
			* Attributes：该Dataset的其他自定义属性
【拓展】h5py使用要注意的性能问题
链接：https://zhuanlan.zhihu.com/p/34405536
```python
# 导入并创建文件

import numpy as np
import h5py
from IPython import display

f = h5py.File('learn_h5py.h5', 'w')
```
* 高效切片
	* 选择合理的切片的大小（切片在硬盘中进行，切片数越少越好）
	* 对内存中的NumPy数组切片速度很快，对硬盘上的HDF5数据集切片读取会慢
```python
f['data1'] = np.random.rand(100, 1000) -0.5
dset = f['data1']

# 数据读取：对HDF5文件进行切片读取时，因为切片在硬盘中以慢速进行，因此设置切片数越少越好
for ix in range(100):
	val = dset[ix, :]  # 切取第ix列，切取过程在硬盘上进行，但索引取到的对象为'numpy.ndarray'
	val[val < 0] = 0  # 将ix列中低于0的值设置为0
    dset[ix, :] = val  # 重新储存
```
* 将数据读入一个已存在的数组：使用Dataset对象中的方法read_direct()，可将HDF5数据直接填入已经存在的数组并自动进行类型转换
	* 标准切片：h5py每次都会在内部创建一个out数组用于保存切片数据，用完即扔（重复创建内存空间，运行多次或数组较大时可能会产生瓶颈）
	* read_direct：用户主动创建内存空间，h5py只负责向内填充数据
```python
# 标准切片
out = dset[:, 0:50]
means = out.mean(axis=1)  # 计算均值

# read_direct
out  = np.empty((100,50), dtype=np.float32)  # 用户主动创建空数组
dset.read_direct(out, np.s_[0:, 0.50])
means = out.mean(axis=1)
```
* 分块储存
	* HDF5默认使用连续存储，如果我们储存100张640*480的图片（形状为(100,480,640)），会将数据以具有640个元素的“扫描线”的形式一条一条保存在磁盘上。数据将以640字节分块储存，这与数据集最后一维的长度相符。当我们要读取第一张图像时，我们从磁盘上读取了480个这样的“块”，全在一个大块里。这样读取**整个图像**的效率非常高。连续储存的好处在于**磁盘上的布局和数据集的形式直接相关**，最后一维上索引向前进一步意味着磁盘上的数据向前进一步。如果数据被存储在一起，通常读起来更快
		```python
		# 切片代码
		image = dset[0, :, :]
		```
	* 但如果只处理图像的部分区域，如第一幅图像角落上一个64*64的像素块时，这种切片方式可能不妙。程序不得不从480个区域收集数据而非一次性读取一整块连续数据。如果想要对每一幅图像都进行这个操作，则同样需要访问所有的区域。这个问题的根本原因在于**默认的连续存储机制跟我们的访问模式不匹配**。解决这个问题需要在保留数据集形状的同时又告诉HDF5对64*64的像素块进行访问优化。这就是HDF5的**分块储存**，它允许你指定最适合你访问模式的N维形状。当需要对磁盘写入数据时，HDF5将数据分成指定形状的块，然后再将它们扁平地写入磁盘。这些块被存放在文件的各地，其坐标由一个B树索引
		```python
		# 使用chunks关键字将数据分成指定的形状
		# 分块形状的值在数据集创建时就被固定且无法更改
		dest = f.create_dataset('chunked', shape=(100,480,640), dtype=np.unit8m chunks=(1, 64, 64))
		
		dest.chunks  # 检查chunks查看分块形状，若为None则意味着没有分块储存
		>>> (1, 64, 64)
		```
* 扩展数据集
	* 在数据集创建时我们为了灵活性会将数据集定义为**可变形数据集**，可变数据集在日常工作中是经常使用的，在获取新的数据后，我们会将其添加到已有的数据集中
	* 当创建了可变形数据集时，分块储存功能就会被自动打开，若未手动指定分块形状，h5py会自动进行分块。对数据集使用特定的自定义分块形状更有助于于性能的优化
	```python
	# 创建不可变数据集
	dset = f.create_dataset('chunck1' shape=(100,480), dtype=np.unit8)
	dset.chunks
	>>> None
	
	# 创建可变形的数据集但不指定chunk
	dset = f.create_dataset('chunck2', shape=(100,480), dtype=np.unit8,
							maxshape=(None, 480))
	dset.chunks
	>>> (50, 240)
		
	# 创建可变形的数据集且指定chunk
	dset = f.create_dataset('chunk3', shape=(100,480), dtype=np.unit8,
							maxshape=(None, 480),
							chunks=(1, 480))
	dset.chunks
	>>> (1, 480)
	```
	* 两种添加数据的方法：第二种理论快一些，因为调用resize()的次数少
		```python
		dset1 = f.create_dataset('mydata1', shape=(1,1000), maxshape=(None, 1000))
		dset2 = f.create_dataset('mydata2', shape=(50000,1000), maxshape=(None, 1000))
		
		
		# 方法1：简单地逐行增加
		def add_mydata_1(arr):
			dset1.resize((dset1.shape[0]+1, 1000))
			dset1[-1, :] = arr
		
		# 方法2：超额分配并最终消减超出的部分
		nmydata = 0
		def add_mydata_2(arr):
			global nmydata
			dset2[nmydata, :] = arr
			nmydata += 1
		
		def done():  # 消减超出的部分
			dset2.resize((nmydata, 1000))
		```