# 4 使用张量表征实际数据
## 4.1 处理图像
图像被表示为一个排列在具有高度和宽度的规则网格的标量集合中，其中高度和宽度以像素为单位。每个网格点（像素）具有一个标量用于表征灰度图像，或具有多个标量（如不同的颜色、不同的特征等）。代表单个像素值的标量通常使用8位整数编码，也可以使用更高精度的编码，如12位或16位，这允许数字具有更大的范围或更高的灵敏度

任何输出NumPy数组的库都可以获得一个PyTorch张量，唯一需要注意的是**维度布局**，如处理图像数据的PyTorch模块要求张量排列为 $ C\times H\times W $（分别表示通道、高度、宽度），批量储存时，张量排列可为 $N \times C \times H \times W$（$N$ 表示图像数量）
### 4.1.1 添加颜色通道
最常见的将颜色编码为数字的方法为RGB，其中颜色由三个数字定义，分别代表红、绿、蓝的强度。可以将一个颜色通道看作一个灰度强度图，只对应所讨论的颜色
### 4.1.2 加载图像文件
**示例使用的是imageio模块，实际还有多种加载图像的方法**
```python
import imageio

# 导入图像
img_arr = imageio.imread(path)  # 读入以后是一个NumPy数组
ima_arr.shape
>>> (720, 1280, 3)  # 分别对应H、W和C
```
### 4.1.3 改变布局
```python
# permute()方法使每个新维度利用旧维度得到一个合适的布局
# 该操作并未复制张量数据，而是让out使用与img相同的底层储存，且在张量级别处理大小和步长 
img = torch.from_numpy(img_arr)
out = img.permute(2, 0, 1)  # 先布局通道2（C），再布局通道0（H）和通道1（W）
```
包含多幅图像的张量创建
```python
# 另一种构建方式：预先分配一个适当大小的张量，使用从目录中加载的图像填充它
batch_size = 3  # 图像数量
batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)  # 预先创建张量

for i, filename in enumerate(filenames):
	img_arr = imageio.imread(filename)
	img_t = torch.from_numpy(img_arr)
	img_t = img_t.permute(2, 0, 1)
	
	img_t = img_t[:3]  # 只保留导入的前三个通道（有时图像还有一个表示透明度的alpha通道）
	batch[i] = img_t  # 将数据填入
```
### 4.1.4 数据归一化
对于神经网络，**当输入数据的范围为0~1或-1~1时，神经网络表现出最佳的训练性能**，这是由其构建块的定义方式所决定的。因此我们要做的一件典型的事情就是将张量转化为浮点数并将其归一化
* 方法1：像素值除以255（8位无符号二进制数可表示的最大数字）
* 方法2：计算输入数据的均值和标准差，对其进行缩放，使每个通道的均值为0，标准差为1
		```python
		n_channels = batch.shape[1]
		for c in range(n_channels):
			mean = torch.mean(batch[:, c])
			std = torch.std(batch[:, c])
			batch[:, c] = (batch[:, c] - mean) / std
	```
我们还可以对输入执行其他操作，如旋转、缩放和裁剪等。这些可能有助于训练，也可能使任意输入符合网络的输入要求
## 4.2 三维图像：体数据
通过将单个二维切片堆叠成一个三维张量，我们可以构建表示一个物体的三维解剖结构的体数据

存储体数据的张量和图像数据没有根本区别：在通道维度之后新增了一个额外的维度，即深度。存储体数据的张量为5维张量：$N \times C \times D \times H \times W$

（具体读入体数据和张量转换操作可见原文）
## 4.3 表示表格数据
表的每一行包括一个样本或记录，每一列则包含关于样本的一部分信息。列可以包含数字，也可以是表示样本属性的字符串，因此表格数据通常是**异构数据**：不同的列具有不同的类型

但需要注意的是，PyTorch张量是**齐次**的。虽然张量也可以支持整数和布尔型，但PyTorch中的信息通常被编码为浮点数（这种数字编码是经过深思熟虑的，因为神经网络是一种数学实体，它以实数为输入，通过连续应用矩阵乘法和非线性函数产生实数作为输出）
### 4.3.1 使用真实的数据集
本节中使用的数据集来自GitHub的awesome-public-datasets中的tabular-wine数据集，其包含如下12列（前11列为特征，最后一列为得分）：fixed acidity、volatile acidity、citric acid、residual sugar、chlorides、free sulfur dioxide、total sulfur dioxide、density、pH、sulphates、alcohol、quality
### 4.3.2 加载张量
三种加载csv的方法，加载后可转化为PyTorch张量
* Python自带的csv模块：可以读取任意类型数据（数字、字符串）
	```python
	from csv import reader
	import numpy as np
	
	with open(filename, 'rt') as raw_data:
		readers = reader(raw_data, delimiter=',')
		data_np = np.array(list(readers)).astype('float')  # 转化为列表再转化为array
	>>> numpy.ndarray
	```
* NumPy：打开纯数字csv
	```python
	import numpy as np
		
	with open(filename, 'rt') as raw_data:
	    # 设置生成数组类型为float32，设置跳过标题行（skiprows=1）
		data_np = np.loadtxt(raw_data, dtype=np.float32, delimiter=',', skiprows=1)
	>>> numpy.ndarray
	```
* **Pandas**：最节省时间和内存，但是转化为PyTorch张量需要额外的操作
	```python
	from pandas import read_csv
		
	names = ['a', 'b', ...]  # 列名
	data = read_csv(filename, names=names)  # 包含列名
	>>> pandas.core.frame.DataFrame
	
	# 转化为PyTorch张量：先转化为ndarray，再转化为张量
	data = np.array(data)
	data_t = torch.tensor(datas)
	```

【拓展】数据类别：连续值、序数值和分类值
* 连续值：严格有序，不同值之间的差异具有严格的意义
	* 比例尺度：如某个物体的质量或距离是另一个物体的2到3倍
	* 区间尺度：如6:00是3:00的两倍是不合理的
* 序数值：对连续值的严格排序仍存在，但值之间的固定关系不再适用。我们能对这些值进行排序，但无法对他们进行“数学运算”
	* 如小杯为1，中杯为2，大杯为3，并不知道大杯比中杯大了多少。若转化为实际体积，如小杯为8，中杯为12，大杯为24，则它们将转化为区间值，但将小杯和大杯取均值不会得到中杯
* 分类值：没有排序意义，也无数字意义，通常只是分配任意数字的可能性的枚举
	* 如咖啡分为1，牛奶分为2和咖啡分为5，牛奶分为-3并不会有明显变化
### 4.3.3 表征分数
在tabular-wine数据集中，我们可以将quality视为一个连续变量，将它当作一个实数执行回归任务，也可以将其作为一个标签作为分类任务的目标有效值
```python
# 区分特征和标签
data = wineq[:, :-1]  # 取所有行和除最后一列以外的所有列，即特征
target = wineq[:, -1]  # 取所有行和最后一列，即标签
>>> tensor([6., 6., ..., 7.])
```
将标签转化为张量有两种方法，一种是将其转换为整数向量。但若目标张量是字符串标签，那么给每个字符串分配一个整数后，可以采用4.3.4的方法
```python
target = wineq[:, -1].long()
>>> tensor([6, 6, ...7])
```
### 4.3.4 独热编码
将10个分数分别编码到一个由10个元素组成的向量中，其中一个元素设置为1，其他元素设置为0，这样每个分数都可以获得一个不同的索引。分数1可以表示为（1,0,0,0,0,0,0,0,0,0)，分数5可以表示为(0,0,0,0,1,0,0,0,0,0)。

独热编码与整数编码有明显区别
* 保存在整数向量中时，可以对quality进行排序，同时也会让quality之间产生某种距离（如1到3的距离与2到4的距离相同）
* 但如果quality是完全离散的，那么采用独热编码更合适，因为没有隐含的顺序与距离。独热编码也适用于quality介于整数之间的值，如2.4，即当quality为两个值中的某一个值时对程序毫无影响的时候。此外，如果要将分数作为网络的分类输入，就必须将其转换成一个独热编码张量

```python
# 使用scatter_()方法得到独热编码
# 对于每一行，取目标标签的索引，并将其作为独热编码张量的列索引。将独热编码张量中该索引处的值设置为1.0，从而得到一个对分类信息进行编码的张量
# dim：1，在第一个维度进行操作
# index：targer.unsqueeze，输入的索引
# src：1.0，输入的向量
target_onehot = torch.zeros(target.shape[0], 10)  # 指定行、列（分类数量，10种）数的全0张量

target_onehot.scatter_(1, target.unsqueeze(1), 1.0)
>>> tensor([0., 0., ..., 0.],
		   [0., 0., ..., 0.],
		   ...,
		   [0., 0., ..., 0.])

# unsqueeze()为目标新增了一个虚拟维度，其在一维张量中添加一个维度将其变为二维张量而不改变其内容，只是使用一个额外的索引来访问元素。如使用target[0]访问target第1个元素，使用target_unsqueezed[0,0]访问与之对应的扩充张量
target.unsqueeze(1)
>>> tensor([[6],
            [6],
            ...,
            [7]])
```
```python
# 另一种调用独热编码的方法
import torch
target_onehot = torch.nn.functional.one_hot(targets, num_classes=10)
```
### 4.3.5 何时分类
有序数据的处理
* 是否是连续数据
	* 是，直接使用数据（4.3.2）
	* 否，是否是有序数据
		* 是，是否**顺序优先**
			* 是，则视为连续数据处理：引入一个任意的距离概念
			* 否，则视为分类数据处理：损失排序部分，只保留类别概念
		* 否，是否是分类数据
			* 是，则使用独热编码或者嵌入（4.3.3、4.3.4）
### 4.3.6 寻找阈值
```python
# 查看target中哪些行对应的分数小于或等于3
bad_indexes = target <= 3  # 比较函数，输出的是bool类型
>>> torch.bool

bad_data = data[bad_indexes]  # 提取分数小于或等于3的结果
```
## 4.4 处理时间序列
本节使用的数据集为2011-2012年华盛顿自行车共享系统每小时的自行车租赁数量以及天气和季节信息（共包含17项内容）本节数据处理的目标是将一个平面的二维数据集转换为三维数据集，即通过将每个样本的日期和时间分离为单独的轴，将一维多通道数据集转换为二维多通道数据集
### 4.4.2 按时间段调整数据
当把2年的数据集按天划分时，得到了序列长度为 $L$（24小时，代表小时），样本数量为 $N$（N天，代表天数）的集合 $C$（17个特征，一维的特征向量）。该时间序列数据集是一个维度为3，形状为 $N \times C \times L$ 的张量
* 需要注意的是，为了使时间序列有意义，其中不能有间隙

现在，让我们创建一个每日骑行次数序列以及其他外生变量数据。为获得每日小时数据集，需要以24小时为单位查看同一个张量
```python
# 原始数据集
bikes.shape
>>> torch.Size([17520, 17])  # 17520小时，17列
bikes.stride()  # 指定维度中从一个元素跳到下一个元素所需的步长（维度顺序与shape一致）
>>> (17, 1)
 
# 数据调整
# 在一个张量上调用view会返回一个新的张量，可在不改变存储的情况下改变维度和步长的信息，即可以零成本重新排列张量。在调用时需要为返回的张量提供形状（可以为任意维度）
# view中的-1代表自动调整-1所在维度上的元素个数，以保证元素总数不变，即在指定后两个维度为24、17时，自动计算维度重排时第一个维度应当有多少元素
daily_bikes = bikes.view(-1, 24, bikes.shape[1])
>>> torch.Size([730, 24, 17])
daily_bikes.shape
>>> (408, 17, 1)  # 延第一个维度前进的步长408=17*24

# 转置维度
daily_bikes = daily_bikes.transpose(1, 2)  # 转置第2、3个维度
daily_bikes.shape
>>> torch.Size([730, 17, 24])
daily_bikes.stride()
>>> (408, 1, 17)
```
### 4.4.3 准备训练
（具体过程可见原文）
## 4.5 表示文本
深度学习在自然语言处理（Natural Language Processing, NLP）领域取得了巨大的成功，特别是循环神经网络（Recurrent Neural Network, RNN）类模型。这类模型使用那些反复使用新输入和以前模型输出的组合的模型，它们已经成功地应用于文本分类、文本生成和自动翻译系统。以前的NLP工作负载的特点是复杂的多级管道，其中包括编码语言语法的规则，而目前，使用最先进的技术在大型语料库上从头开始训练网络，可以让这些规则从数据中浮现出来

本节的目标是将文本转化为神经网络可以处理的内容——数字张量。之后，我们可以为文本处理工作选择正确的架构，从而使用PyTorch实现NLP
### 4.5.1 将文本转化为数字
网络对文本的操作层次
* 字符级：一次处理一个字符
* 单词级：一次处理一个单词。单词是网络可以看到的最细粒度的实体

无论在字符级别还是单词级别，将文本信息编码为张量形式的技术都是一样的，即**独热编码**
### 4.5.2 独热编码字符
编码是一个相当大的话题，此处在实际操作前我们需简单了解部分。每个字符都由一个代码表示：一个长度适当的比特序列，以便于每个字符都能被唯一标识。最简单的编码之一是ASCII。ASCII使用128个整数编码128个字符

【拓展】Unicode：Unicode将已知字符（更大范围的字符）映射到数字，由特定编码提供的这些数字的位数表示。比较流行的有UTF-8、UTF-16和UTF-32，其中的数字分别表示8位、16位或32位整数序列

对字符进行独热编码时，将独热编码限制为对要分析的文本有用的字符集是很有效的（原文示例位纯英文文本，因此使用ASCII编码是安全的），如可将字符都转化为小写、剔除标点、数字或其他与我们期望的文本类型无关的字符。这可能会影响训练的神经网络，也可能不会影响

我们需要解析文本中的字符，并为每个字符提供独特编码，每个字符都由一个**长度等于编码中不同字符数的向量**表示，该向量除了与编码中字符位置对应的索引为1，其他位置都为0
```python
# 以文本的一行为例进行展示
line = '"Impossible, Mr Bennet, impossible, when I am not acquainted with him"'

letter_t = torch.zeros(len(line), 128)  # 创建一个能容纳整行字符的独热编码的总数的张量
letter_t.shape
>>> torch.Size([70, 128])

for i, letter in enumerate(line.lower().strip()):  # 对每个字符进行操作
    # ord(): 返回字符的ASCII编码或Unicode编码
    letter_index = ord(letter) if ord(letter) < 128 else 0  # if else的奇妙写法
    letter_t[i][letter_index] = 1
```
### 4.5.3 独热编码整个词
单词级编码也可沿着词序列，即行张量，建立一个词汇表和独热编码句子。但由于词汇表会有很多单词，这将会产生非常宽的编码向量。在建立具有较多单词的独热编码时，可以采用字典的形式：单词为键，整数为值

在字符级编码和单词级编码之间需要进行权衡，也可以寻求、建立一些新的方法（如4.5.4）
* 字符级
	* 许多语言中，字符比单词要少很多，表示字符只需几个类别而单词则需要很多类别
	* 在实际应用中，可能还需要处理“字典”中不存在的单词
* 单词级
	* 单词可以提供更多的信息
### 4.5.4 文本嵌入
当要编码的数据量很大时，独热编码就无能为力了。我们当然可以使用如4.5.2的方式限制单词量，但每遇到一个新词，就要给向量添加一个新的列，这意味着要为模型添加一组新的权重以解释新的词汇条目

我们可以用浮点数向量来代替多个0和一个1向量。例如，100个**浮点数**组成的向量确实可以表示大量的单词，诀窍就是找到一种有效的方法，将单个单词映射到这个100维空间中，以便于下游学习，这就叫做“嵌入”

原则上可以为每个单词生成一组由100个随机浮点数组成的集合，这实际也是可行的。因为我们可以将一个非常大的词汇塞进100个数字中，但这舍弃了**任何基于意义或上下文的单词之间的距离概念**。一个理想的解决方案是以这样一种方式生成嵌入，即把在相似上下文中使用的单词映射到嵌入的附近区域（例子见原文）

这种工作是可以自动化实现的：通过处理大量的有机文本语料库，可以生成上述类似的嵌入，但与上述原文例子相反，坐标轴并不直接映射到概念上，而是概念上相似的单词映射在嵌入空间的邻近区域，其轴时任意的浮点维度

嵌入通常是使用神经网络生成的，它试图从句子中的邻近单词（上下文）中预测一个单词。这种情况下，我们可以从一个独热编码的单词开始，并使用（通常相当浅的）神经网络来生成嵌入。一旦可用，就可以用于下游任务。由此产生的嵌入一个有趣的方面是，相似的单词最终不仅聚集在一起，而且还与其他单词具有一致的空间关系
### 4.5.5 文本嵌入作为蓝本
本书中不会使用文本和文本嵌入，作者认为文本的表示和处理方式可以看作是处理分类数据的一个例子。在非文本应用中，我们通常不具备预先构造嵌入的能力，但我们将从我们先前摒弃的随机数开始，考虑在学习问题中改进它们。这是一种非常标准的技术，因此对于任何分类的数据，嵌入都是一种重要的、替代独热编码的方法。此外，在处理文本时，解决手头问题的同时改进预先学习的嵌入已经成为一种常见的做法

当对观察值的**共现**感兴趣时，我们之前看到的词嵌入可以作为一种蓝本。如推荐系统：喜欢我们的书的顾客也购买了xxx（使用顾客已经接触过的商品作为上下文来预测还有什么东西会引起顾客的兴趣）。同样，处理文本可能是处理序列时最常见、最广泛研究的任务，如在处理时间序列任务时，我们可能会从自然语言处理中找灵感