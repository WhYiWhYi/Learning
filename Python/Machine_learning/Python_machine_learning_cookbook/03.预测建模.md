# 3 预测建模

## 3.1 预测建模简介
预测建模是一种用于**预测系统未来行为**的分析技术，其由一群**能够识别独立输入变量与反馈目标关联关系的算法构成**。在预测建模中，需要收集已知的数据训练模型，在经过一些指标的验证后，可以用其预测未来发生的事情
* 预测模型是**由数学语言或公式所描述的事物间的数量关系**，它对预测准确度有极大影响。任何一种具体的预测方法都是以特定的数学模型为特征
* 预测建模是由若干**可能对系统行为产生影响的特征**构建的，在构建模型的时候，需要将所有可能的特征都考虑进去
## 3.2 使用SVM建立线性、非线性分类器
### 3.2.1 支持向量机（SVM）简介
支持向量机（SVM）模型是一种监督学习算法，用于对二值和分类响应数据进行分类

数学化描述SVM：
* 训练样本数据：n个样本（x~1~, x~2~, ..., x~n~），p个输入特征（对于样本i有：X~i~ = (x~i1~, x~i2~, ..., x~ip~)^T^），一个输出 y
* 目的：以训练样本为研究对象，在样本的特征空间中找到一个超平面 **W^T^X+b = 0** (W = ω~1~,ω ~2~, ...,ω ~p~，针对每一个特征，都有对应的W^T^X)，将不同类别的样本分开
	* X是样本的向量表示，以二维数据（具有两个输入特征）为例：X^T^ =  (x1, x2)。由于向量一般为列向量，当以行向量表示时，就加上转置**T**
	* W^T^是一组行向量，是未知参数，可以将其中的每一项 ω~i~ 理解为 x~i~ 的系数
	* 行向量和列向量相乘（W^T^X）得到一个**1\*1**的矩阵，即一个实数

以两个特征为例说明如何寻找最佳线性分类器（原理介绍）：
* 在理想状态下，二维中的两类点是**完全线性可分**的，但此时可以有多条线能够分割数据，此时就需要确定哪一条是最佳的
* SVM中，对最好分类器的定义是**最大边界超平面，即距两个类别的边界观测点最远的超平面**
![](E:\SynologyDrive\READING\CS\note\机器学习\img\3.2.1.SVMtheory.png)
* 从上图可见，在寻找最佳分类器时，只会考虑A类中离B类最近的点和B类中离A类最近的点，即1、2、3；a、b、c这些点。因为**最大边界超平面仅取决于两类别的边界点**，这些边界点即称**支持向量**
* 详细原理可见：https://zhuanlan.zhihu.com/p/74484361

SVM的优势
* SVM的优势在于解决小样本、非线性和高维的回归和二分类问题
* **小样本**：与问题的复杂度相比，SVM要求的样本数相对较少
* **非线性**：SVM擅长应对样本数据线性不可分的情况，主要通过**核函数和松弛变量**实现（SVM精髓）
* **高维**：即便数据为高维数据，但SVM产生的分类器很简洁，使用的样本信息很少（仅使用支持向量），可以**有效避免过度拟合**
### 3.2.2 使用SVM构建线性分类器
```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC  # 分类器SVC，回归器SVR
from sklearn.metrics import classification_report
import utilities  # 用于快速绘制简单图形
import matplotlib.pyplot as plt

x = np.array([])  # 原始数据集
y = np.array([])

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=5)

# 使用线性核函数（linear kernel）初始化一个SVM对象
params = {'kernel': 'linear'}
svm_classifier = SVC(**params)

# 训练分类器，使用分类器预测测试集结果，并使用图形展示
svm_classifier.fit(x_train, y_train)
y_predict = svm_classifier.predict(x_test)

utilities.plot_classifier(svm_classifier, x_train, y_train, 'Training dataset')
plt.show()
utilities.plot_classifier(svm_classifier, x_test, y_test, 'Test dataset')
plt.show()

# 查看分类器为测试集数据生成的分类报告
target_names = ['class-' + str(int(i)) for i in set(y)]
print(classification_report(y_test, y_predict, target_names=target_names))
```
### 3.2.3 使用SVM构建非线性分类器

对于线性不可分的数据，需要使用不同的核函数建立非线性分类器。当想要表示两种类型数据的曲线边界时，可以考虑多项式函数，也可以考虑径向基函数（Radial Basis Function, RBF）
```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report
import utilities  # 用于快速绘制简单图形
import matplotlib.pyplot as plt

# 使用多项式函数初始化SVM对象
# degree：表明使用一个n次多项式方程。增加方程的次数可以使曲线更加弯曲，但训练时间也会更长
params = {'kernel': 'poly', 'degree': 3}

# 使用径向基函数初始化SVM对象
params = {'kernel': 'rbf'}
```
## 3.2.4 解决类型数量不平衡问题
在真实数据中，如果不同类型数据点的数量不均衡（甚至相差很大），训练的分类器就会有偏差，边界线也难以反映数据的真实特性。为此，需要慎重考虑这种差异性并加以调和
```python
# 以3.2.2所使用的代码为参照
# 在params参数中增加“class_weight”参数
# class_weight：统计不同类型数据点的数量，调整权重，降低类型不平衡对分类效果的影响
params = {'kernel': 'linear', 'class_weight': 'auto'}
```
### 3.2.5 获取SVM分类器的置信度
训练SVM获取对未知数据进行分类的置信度
```python
input_points = np.array([[2, 1.5], [8, 9], [4.8, 5.2], [4, 4], [2.5, 7], [7.6, 2], [5.4, 5.9]])

# 计算数据点与边界的距离
for i in input_points:
	print(i, "-->", classifier.desicion_function(i)[0])

# 测量置信度
# 在params参数中增加“probablility”参数，用于告诉分类器在训练时要计算出概率
params = {'kernel': 'rbf', 'probability': True}
svmp_classifier = SVC(**params)

svmp_classifier.fit(x_train, y_train)
for i in input_points:
	print(i, "-->", classifier.predict_proba(i)[0])  # 计算置信度
>>> [2. 1.5] --> [0.05126939, 0.94873061]  # 两个值分别对应两类标签的置信度
>>> [8, 9] --> [0.11146888, 0.8853112]
```
### 3.2.6 寻找最优超参数（以SVM为例）

在加载、分配数据为训练集和测试集后，使用**交叉验证**设置**在指定参数内寻找最优超参数**
```python
from sklearn.svm import SVC
from sklearn.model_selecion import GridSearchCV  # 导入寻找最佳超参数的模块

# 定义要试验的参数
parameter_grid = [{'kernel': ['linear'], 'C': [1, 10, 50, 600]},
                  {'kernel': ['poly'], 'degree': [2, 3]},
                  {'kernel': ['rbf'], 'gamma': [0.01, 0.001], 'C':[1, 10, 50, 600]}]
# 定义需要计算的指标
metrics = ['precision', 'recall_weighted']

# 开始搜索最佳参数（使用不同参数构建分类器）
for metric in metrics:
	# 传入要试验的参数、要计算的指标，训练模型
	classifier = GridSearchCV(SVC(C=1), parameter_grid, cv=5, scoring=metric)
	classifier.fit(x_train, y_train)

# 查看指标得分
for params, avg_score, _ in classifier.grid_scores_:  # _代表无用参数
	print(params)  # 打印参数
	print(round(avg_score, 3))  #打印对应参数的模型分数（保留三位小数）

print("Highest scoring parameter: ".format(classifier.best_params_))  # 打印获得最高模型分数的参数值
```
## 3.3 SVM建立事件预测器示例：预测大楼进出楼门的人数

### 3.3.1 任务介绍
预测大楼是否举行活动
* 文件：buiding_event_binary.txt
* 特征（按序依次）：星期、日期、时间、离开大楼人数、进入大楼人数
* 标签：是否有活动

预测大楼将举行什么活动
* 文件：building_event_multiclass.txt
* 特征（按序依次）：星期、日期、时间、离开大楼人数、进入大楼人数
* 标签：活动类型

### 3.3.2 预测是否发生活动（流程类似2.6.1）
```python
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC

# 导入文件
inputfile = "building_event_binary.txt"
x = []
with open(inputfile) as file:
    for line in file.readlines():
        data = line[:-1].split(",")  # 不要最后一项：最后一项为换行符
        x.append([data[0]] + data[2:])  # 依据分析目的（要求）导入所需数据
x = np.array(x)

# 转换输入数据格式（编码）
label_encoder = []
x_encoded = np.empty(x.shape)  # 创建与导入的x形状相同数组，用于储存转化为数值的特征值

for i, item in enumerate(x[0]):  # 获取列类型只需要对第一行进行操作即可
    if item.isdigit():  # 如果该列为数字型，无需更改
        x_encoded[:,i] = x[:,i]
    else:
        label_encoder.append(preprocessing.LabelEncoder())  # 对特定列创建新的编码器
        x_encoded[:,i] = label_encoder[-1].fit_transform(x[:,i])  # 使用特定编码器编码特定列

x = x_encoded[:,:-1].astype(int)  # 储存所有特征项（除最后一列），并转化为整数
y = x_encoded[:,-1].astype(int)  # 储存标签项（最后一列，并转化为整数）

# 训练分类器（使用径向基函数、概率输出、类型平衡）
params = {"kernel": "rbf", "probability": True, "class_weight": "auto"}
svm_classifier = SVC(**params)
svm_classifier.fit(x, y)

# 进行交叉验证
accuracy = model_selection.cross_val_score(svm_classifier, x, y, scoring='accuracy', cv=3)
print("Accuracy is {} %".format(str(round(100*accuracy.mean(), 2))))  # 交叉验证产生多个结果，需取均值
>>> Accuracy is 89.88%


# 使用测试数据进行测试（单一样本）
inputdata = ['Tuesday', '12:30:00', '21', '23']
inputdata_arary = np.array(inputdata).reshape(len(inputdata), 1)  # 单一样本进行reshape，行转列，下同
inputdata_encoded = np.array([-1] * len(inputdata)).reshape(len(inputdata), 1)

count = 0  # 由于不是每一列都进行编码，因此count用于对编码列进行计数
for i, item in enumerate(inputdata):  # 对每一列（特征）进行操作
    if item.isdigit():
        inputdata_encoded[:,i] = int(inputdata[i])  # 转化为int
    else:
        inputdata_encoded[:,i] = label_encoder[count].transform.(inputdata[i])
        count += 1
# 若为单样本（单行），则需转化为二维数组（训练时投入的是二维数组）
inputdata_encoded = inputdata_encoded.reshape(1,-1)

y_predict = svm_classifier(inputdata_encoded)  # 进行数值预测
y_predict_class = label_encoder[-1].inverse_transform(y_predict)  # 使用最后一列的编码器（-1）解码标签
print("Output class is: {}".format(y_predict_class))
>>> Output class is: event
```