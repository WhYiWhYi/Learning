# 1 监督学习

## 1.2 数据预处理技术

真实世界中的原始数据是机器学习算法无法理解的，为了使机器学习理解算法原始数据，需要对数据进行预处理，接下来将以示例数据为例，介绍几种常见的数据预处理技术
```python
# 示例数据：一行一个样本，一列一个特征
data = np.array([[3, -1.5, 2, -5.4], [0, 4, -0.3, 2.1], [1, 3.3, -1.9, -4.3]])
```
#### 1.2.1 均值移除 (Mean removal)
在某些模型中（如回归算法、KNN算法等），各个特征的**基准值**和**分散度**不同可能会大大影响模型的预测效果，因此在应用这些模型的时候，要确保**每一个特征列的数值都在类似的数据范围之间，防止某一个特征列数据数值过大而影响结果**。均值移除即可做到**消除特征彼此间的偏差**，使每一个特征的**均值为0，方差为1**

均值移除
* 以**特征**为单位进行操作
* 标准化的一种，为**z-score标准化 (zero-maen normalization)**
* 原理：均值化为0，标准差化为1（符合标准正态分布）
* 数学方法：对于特征数据集X=[a1, a2, a3, ...]，其均值为μ，标准差为σ。则每一个值均值移除的方法为：**(ai-μ)/σ**
```python
# 普通方法（依据原理实现）
for col in data.T:  # 因为特征在列上，因此处理需要进行array的转置
	col_mean = col.mean()
	col_std = col.std()
	col -= col_mean
	col /= col_std
```
一般使用调用sklearn库方法实现均值移除
```python
from sklearn import preprocessing

data_standardized = preprocessing.scale(data)  # 传入均值移除目标array
```
### 1.2.2 范围缩放 (Scaling)
范围缩放用于**统一数据矩阵中不同特征的最大值和最小值范围（通常为1-0）**。其数据处理效果与均值移除类似，也是使**每一个特征列的数值都在类似的数据范围之间**。将不同特征缩放到一个指定的最大、最小值间可以对方差非常小的属性增强其稳定性，也可维持稀疏矩阵中为0的条目

范围缩放
* 以**特征**为单位进行操作
* 标准化的一种，为**0-1标准化 (0-1 normalizzation)**
* 原理：离差标准化，进行线性变换使范围变为指定范围，通常为0-1
* 数学方法：对于特征数据集X=[a1, a2, a3, ...]，其最大值为amax，最小值为amin。则每一个值范围缩放的方法为：**(ai-amin)/(amax-amin)**
```python
from sklearn import preprocessing

data_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))  # 创建范围缩放对象（器），缩放范围为0-1
data_scaled = data_scaler.fit_transform(data)  # 进行范围缩放
```
### 1.2.3 归一化 (Normalization)
数据归一化用于需要对样本的**特征向量（向量：指一个样本所含的多个特征）**的值进行调整时，以**保证每个特征向量的值都缩放到相同的数值范围**。这一方法常用于确保数据没有因为特征的基本性质而产生较大差异，确保**所有特征的数据处于同一数量级**。提高不同特征数据的可比性

归一化：
* 以**样本**为单位进行操作
* 原理：用特征占比表示特征大小，调整方式有两种：调整为**L1范数**或**L2范数**，机器学习常用方法为调整为**L1范数**
* 数学方法：对于样本数据集（包含多个特征）Y=[f1, f2, f3, ...]。分子为fi，在L1范数中，分母为：Σ|fi|；L2范数中，分母为：√(Σ|fi|)2
```python
from sklearn import preprocessing

data_normalized = preprocessing.normalize(data, norm="l1")  # 传入归一化目标array，调整方式为l1
```
### 1.2.4 二值化 (Binarization)

二值化为将数值特征向量转换为布尔类型向量，也即只有True和False两类。二值化方法对于所有的特征值使用同一个阈值标准，因此对于特征值差异较大的特征，不适合使用二值化的方法。此外，注意**二值化方法不可逆**，若希望可逆，则可考虑使用独热编码

二值化
* 对每一个特征值进行操作
* 原理：设定阈值，用0和1来表示各个特征值相对于某个给定阈值高于或低于它的元素
```python
from sklearn import preprocessing

data_binarizer = preprocessing.Binarizer(threshold=1.4)  # 创建二值化对象，设定阈值为1.4
data_binarized = data_binarizer.transform(data)
```
### 1.2.5 标签编码 (Label Encoding)
监督学习中需要处理各种各样的标签，可能是数字、可能是单词。大多数情况下，为了方便人们理解，标签多采用单词的形式。标签编码就是**把单词转化为数值形式**，使算法懂得如何操作标记

标签编码
* 以**特征**为单位进行操作
* 将离散型变量转化为连续的数值型变量，即对不连续的数字或文本进行编号
* 注意：**不同的特征编码表不同且相互独立**，**编码和解码要使用对应特征的编码表**
* 标签编码有时候会产生一些限制，如有[dog,cat,dog,mouse,cat]可转换为[1,2,1,3,2]。这里就产生了一个奇怪的现象：dog和mouse的平均值是cat（算法中要对特征数值进行一定计算的）
```python
import numpy as np
from sklearn import preprocessing

label_classes = np.array(['audi', 'ford', 'ford', 'bmw', 'toyota', 'ford', 'audi'])  # 待编码特征

# 创建标签编码对象：编码和解码都需要使用
# 如果对不同的特征进行编码，需要使用不同的编码器
label_encoder = preprocessing.LabelEncoder()

# 传入目标array进行编码，同时构建标签编码表字典（训练时推荐使用）
label_encoded_fit = label_encoder.fit_transform(label_classes)
# 传入目标array进行编码，使用已有的编码表字典（预测试使用：使用训练时编码好的字典进行对应）
label_encoded = label_encoder.transform(label_classes)

# 逆解码：获取原始数据
# inverse_transform()方法中不能传入空数组，最好在使用前进行判断，判断array不为空
label_inv = label_encoder.inverse_transform(label_encoded_fit)
```
### 1.2.6 独热编码 (One-Hot Encoding)

独热编码又称一位有效编码，其使用N位状态寄存器来对特征的**N个状态**进行编码。每个状态都有自己独立的寄存器位，并且在任意时候，其中只有一位有效。即**有多少个状态就有多少个bit，而且只有一个bit为1，其他全为0**的一种编码。这种编码方式把特征向量的每个特征与特征的非重复总数相对应，通过one-of-k（k指的是某一特征的非重复计数值，也即在该特征中有多少种类型的特征值）的形式对每个值进行编码。按照这种方式编码**可以更加有效地表示空间**（以空间表示特征）
* 将编码看作向量，则每个特征的每个值都是一个指向K维空间中的独特变量（在特征方向上长度固定为1，其他方向为0）
* 这种编码方式使特征长度固定为1，则同一特征中不同特征值的取值是一致的
```python
sex: {male, female}  # feature1
class: {class1, class2, class3}  # feature2

# 进行独热编码：有几个状态就有几位，每个状态码只有一个独特位是1，其他位置均为0
# 如果有多个特征，则多个特征单独进行编码：第i列特征不能使用第j列特征相同的数字的编码来编码
sex: {10, 01}  # 可看作指向空间的二维向量：特征值所在的一维是1，其余维度为0
class: {100, 010, 001}  # 可看作指向空间的三维向量：特征值所在的一维是1，其余维度为0

# 多个特征的编码拼接起来即为完整的独热编码
sample1: {male, class2}  -->  10010 (10 + 010)
sample2: {female, class1}  -->  01100 (01 + 100)
```

独热编码：
* 作用：对于离散型的数据（如上述示例的sex和class），如果单纯使用 {0,1,2} 进行编码（即标签编码），在模型训练中**不同的值可能会使同一特征在样本中的权重发生变化**（当以数字进行表示时，该特征中越靠后被编码的特征值将会具有更大的数字，也即更大的权重）。独热编码的作用在于**对离散型的分类型数据数字化，解决分类器不好处理属性数据的问题，且在一定程度上起到扩充特征的作用**，如将文本分类属性的性别进行数字化
* 为什么使用独热编码
	* 大部分算法是**基于向量空间中的度量**进行计算，为了使**非偏序关系**的变量取值不具有**偏序性**，且到原点的距离等距，需要使用独热编码
	* 使用独热编码后，可以将离散特征的取值扩展到欧氏空间（任意维度，取决于特征种类），离散特征的某个值就对应欧式空间的某个点，使特征之间的距离计算更加合理
		* 为什么要映射到欧氏空间：在回归，分类，聚类等机器学习算法中，特征之间常用的**距离的计算或相似度的计算**都是在欧氏空间的计算（余弦相似性，基于欧氏空间）
	* 编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化**（编码后的特征可适用于前述的各类标准化中）**
* 优缺点
	* 优点
		* 能够很好地处理非连续型数值特征
		* 在一定程度上扩充了特征（如性别特征扩充为“男”、“女”两个特征，且二者向量值相同，储存在不同方向）
	* 缺点
		* 当特征的特征值种类很多时，特征空间会变得非常大，稀疏矩阵会很稀，占内存空间大
		* 占空间大时可使用PCA来减少维度（One-Hot Encoding+PCA组合在实际中也非常有用）
* 独热编码的使用情景
	* 使用
		* 独热编码用于解决类别型数据的离散值问题
		* 基于**参数**、基于**距离**的模型，需要进行特征的归一化，因此需要对类别型特征进行独热编码以适用归一化
	* 不使用
		* 离散型特征无需独热编码即可很合理地计算出距离，则没必要使用独热编码
		* 基于**树**的方法无需进行特征的归一化，如随机森林、bagging、boosting等。对于决策树来说，独热编码会增加数的深度
```python
from sklearn import preprocessing

# 创建独热编码对象
# sparse：是否采用压缩格式
# dtype：元素类型
encoder = preprocessing.OneHotEncoder(sparse=False, dtype=int)  # 创建独热编码对象

# 传入目标array进行独热编码，同时构建编码表字典（推荐使用）
data_encoded_fit = encoder.fit_transform(data)
# 传入目标array进行独热编码，使用已有的编码表字典
# 使用已构建过的独热编码字典进行编码,前提是特征中的状态必须是已有编码字典里的状态，如果存在未出现过的状态，则编码会出现错误
data_encoded = encoder.transform(data) 
```
## 1.3 创建线性回归器
回归是**估计输入数据与连续值输出数据之间关系的过程**，在回归中，我们的目的是找到满足输入到输出映射关系的基本函数。此处以最简单的线性回归示例如何创建一个回归器

线性回归用输入变量的线性组合来估计基本函数，其目标是提取输入变量与输出变量的关联线性模型，这要求实际输出与线性方程预测的输出的**残差平方和最小化 (sum of squares of differences)**，这种方法称为**普通最小二乘法 (Ordinary Least Squares, OLS)**

线性回归的拟合效果或许不是最好的，但其方程一定是最简单的。若要得到更加准确的模型，可以使用非线性回归，但这样拟合速度会慢很多
### 1.3.1 创建线性回归器
```python
import numpy as np
from sklearn import linear_model
import matplotlib.pyplot as plt

# TODO：导入数据：“输入值（即x），输出值（即y）”，输入值和输出值均为浮点型，之间以逗号进行分隔
x = []  # 储存输入值
y = []  # 储存输出值
with open(filename, "r") as file:
	for line in file.readlines():  # 对打开文件的每一行进行操作
		xt, yt = [float(i) for i in line.split(",")]  # 将输入值输出值分别赋值给xt、yt
		x.append(xt)
		y.append(yt)

# TODO：将数据分割为训练集和测试集（手动）
num_training = int(0.8*len(x))  # 设置80%的数据为训练集
num_test = len(x) - num_training  # 设置20%的数据为测试集

x_train = np.array(x[:num_training]).reshape((num_training, 1))  # 输入值要进行reshape，一行→一列
y_train = np.array(y[:num_training])  # 输出值不用进行reshape
x_test = np.array(x[num_training:]).reshape((num_test, 1))
y_test = np.array(y[num_training:])

# TODO：创建线性回归对象并训练模型并画图
linear_regressor = linear_model.LinearRegression()  # 创建线性回归器
linear_regressor.fit(x_train, y_train)  # 使用训练集训练模型
# 以训练集为例看看拟合结果
y_train_predict = linear_regressor.predict(x_train)  # 向回归器中投入x_train的数据，预测y值
# 画图
plt.figure()  # 创建图形实例（可看作创建了画布）
plt.scatter(x_train, y_train, color="green")  # 实际x和y值绘制的散点图
plt.plot(x_train, y_train_predict, color="black", linewidth=2)  # 实际x和预测y值绘制的线
plt.title("Training data")  # 命名
plt.show()  # 以窗口形式展示图

# TODO：使用训练的模型对测试集进行预测并画图
y_test_predict = linear_regressor.predict(x_test)  # 向回归器中投入x_test的数据，预测对应y_test的值
plt.figure()
plt.scatter(x_test, y_test, color="green")
plt.plot(x_test, y_test_predict, color="black", linewidth=2)
plt.title("Test data")
plt.show()
```
### 1.3.2 评估回归准确性
回归器拟合效果的评估需要使用**测试集的真实数据 (y_test) **和**回归器拟合预测的数据 (y_test_predict)**

常见的衡量回归器拟合效果的指标（metric）
* 平均绝对误差 (mean absolute error, MAE)：给定数据集**所有数据点的绝对误差平均值**，能更好地反映预测值误差的实际情况，对于较大误差的权重较高（受极端值影响较大），**越小越好**
* **均方误差 (mean squared error, MSE)**：给定数据集**所有数据点的误差的平方的平均值**，是最流行的指标之一，对于较大误差的权重较高（受极端值影响较大），**越小越好**
* **均方根误差 (root mean squared error, RMSE)**：均方误差开根，是典型指标，用于指示模型在预测中会产生多大的误差，对于较大误差的权重较高（受极端值影响较大），**越小越好**
* 中位数绝对误差 (median absolute error, MAD)：给定数据集**所有数据点的误差的中位数**，该指标的优点在于**可消除单个异常值 (outlier) 的干扰**，而均值误差指标会受到异常值的影响，**越小越好**
* 解释方差分 (explained variance score)：衡量模型**对数据集波动的解释能力**，**越接近1越好**
* **R2得分 (R2 score, R2)**：确定性相关系数，用于衡量模型**对未知样本的预测结果**，**越接近1越好**，值可以是负数
```python
import sklearn.metrics as metrics

# 平均绝对误差
round(metrics.mean_absolute_error(y_test, y_test_pred), 2)  # round取小数两位
# 均方误差
round(metrics.mean_squared_error(y_test, y_test_pred), 2)
# 中位数绝对误差
round(metrics.median_absolute_error(y_test, y_test_pred), 2)
# 解释方差分
round(metrics.explained_variance_score(y_test, y_test_pred), 2)
# R2值
round(metrics.r2_score(y_test, y_test_pred), 2)
```
在选用评价指标时，需要考虑几个方面：
* 数据中是否有0：如果有则不能使用MPE、MAPE之类的指标
* 数据的分布如何：如果是长尾分布可选择带对数变换的指标；**中位数指标**比平均数指标更好
* 是否存在极端值：MAE、MSE、RMSE等容易受到极端值的影响
* 得到的指标是否依赖于量纲（绝对度量）：若依赖量纲，则不同模型之间可能因为量纲不同而无法比较
### 1.3.3 保存模型数据
在训练结束后可以将模型保存为文件，下次使用时可直接加载

方法一：使用python自带的pickle模块
```python
import pickle

savemodelfile = "C:/Users/Hao_Yu/Desktop/LinearModel.pickle"
# 储存模型
with open(savemodelfile, "wb") as file:  # 因为是bytes文件，所以使用wb
	pickle.dump(linear_regressor, file)  # 参数1：储存的模型；参数2：储存的文件路径

# 读取模型
with open(savemodelfile, "rb") as file:  # 因为是bytes文件，所以使用rb
	new_regressor = pickle.load(file)
```
方法二：使用sklearn中的joblib模块（推荐使用，更快）
```python
from sklearn.externals import joblib

savemodelfile = "C:/Users/Hao_Yu/Desktop/LinearModel.pickle"
# 储存模型
joblib.dump(linear_regressor, savemodelfile)

# 读取模型
new_regressor = joblib.load(savemodelfile)
```
## 1.4 创建岭回归器
线性回归有一个主要问题就是**对异常值过于敏感**。由于其使用的是普通最小二乘法，在有明显偏离正常的异常值出现时，会使平方误差的绝对值很大，因此影响模型的拟合度。为了避免这一问题，引入**正则化项的系数（即 alpha）**作为阈值来消除异常值的影响，这样的回归称为岭回归器**（岭回归器也是线性回归的一种）**

岭回归最早用于处理特征数多于样本的情况，现在也用于在估计中加入偏差以得到最好的估计，其是一种**有偏估计**，实质是对最小二乘法的补充，通过放弃最小二乘法的无偏性，以**损失部分信息、降低精度**为代价，获得回归系数更符合实际、更可靠的回归方法
```python
from sklearn import linear_model

# alpha趋于0时，岭回归器趋于普通最小二乘法的线性回归。若希望模型对异常值不敏感，需设置较大的alpha值
ridge_regressor = linear_model.Ridge(alpha=0.01, fit_intercept=True)
ridge_regressor.fit(x_train, y_train)  # 使用训练集训练模型
y_test_predict_ridge = ridge_regressor.predict(x_test)  # 投入x_test的数据，预测y_test的值
```
## 1.5 创建多项式回归器
线性回归中，只能把输入数据拟合成直线，而多项式回归模型则可**通过拟合多项式方程**将数据拟合为**曲线**，对于一些数据来说，可以提高模型的准确性

多项式回归模型的曲率由**多项式的次数**决定，随着模型曲率的增加，模型会变得更加准确**（注意，不是所有模型都会变得更加准确，只有非线性模型会随着多项式次数的增加而变得更加准确；线性模型会随着多项式次数的增加而变得越来越离谱）**。但增加模型的曲率也会增加模型的复杂性，降低拟合速度

多项式拟合中，训练集和测试集的输入数据均需要经过多项式拟合器进行拟合。**拟合过后的训练集输入数据**和**原始输出数据**用于对线性回归器进行训练，训练后的线性回归器输入**拟合过后的测试集输入数据**用于预测数值
```python
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures  # 多项式拟合在preprocessing模块中

# 创建多项式拟合器
polynomial = PolynomialFeatures(degree=3)  # 设置多项式次数为3

# 将训练集输入项进行多项式拟合
x_train_poly = polynomial.fit_transform(x_train)

# 进行线性多项式回归
poly_linear_model = linear_model.LinearRegression()  # 创建线性回归器
poly_linear_model.fit(x_train_poly, y_train)  # 输入经过多项式拟合的x_train以及未拟合的y_train进行训练

# 将测试集输入值进行多项式拟合
data_test = np.array([10,12,14]).reshape(3,1)  # 对于一维数据，需要进行reshape行转列
data_test_poly = polynomial.fit_transform(data_test)

# 投入测试集输入值进行线性回归和多项式回归
linear_regressor.predict(data_test)  # 普通线性回归时使用未经过多项式拟合的数据
poly_linear_model.predict(data_test_poly)  # 多项式回归使用经过多项式拟合的数据
```

## 1.6 监督学习示例：使用决策树估算房屋的价格&评估特征重要性
### 1.6.1 决策树估算房屋价格
一些概念
* 决策树：一个树状模型，每个节点做出一个决策，从而影响最终结果。决策树的分支表示根据输入特征做出的中间决策，叶子节点表示输出数值
* AdaBoost算法：即自适应增强算法 (Adaptive boosting)，其可利用其它系统增强模型准确性。**这种技术是将不同版本的算法结果进行组合，用加权汇总的方式获得最终结果**，被称为**弱学习器 (weak learners)**。AdaBoost算法在每个阶段获取的信息都会反馈到模型中，学习器就可以在后一阶段重点训练难以分类的样本，这样的方式可提高系统分类的准确性

在本示例中，首先使用AdaBoost算法对数据集进行回归拟合并计算误差，**根据误差评估结果，用同样的数据集重新拟合**，此即回归器的调优过程，直到达到预期的准确性
```python
# 使用的数据为sklearn中内置的房价参数（13种）-房价数据集

import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn import datasets
from sklearn.metrics import mean_squared_error, explained_variance_score
from sklearn.utils import shuffle
import matplotlib.pyplot as plt

# TODO：导入房屋数据，设置训练集和测试集
housing_data = datasets.load_boston()
x, y = shuffle(housing_data.data, housing_data.target, random_state=7)  # 使用shuffle打乱数据集顺序

num_traing = int(len(x) * 0.8)  # 训练集：测试集=8：2
x_train, y_train = x[:num_traing], y[:num_traing]
x_test, y_test = x[num_traing:], y[num_traing:]

# TODO：拟使用普通决策树回归模型进行拟合
dt_regressor = DecisionTreeRegressor(max_depth=4)  # 限制最大深度为4
dt_regressor.fit(x_train, y_train)

# TODO：使用带AdaBoost算法的决策树回归模型进行拟合
# max_depth：树的最大深度
# n_estimators：评估器的数量，也即使用的决策树的数量
# random_state：随机数种子
abdt_regressor = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=400, random_state=7)
abdt_regressor.fit(x_train, y_train)

# TODO：输出普通决策树回归模型拟合结果
y_pred_dt = dt_regressor.predict(x_test)
mse_dt = mean_squared_error(y_test, y_pred_dt)
evs_dt = explained_variance_score(y_test, y_pred_dt)
print("DT: {}\t{}".format(mse_dt, evs_dt))

# TODO：输出带AdaBoost算法的决策树回归模型结果
y_pred_abdt = abdt_regressor.predict(x_test)
mse_abdt = mean_squared_error(y_test, y_pred_abdt)
evs_abdt = explained_variance_score(y_test, y_pred_abdt)
print("ABDT: {}\t{}".format(mse_abdt, evs_abdt))

>>> DT: 14.790048392224149	0.8206001721287847
>>> ABDT: 7.541922646752027	0.908738757946907
```
### 1.6.2 计算特征的相对重要性
在1.6.1的案例中，总共使用了13个特征，当然每个特征对结果的贡献都是不同的，如果需要减少一些特征，就需要知道哪些特征相对更不重要，因此需要评估各个特征的重要性

对于回归器regressor，有方法 ```feature_importances_``` 获取每个特征的重要性，输出值为由浮点数组成的列表
```python
# TODO：构建函数，为每个特征的重要性创建散点图
def plot_feature_importanes(feature_importrances, feature_names, title):
	# 标准化特征重要性
	feature_importances = 100.0 * (feature_importances / max(feature_importances))
	# 得分由高到低排序
	# np.argsort：返回数组值从小到大排列时对应的索引列表
	# np.flipud：翻转列表
	index_sorted = np.flipud(np.argsort(feature_importances))
	
	# 画图
	pos = np.arange(index_sorted.shape[0]) + 0.5  # 将标签居中显示
	
	plt.figure()
	plt.bar(pos, feature_importances[index_sorted], align='center')  # 设置bar的数据
	plt.xticks(pos, feature_names[index_sorted])  # 设置X轴分类
	plt.ylabel("Relative Importance")  # 设置Y轴标题
	plt.title(title)  # 设置图片名称
	plt.show()
```









