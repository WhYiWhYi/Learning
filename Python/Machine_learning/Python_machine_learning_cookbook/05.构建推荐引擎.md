# 5 构建推荐引擎
## 5.1推荐引擎简介
推荐引擎是一个能**预测用户兴趣点的模型**，其本质是**预测数据**

推荐引擎通常使用协同过滤（collaborative filtering）或基于内容的过滤（cotent-based filtering）产生推荐：
* **协同过滤**：从**当前用户过去的行为**和**其他用户对当前用户的评分**来构建模型
* 基于内容的过滤：用**商品本身的特征**给用户推荐更多的商品
## 5.2 创建函数流水线
在构建一个准确的、可扩展的机器学习系统的过程中，拥有一个数据处理流水线很重要。通常数据处理流水线是**一些基本函数的组合**。一般情况下，**不推荐使用嵌套或循环的方式调用这些函数**，而是用**函数式编程的方式**创建函数组合
### 5.2.1 简单函数流水线示例
```python
import numpy as np

def add3(input_array):
    # lambda函数匿名+
    # map对input_array中每一项执行操作
    # 在python3中，map返回的是map对象，需使用list()将其转化
    return list(map(lambda x: x + 3, input_array))

def mul2(input_array):
    return list(map(lambda x: x * 2, input_array))

def sub5(input_array):
    return list(map(lambda x: x - 5, input_array))

def function_composer(*args):
	# args是可变参数，传入的是要引用的函数列表（因为可变，所以列表的项数可变）
	# 定义两层函数f、g，并使用reduce对于传入的args（函数）依次调用（以下面的示例为例）：
	# func(func(add3, mul2), sub5)，等效于sub5(mul2(add3(x)))
	# 由于函数是一层调用一层，所以注意args中函数的传入顺序要逆过来
    return reduce(lambda f, g: lambda x: f(g(x)), args)

if __name__ == "__main__":
    x = np.array([1, 1, 1])
    x1 = add3(x)
    x2 = mul2(x1)
    x3 = sub5(x2)
    print("Nomarl way result: {}".format(x3))

    function_composed = function_composer(sub5, mul2, add3)
    print("Cool way result: {}".format(function_composed(x)))
>>> Nomarl way result: [3, 3, 3]
>>> Nomarl way result: [3, 3, 33]
```
### 5.2.2 构建机器学习流水线
sklearn库中包含了构建机器学习流水线的方法，只需指定函数，就会构建一个组合对象，是数据通过整个流水线，可包括预处理、特征选择、监督式学习、非监督式学习等函数

sklearn.pipeline.Pipleline(steps, \*, memory=None, verbose=False)：
* 参数：
	* step：进行组transformer列表，每个transformer以 (name, transformer())的形式传入
	* memory：传入缓存路径的字符串，用于指定路径缓存pipeline中的transformer。当训练耗时较多时，可选用
	* verbose：True时，完成每个步骤时所经过的时间会被打印出来
* 方法：
	* fit：训练pipeline器，pipeline中的transformer会依次调用
	* get_params：获取pipeline器中的参数
	* set_params：设置pipeline中的参数（需以参数名指定要设置的参数，见下）
	* predict：测试pipeline器
	* score：获取pipeline器的得分
	* 其他方法见：https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html

选择k个最好的特征
* 基于单变量的特征选择，进行单变量统计测试，从特征中抽取最优秀的特征。其可使处理的数据维度降低，有利于减少计算复杂度
* 单变量统计测试后，向量空间中每个特征会获得一个评价分数，特征选择即基于这些评价分数
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.pipeline import Pipeline

x, y = [], []  # 样本数据

# 建立特征选择器：选择k个最好的特征
# score_func：传入x、y数组，返回一组数组（scores, pvalues）或一个值（scores）的函数。默认值为f_classif，可选项可见https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html
# k：要选择的特征
selector_k_best = SelcetKBest(f_regression, k=10)

# 建立随机森林分类器
rm_classifier = RandomForestClassifier(n_estimator=50, max_depth=4)

# 构建流水线分类器（直到此步尚未对数据进行分析）
# Pileline中传入的为一个包含要使用“步骤”元组的列表。一个步骤存放在一个元组中：(name, transform())
pipeline_classifier = Pipeline([('Selector', selector_k_best()), ('rf', rf_classifier())])

# 训练分类器，输出预测结果和得分
pipeline_classifier.fit(x, y)
preiction = pipeline_classifier.predict(x)
score = pipeline_classifier.score(x,y)

# 打印被选中的特征（需从流水线分类器中指定到'selector'分类器）
features_states = pipeline_classifier.named_steps['selector'].get_support()
```
## 5.3 寻找最近邻算法
最近邻模型是一个通用算法类，其目的是依据训练数据集中最近邻数量来决定对策

k近邻算法（k-Nearest Neighbor, k-NN）采用测量不同特征值之间的距离方法进行分类，其是一种监督学习的分类器算法
* 工作原理：将未知标签的数据的每个特征与已有标签数据的特征进行比较，由算法提取样本数据集中**前k个（k一般小于20）**最相似数据（**最相似数据=经过距离计算后得到的结果最小**）的分类标签，在这k个数据中**出现次数最多**的分类即作为新数据的分类
* 常用距离计算公式
	* 欧式距离公式：√(Σ(ai-bi)²)
	* 曼哈顿距离公式：Σ|ai-bi|
* 优点：算法简单易于实现、精度高、对异常值不敏感、无数据输入假定
* 缺点：计算复杂度高、空间复杂度高、**结果受k值设定影响**、**结果受训练样本集不平衡影响**（如果某个分类数量极大，则前k个很可能就会接近该分类而不是目标分类）
* 适用数据范围：数值型、标称型
### 5.3.1 KNN分类器
自建KNN分类器
```python
import numpy as np
import operator

# 数据的特征和目标变量要分别存放
group = np.array([[1, 1.1], [1, 1], [0, 0], [1, 0.1]])  # 训练数据的特征，储存在array中
lable = ['A', 'A', 'B', 'B']  # 训练数据的目标变量，储存在列表中

def classify(inX, dataSet, labels, k):
	"""
	inX：待分类的测试集数据
	dataSet：储存训练数据特征的array
	labels：训练数据的分类list
	k：设置的k值
	"""
	# shape获取的是一个元组，第一项为array中元素的项数（行数），第二项为每个元素中元素的项数（列数）
	dataSetR = dataSet.shape[0]  # 获取训练数据集的行数、列数
	# np.tile可将输入的列表（para1）复制扩展为指定的形状（para2，元组中为行数、1）
	# 输入列表列数已经与dataSet相同，无需更改
	# 在np的array中，两个形状相同（行、列数相同）的array可以直接相减（相加也可），结果为对应位置的相减值
	diffMat = np.tile(inX, (dataSetR, 1)) - dataSet
	
	# 欧式距离计算
	# 同理，对array进行简单数值的乘除法时，也是对array中每一个元素执行
	sqdiffMat = diffMat ** 2  # 对array中每个元素求平方值
	# 对二维array元素求和时，axis=0代表对每一列分别求和，axis=1代表对每一行分别求和
	# 补充：axis越大，计算跨越的维度越低（=1时在子列表中求和，=0时跨一维列表求同位置元素的和）
	# 每次求和都会将原始array降维一层
	sqDistances = sqdiffMat.sum(axis=1)  # 对array中每一行求和
	oDistances = sqDistances ** 0.5
	
	# 对计算后的元素进行排序
	sortedOIndex = oDistances.argsort()  # argsort：排序后返回元素排序前的index，默认由小到大
	
	# 获取距离最近的前k个元素：label-前k次中出现的频次
	classCount = {}
	for i in range(k):
		# 此处为按sort顺序依次提取对应元素的label
		# 由于原始label顺序与array中元素顺序一致，可据此获取排序后各元素的label
		sortedLabel = labels[sortedOIndex[i]]
		# 字典中的特殊计数方法，不用使用Counter方法
		# Dict.get(key, default)：如果Dict中有key则返回key，如果没有则返回default
		# default默认为None值，但可自行指定
		# 新值：此处对没记录过的key赋值为0后+1，然后将新值作为新key储存到字典中
		# 已存在的值：获取key的计数后，+1，将新值赋值给原key
		classCount[sortedLabel] = classCount.get(sortedLabel,0)+1  # 对前k个label计数
	
	# 对距离最近的前k个元素进行排序：依据出现频次由大到小（reverse=True），返回值为排序后的字典
	# key=operator.itemgetter()：取出索引位置处的数据，作为排序的依据
	sortedclassCount = sorted(classCount.items(), key=operator.itemgetter(1),reverse=True)
	return sortedclassCount[0][0]
```
利用sklearn建立KNN分类器
```python
import numpy as np
from sklearn import datasets
from sklearn.neighbors import KneighborsClassifier
import matplotlib.pyplot as plt

x, y = [], []  # 输入数据：特征和标签

# 设置最近邻个数，训练最近邻分类器
num_neighbors = 10
knn_classifier = KneighborsClassifier(num_neighbors, weights='distance')
knn_classifier.fit(x, y)

# 生成网格评价分类器，进行边界可视化
steps = 0.01  # 定义步长
x_min, x_max = x[:,0].min() - 1, x[:,0].max() + 1
y_min, y_max = x[:,1].min() - 1, x[:,1].max() + 1
x_grid, y_grid = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# 绘制网格、训练集数据点
predicted_values = knn_classifier_predict(np.c_[x_gridravel(). y_grid.ravel()])
predicted_Values = predicted_values.reshape(x_grid.shape)
plt.figure()
cm = plt.cm.Pastell
plt.pcolormesh(x_grid, y_grid, predicted_values, cmap=cm)

markers = '^sov<>hp'
mapper = np.array([markers[i] for i in y])
for i in range(x.shape[0]):  # 对于每一行
	plt.scatter(x[i,0], x[i,1], marker=mapper[i], s=50, edgecolors='black', facecolors='none')
plt.xlim(x_grid.min(), x_grid.max())
plt.ylim(y_grid.min(), y_grid.max())

# 画出测试集数据点
test_point = [4.5, 3.6]
plt.scatter(test_point[0], test_point[1], marker='x', s=200, facecolors='black')

# 提取测试集数据点的KNN
# dist：distance，输出最近k个点与测试点的距离
# indices：输出最近k个点的信息
dist, indices = knn_classifier.kneighbors(test_point)

# 画出与测试集数据点最近的k个点
for i in indices:
	plt.scatter(x[i,0], x[i,1], marker='o', linewidth=3, s=100, facecolors='black')
plt.show()
```
### 5.3.2 KNN回归器
此节中的例子详见书