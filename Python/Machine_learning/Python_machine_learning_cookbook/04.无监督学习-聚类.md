# 4 无监督学习——聚类

## 4.1 无监督学习
最常见的无监督学习就是**聚类**，当需要把无标记的数据划分为几种集群时，就用它进行分析，这些集群同时是依据某种相似度指标进行划分的（如欧氏距离）

无监督学习广泛应用于如数据挖掘、医学影像、股票市场分析、计算机视觉、市场细分等
## 4.2 k-means算法
### 4.2.1 k-means算法
算法步骤
* 选择初始化的**k个**样本作为**初始聚类中心** a=a~1~, a~2~, ..., a~k~
* 针对数据集中每个样本x~i~，计算它到k个聚类中心的距离，并将其分到距离最小的聚类中心所对应的类a~j~中
* 针对每个类别a~j~，重新计算该类别的聚类中心（即属于该类的所有样本的质心）
* 重复上面2-3步，直到达到某个终止条件（迭代次数、最小误差变化（数据与中心点距离平方和最小化）等）

复杂度
* 时间复杂度：**O(tknm)**；t：迭代次数；k：簇数目；n：样本数；m：样本点维度（特征数）
* 空间复杂度：**O(m(n+k))**；k：簇数目；n：样本数；m：样本点维度（特征数）

优缺点
* 优点
	* 容易理解，聚类效果不错；虽然是局部最优，但往往局部最优就够了
	* 处理大数据集的时候，该算法可以保证较好的伸缩性
	* 当簇近似高斯分布时，效果十分不错
	* **算法复杂度低**
* 缺点
	* k值需要人为设定，不同k值得到的结果可能不同（可使用手肘法或Gap statistic方法寻找最优k值）
	* **对初始的簇中心敏感**，不同的选取方式会得到不同的结果
	* **对异常值敏感**
	* 样本只能归于一类，不适合多分类任务
	* 不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类

算法调优
* 数据预处理
	* k-means的本质是**基于欧氏距离**的数据划分算法，**均值**和**方差**大的维度会对数据聚类产生决定性影响。因此，在进行k-means算法前需进行归一化、标准化处理
	* 此外，离群点或者噪声也会产生影响，因此异常值检测也是必要的
* 合理选择k值（获取最佳起始k）
	* 手肘法
		![](E:/SynologyDrive/READING/CS/note/机器学习/img/4.2.1.手肘法.jpg)
		* 如图可见，k=3是整个曲线的**拐点**，因此认为k=3是k的最佳值
		* 然而，手肘法也有缺点，即需要**人工**判断
	* Gap statistic法
		* Gap(K) = E(logD~k~) - logD~k~为损失函数，
		* D~k~为损失函数
		* E(logD~k~)为logD~k~的期望，通常由蒙塔卡洛模拟产生：在样本所在区域内按照均匀分布随机产生和原始样本数一样多的随机样本，并对这个随机样本进行k-means得到D~k~，重复多次后求均值，可得到E(logD~k~)的近似值
		* 当**Gap(K)取值最大时的k**为最佳k
* 采用核函数
	* 基于欧式距离的k-means假设各个数据簇的数据具有一样的先验概率并呈球形分布
	* 面对非凸的数据分布形状时，可以引入核函数进行优化，此时算法称为核k-means算法，为核聚类方法的一种
	* 非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果
* k-means ++（改进初始值选取）
	* k-means ++步骤
		* 随机选取一个中心点a~1~
		* 计算数据到之前n个聚类中心最远距离，并以一定概率选择新的中心点a~i~
		* 重复第二步
	* 简单来说，k-means++就是选择离已选中心点最远的点。但其难以并行化，因此k-means II改变取样策略：
		* 每次遍历取样k个，重复该取样过程log(n)次，得到klog(n)个样本点组成的集合，之后从中选择k个
		* 一般无需log(n)次取样，5次即可
* ISODATA（解决k值人为选取）
	* 迭代自组织数据分析法
	* 基本思路：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别
### 4.2.2 使用k-means算法聚类数据
KMeans()初始化条件：
* init：聚类中心的初始化方案，如上所述，使用k-means++，其余选项包括random、ndarray
* n_cluster：起始时聚类中心数目，默认值为8
* max_iter：算法运行的最大迭代次数，默认值为300
* n_init：k-means算法随即运行的次数，默认值为10
* n_jobs：并发数，用于并行计算每个n_init。设置为-1则使用所有CPU，设置为1则并行
* random_state：设置随机种子
* algorithm：可选的K-means距离计算算法，默认值为auto，其余选项包括full（传统计算方式）、elkan（使用三角不等式，效率更高，但目前不支持稀疏数据）
* precompute_distance：是否将数据全部放入内存计算
```python
import numpy as np
from sklearn import metrics
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

data = np.array([[1, 2], [3, 2], [11, 5], [12, 4], [2, 5], [2, 6], [8, 6]])  # 导入数据

# 训练模型
kmeans = KMeans(init='k-means++', n_clusters=4, n_init=10)
kmeans.fit(data)

# 数据边界可视化
step_size = 0.01
x_min, x_max = min(data[:,0]) - 1.0, max(data[:,0]) + 1.0
y_min, y_max = min(data[:,1]) - 1.0, max(data[:,1]) + 1.0
x_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))

# 计算分类结果
mesh_output = kmeans.predict(np.c_[x_values.ravel(), y_values.ravel()])
mesh_output = mesh_output.reshape(x_values.shape)  # 数组维度变形

# 画图（边界+散点）
plt.figure()
plt.clf()  # 清除旧图形
cm1 = plt.cm.get_cmp("Paired")
plt.imshow(mesh_output, interpolation='nearest',
			extent=(x_values.min(), x_values.max(), y_values.min(), y_values.max()),
			cmap=cm1)
plt.scatter(data[:,0], data[:,1], marker='o', facecolors='none', edgecolors='k', s=30)

# 画图（画出各个聚类的中心点）
centroids = kmeans.cluster_centers_  # 获取聚类中心
plt.scatter(centroids[:,0], centroids[:,1], marker='o', s=40, linewidth=3, color='k', zorder=10, facecolors='black')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())
plt.show()
```
### 4.2.3 使用矢量量化压缩图片
矢量量化：简单来说当把“四舍五入”这个概念推广到N维数据时，即矢量量化

k-means聚类的主要应用之一就是矢量量化，此节中的例子详见书
## 4.3 建立均值漂移聚类模型
均值漂移原理：
* 均值漂移与k-means都是**基于聚类中心的聚类算法**，但其**无需事先制定类别个数k**
* 将数据分布看作**概率密度函数**，希望在特征空间中依据函数分布特征找出数据点的**分布模式**，这些分布模式就对应于一群局部最密集分布的点
* Mean-Shift 聚类就是对于集合中的每一个元素执行下面的操作：把该元素**移动**到它邻域中所有元素的特征值的均值的位置，不断重复直到收敛。移动并非真的移动元素，而是**把该元素与它的收敛位置的元素标记为同一类**

算法步骤
* 假设一个有N个样本点的特征空间。初始确定一个中心点c，计算**在设置的半径为D的圆形空间内**所有点与中心点c的**向量**
* 计算整个圆形空间内所有向量的均值，得到一个偏移均值向量
* 将中心点c移动到偏移均值的位置，重复1-3步骤，直至满足一定条件结束
	![](E:/SynologyDrive/READING/CS/note/机器学习/img/4.3.mean-shift.png)
```python
import numpy as np
from sklearn.cluster import MeanShift, estimate_bandwidth
import matplotlib.pyplot as plt

data = np.array([[1, 2], [3, 2], [11, 5], [12, 4], [2, 5], [2, 6], [8, 6]])  # 导入数据

# 建立均值漂移聚类器
# 通过estimate_bandwidth设置带宽参数bandwidth。注意bandwidth的值要大于1，可以print出来确认一下
# data：传入的数据
# n_samples：从传入的数据中随机挑选的样本数目，用于计算每一对样本的距离。决定函数的计算量
# quantile：分位数，在计算每一段样本的距离后，选取这些距离的0.2分位数作为返回值
bandwidth = estimate_bandwidth(data, quantile=0.4, n_samples=len(data))
# 建立聚类器并进行训练
# bin_seeding：设置为True时不会将所有点都初始化为核心位置，从而加速算法
meanshift_estimator = MeanShift(bandwidth=bandwidth, bin_seeding=True)
meanshift_estimator.fit(data)

# 提取标记数量和集群中心点
labels = meanshift_estimator.labels_  # 提取数据的标记
centroids = meanshift_estimator.cluster_centers_  # 提取中心点
labels_unique = np.unique(labels)  # 标记种类去重
n_clusters = len(labels_unique)  # 计算标记种类
print("number of estimated clusters : %d" % n_clusters)

# 画图（对每一类标记迭代式画图）
plt.figure()
markers = '.*xv'  # 为每种集群设置不同标记（也可为不同集群设置不同颜色）

for i, marker in zip(range(n_clusters), markers):  # 对每一类标记进行画图
	# 画出一类标记的散点
	plt.scatter(data[labels==i, 0], data[labels==i, 1], marker=marker, color='k')
	# 画出集群中心（圈）
	centroid = centroids[i]
	plt.plot(centroid[0], centroid[1], marker='o',
			markerfacecolor='k',  # 圈中心颜色
			markeredgecolor='k',  # 圈边缘颜色
			markersize=15)  # 圈大小
plt.show()
```
## 4.4凝聚层次聚类
### 4.4.1 凝聚层次聚类概述
层次聚类（Hierarchical clustering）：通过不断地**分解**或**合并**集群聚类来构建树状集群
* **凝聚层次聚类**
	* 自下而上
	* 每个数据点被当作独立的集群，这些集群不断合并，直到所有集群都合并为一个巨型集群
* 分裂层次聚类
	* 自上而下
	* 从包含所有数据点的集群开始，每一步分裂一个簇，直至最后所有集群都变为一个单独的数据点

算法步骤
* 将每个数据点单当作一类，计算两两之间的距离，获取**邻近度矩阵**
	* 点与点之间的邻近度：本质是两个数据点之间的距离,有多种定义方法，如欧氏距离、曼哈顿距离、马氏距离、余弦距离、Jaccard系数、Bregman散度等等
	* 簇与簇之间的邻近度：
		* 单链（Single Linkage）：将两个组合数据点中**距离最近**的两个数据点间的距离作为这两个组合数据点的距离，容易受到极端值的影响（不相似的簇可能因为极端值组合在一起）
		* 全链（Complete Linkage）：将两个组合数据点中**距离最远**的两个数据点间的距离作为这两个组合数据点的距离，容易受到极端值的影响（相似的簇可能因为极端值无法组合在一起）
		* 均链（Average Linkage）：计算两个组合数据点中的**每个数据点与其他所有数据点的距离，其均值**作为两个组合数据点间的距离
* 依据矩阵计算结果，合并最接近的两个簇（在第一次迭代时，即合并最接近的两个数据点）
* 合并完成后，重新计算每个类与所有类之间的距离，重复2-3直到所有类合并为一类
	![](E:/SynologyDrive/READING/CS/note/机器学习/img/4.4.凝聚层次聚类.jpg)

层次聚类的优缺点
* 优点
	* 距离和规则的相似度容易定义，限制少
	* **无需预先指定聚类数目**
	* 可以发现类与类之间的层次关系
	* 可以聚类为其他形状
* 缺点
	* 计算复杂度高
	* 奇异值可能产生很大影响
	* 算法也可能聚类为链状
### 4.4.2 使用凝聚层次聚类进行数据分组
```python
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import Kneighbors_graph
import matplotlib.pyplot as plt

def perform_clusting(x, connectivity, title, num_clusters=3, linkage='ward'):
    # 实现凝聚层次聚类并画图的函数
    model = AgglomerativeClustering(linkage=linkage, connectivity=connectivity, n_clusters=num_clusters)
    model.fit(x)
    
    labels = model.labels_  # 提取数据标记
    
    plt.figure()
    markers = '.vx'  # 为每种集群设置不同标记
    
    for i, marker in zip(range(num_clusters), markers):
        plt.scatter(x[labels==i, 0], x[labels==i, 1], s=50, marker=marker, color='k', facecolors='none')
    
    plt.title(title)
    
def add_noise(x, y, amplitude):
    # 在创建螺旋状数据点时，增加一些噪声以增加不确定性
    x = np.concatenate((x, y))
    x += amplitude * np.random.randn(2, x.shape[1])
    return x.T
      
def get_spiral(t, noise_amplitude=0.5):
    # 创建一组呈螺旋状的数据点：空间中连接到一起、距离上也十分接近的点
    # 希望连接在一起的数据聚类，而非空间上非常接近的数据聚类
    r = t
    x = r * np.cos(t)
    y = r * np.sin(t)
    return add_noise(x, y, noise_amplitude)

if __name__ == "__main__":
    # 生成样本数据
    n_samples = 500
    np.random.seed(2)  # 设置随机种子
    # np.pi：常量，代表Π
    # np.random.rand：创建一个由随机生成的0-1的数字填充的、指定形状的数组
    t = 2.5 * np.pi * (1 + 2 * np.random.rand(1, n_samples))
    x = get_spiral(t)  # 创建螺旋数据点
    
    # 不考虑螺旋形的数据连接性（不使用连接特征）：按在螺旋线上的位置聚类
    connectivity = None
    perform_clusting(x, connectivity, 'No connectivity')
    
    # 根据数据连接线创建k个临近点的图形（使用连接特征）：将连接在一起的数据合成一组
    connectivity = kneighbors_graph(x, 10, include_self=False)
    perform_clusting(x, connectivity, 'K-Neighbors connectivity')
    
    plt.show()
```
## 4.5 聚类算法的评价
在监督学习中，可以使用预测值与原始值进行比较来计算模型的准确性，而在无监督学习中，由于没有标记，因此需要一种度量聚类算法的方法

轮廓系数（Silhouette Coefficient）
* 该得分是为每个数据点定义的，用于评估集群被分离的离散程度
* score = (x-y) / max(x,y)
	* x：同一集群中某个数据点与其他数据点的平均距离
	* y：某个数据点与最近的另一个集群的所有点的平均距离
```python
import numpy as np
from sklearn import metrics
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

data = np.array([[1, 2], [3, 2], [11, 5], [12, 4], [2, 5], [2, 6], [8, 6]])

scores = []  # 储存轮廓系数得分的列表
range_values = np.arange(2, 6)  # 设置不同的聚类数量

for i in range_values:
	# 训练模型
	kmeans = KMeans(init='k-means++', n_clusters=i, n_init=10)
	kmeans.fit(data)
	# 计算得分
	score = metrics.silhouette_score(data, kmeans.labels_, metric='euclidean', sample_size=len(data))
	scores.append(score)
	print(i, score)

>>> 2 0.6476440214802881  # 聚类为两类是较为合适的
>>> 3 0.5953029506147003
>>> 4 0.5113865920084895
>>> 5 0.38254217820429043
```
## 4.6 DBSCAN算法估算集群数量
DBSCAN（Density-Based Spatial Clustering of Applications with Noise）
* 将数据点看成是紧密集群的若干组
* 如果某个点属于一个集群，那就应该有许多点也属于同一集群
* DBSCAN使用参数 ```epsilon``` 控制某个点到其他点的最大距离，如果两点之间的距离超过这个值，他们就不可能属于同一集群
* DBSCAN的优点在于**对于数据稀疏区，DBSCAN可将其看作异常值而非强制纳入某一集群**

```python
import numpy as np
from itertools import cycle
from sklearn.cluster import DBSCAN
from sklearn import metrics
import matplotlib.pyplot as plt

x = []  # 输入数据

# 寻找DBSCAN的epsilon最佳参数（寻找最佳模型）
# 初始化寻找最佳参数的变量
eps_grid = np.linspace(0.3, 1.2, num=10)
silhouette_scores = []
silhouette_scire_max = -1
eps_best = eps_grid[0]  # 先初始化为第一个数值，后续将进行逐一比较、替换
model_best = None
labels_best = None

# 寻找最佳epsilon数值
for eps in eps_grid:
	model = DBSCAN(eps=eps, min_samples=5).fit(x)  # 训练模型
	labels = model.labels_  # 提取标记
	
	score = round(metrics.silhouette_score(x, labels), 4)  # 计算每个模型的轮廓系数（四位小数）
	silhouette_scores.append(score)
	print(eps, score)
	
	# 保存最佳epsilon值、最佳得分、最佳模型
	if score > silhouette_score_max:  # 每次迭代都进行以此判断，据此输出最大值
		silhouette_score_max = score
		eps_best = eps
		model_best = model
		labels_best = labels

# 判断数据中是否有未分配集群样本，如果有则在分配集群需要将其扣除（未分配集群样本不算集群）
offset = 0
if -1 in labels:
	offset = 1

num_clusters = len(set(lebels)) - offset  # 计算集群数量

# 从训练模型中提取核心样本数据点的索引
mask_core = np.zeros(labels.shape, dtype=np.bool)  # 创建labels形状的空数组，设置类型为布尔值
msak_core[model.core_sample_indices_] = True  # 将核心样本的值设置为True

# 画图
plt.figure()
labels_uniq = set(labels)
markers = cycle('vo^s<>')

for cul_label, marker in zip(labels_uniq, markers):
	# 未分配集群样本
	if cur_label == -1:
		marker = '.'
	
	cur_mask = (labels == cur_label)
	cur_data = x[cur_mask & mask_core]
	plt.scatter(cur_data[:,0], cur_data[:,1], marker=marker, edgecoldes='black', s=96, facecolors='none')
	
	cur_data = x[cur_mask & ~mask_core]
	plt.scatter(cur_data[:,0], cur_data[:,1], marker=marker, dgecoldes='black', s=32)

plt.show()
```