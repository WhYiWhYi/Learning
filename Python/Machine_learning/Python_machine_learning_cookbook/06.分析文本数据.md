# 6 分析文本数据
## 6.1 简介
**文本分析**和**自然语言处理（Natural Language Procesing, NLP）**是现代人工智能的重要组成部分

基于机器学习的NLP算法可检测非结构化的自由文本数据的模式，从中解析信息。NLP常用的领域包括搜索引擎、情感分析、主题建模、词性标注、实体识别等

本节中，将使用 ```nltk``` 库
## 6.2 使用标记解析的方法与处理数据
nltk库中的四类解析方法
* sent_tokenize：解析句子
* PunktSentenceTokenizer.tokenize：解析句子。以标点符号分割文本，句子中的标点符号保留不做分割
* word_tokenize：解析单词
* WordPunctTokenizer.tokenize：解析单词。以标点符号分割文本，句子中的标点符号将单独分割
```python
from nltk.tokenize import sent_tokenize
from nltk.tokenize import PunktSentenceTokenizer
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer

text = "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences to see it in action."

sent_tokenize(text)
>>> ['Are you curious about tokenization?', "Let's see how it works!", 'We need to analyze a couple of sentences to see it in action.']

punkt_sent_tokenizer = PunktSentenceTokenizer()
punkt_sent_tokenizer.tokenize(text)
>>> ['Are you curious about tokenization?', "Let's see how it works!", 'We need to analyze a couple of sentences to see it in action.']

word_tokenize(text)  # 单词中的标点符号不分割（"'s"）
>>> ['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', "'s", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'to', 'see', 'it', 'in', 'action', '.']

word_punk_tokenizer = WordPunctTokenizer()
word_punk_tokenizer.tokenize(text)  # 单词中的标点符号分割（"'", "s"）
>>> ['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', "'", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'to', 'see', 'it', 'in', 'action', '.']
```
## 6.3 提取文本数据的词干、词形还原
### 6.3.1 提取词干
处理文本数据时，一个单词可能会出现不同形式，提取单词的原型十分有效，其可帮助我们**提取一些统计信息**来分析整个文本。提取词干的目的是**将不同词形的单词变成原型**，其常用**启发式处理方法截取单词的尾部**提取单词原型
```python
from nltk.stem.porter import PorterStemmer
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem.snowball import SnowballStemmer

words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']

stemmer_porter = PorterStemmer()  # 提取最宽松
stemmer_lancaster = LancasterStemmer()  # 提取最严格
stemmer_snowball = SnowballStemmer('english')  # 介于二者之间

porter = [stemmer_porter.stem(word) for word in words]
>>> ['tabl', 'probabl', 'wolv', 'play', 'is', 'dog', 'the', 'beach', 'ground', 'dreamt', 'envis'] 

lancaster = [stemmer_lancaster.stem(word) for word in words]
>>> ['tabl', 'prob', 'wolv', 'play', 'is', 'dog', 'the', 'beach', 'ground', 'dreamt', 'envid']

snowball = [stemmer_snowball.stem(word) for word in words]
>>> ['tabl', 'probabl', 'wolv', 'play', 'is', 'dog', 'the', 'beach', 'ground', 'dreamt', 'envis']
```
### 6.3.2 词形还原
词形还原变形词的结尾，例如”-ing“或“-ed”，然后返回单词的原形形式，输出结果取决于标记是一个动词还是一个名词
```python
from nltk.stem import WordNetLemmatizer

words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']

lemmatizer_wordnet = WordNetLemmatizer()

noun = [lemmatizer_wordnet.lemmatize(word, pos='n') for word in words]  # 名词还原
>>> ['table', 'probably', 'wolf', 'playing', 'is', 'dog', 'the', 'beach', 'grounded', 'dreamt', 'envision']

verb = [lemmatizer_wordnet.lemmatize(word, pos='v') for word in words]  # 动词还原
>>> ['table', 'probably', 'wolves', 'play', 'be', 'dog', 'the', 'beach', 'ground', 'dream', 'envision']
```
## 6.4 创建词袋模型
### 6.4.1 用分块的方法划分文本
分块：基于任意随机条件将输入的文本分割成块，当处理非常大的文本文档时就需要进行分块以便下一步操作
```python
from nltk.corpus import brown

def splitter(data, num_words):
    # 将文本分割成块的函数
    words = data.split(" ")  # 以空格分隔文本
    output = []

    # 初始化变量
    cur_count = 0  # 已输出单词数
    cur_words = []  # 已输出单词

    for word in words:
        cur_words.append(word)
        cur_count += 1

        if cur_count == num_words:  # 当单词数量达到指定上限时，重置计数和列表
            output.append(' '.join(cur_words))  # 将列表以空格区分组合为
            cur_words = []
            cur_count = 0

    output.append(' '.join(cur_words))  # 最后一块不一定会到达块上限，需单独添加到output中
    return output

if __name__ == "__main__":
    data = " ".join(brown.words()[:10000])  # 从布朗语料库加载data

    num_words = 1700  # 定义每块的单词数目
    text_chunks = splitter(data, num_words)
    print(len(text_chunks))
```
### 6.4.2 创建词袋模型
对于提取得到的单词，需要将其**转化为数值形式**，以便让机器使用这些数据来学习算法。词袋是**从所有文档的所有单词中学习词汇的模型**，学习之后将构建文档中所有单词的直方图来对每篇文档进行建模

以三句话为例说明其工作原理
* The brown dog is runing.
* The black dog is in the black room.
* Running in the room is forbidden.

这三句话中，包括唯一的九个单词：the, brown, dog, is, running, black, in, room, forbidden。以上三个句子转化为直方图，每个特征向量包含九个特征（单词）：
* [1, 1, 1, 1, 1, 0, 0, 0, 0]
* [2, 0, 1, 1, 0, 2, 1, 1, 0]
* [0, 0, 0, 1, ,1, 0, 1, 1, 1]
```python
from sklearn.feature_extraction.text import CountVectorizer

text_chunks = splitter(data, num_words)  # 分块导入数据

chunks = []
for i, text in enumerate(text_chunks, 1):
	chunk = {'index': i, 'text': text}  # 将数据构建为字典
	chunks.append(chunk)

# 提取文档-词矩阵
vectorizer = CountVectorizer(min_df=5, max_df=0.95)

doc_term_matrix = vectorizer.fit_transform(chunk['text'] for chunk in chunks)
vocab = np.array(vectorizer.get_feature_names())  # 从vectorizer中提取词汇（特征）

for word, item in zip(vocab, doc_term_matrix.T):  # 需转置将特征转到行上
	# item是压缩的稀疏矩阵（csr_matrix）数据结构
	output = [str(x) for x in item.data]  # 使用data方法从item中提取结果
	print(word, "-->", output)
```
## 6.5 示例：创建文本分类器
文本分类的目的是将文本文档分为不同的类，此处使用**词频-逆文档频率（term-frequency inverse document frequency, tf-idf）**的统计技术。这一工具有助于理解一个单词在一组文档中对某一个文档的重要性，其可用于作为特征向量对文档进行分类

tf-idf原理
* tf-idf技术常用于了解文档中每个单词的重要性。对于单词来说，除去像”is“、”be“之类的普通词汇外，**单词在文档中出现的词频可以反映词汇的重要性**。提取每个句子的词频并将其转化为特征向量，从而可使用分类器对句子进行分类
* **词频**表示一个单词在给定文档中出现的频次。在统计词频时，由于文档长度的差异，同一单词的词频可能会有很大差异，因此需要将其规范化，使不同文档的频次在同一条件下进行比较。为了实现规范化，使用**频次除以文档中所有单词的个数**
* **逆文档频率**表示给定单词的重要性。当需要计算词频时，假定所有单词是同等重要的，同时会设置一个系数让经常出现单词的权重降低。使用**计算稳定的总数目除以该单词出现的文档数目的比值，该比值取对数即为逆文档频率**

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer  # tf-idf变换器

# 训练部分：1.使用CountVectorizer提取特征；2.使用TfidTransformer进行tf-idf转化；3.使用MultinomialNB训练分类器
# 选择类型列表，这些类型是加载的新闻组数据集的一部分
category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles', 'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', 'sci.space': 'Space'}

# 定义训练集数据；创建训练器，提取特征
train_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=7)
vectorizer = CountVectorizer()
x_train_termcounts = vectorizer.fit_transform(train_data)
x_train_termcounts_shape = x_train_termcounts.shape

# 定义tf-idf变换器对象并训练（使用提取的特征信息（即词频）进行训练，运用tf-idf方法转化）
tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_termcounts)

# 定义多项式朴素贝叶斯分类器对象并进行训练
# train_data.target获取了训练对象的分类信息
mnb_classifier = MultinomialNB().fit(x_train_tfidf, train_data.target)


# 测试部分
# 要分类的句子
inputdata = ["The curveballs of right handed pitchers tend to curve to the left",
			"Caesar cipher is an ancient form of encryption",
			"This two-wheeler is really good on slippery roads"]

# 依次对测试数据进行上述1、2、3的步骤
input_termcounts = vectorizer.transform(inputdata)
input_tfidf =  tfidf_transformer.fit_transform(input_termcounts)
predicted_categories = mnb_classifier.predict(input_tfidf)

# 打印输出
for sentence, category in zip(inputdata, predicted_categories):
	print(sentence)
	# 分类预测输出项需使用target_names提取后，回到类型列表中进行匹配
	print(category_map[train_data.target_names[category]])
```
## 6.6 示例：分析句子情感
此示例使用NLTK的朴素贝叶斯分类器进行分类。由于NLTK分类器需要的输入数据格式是**字典**，因此此处将使用字典格式。在特征提取函数中，基本提取了所有的**唯一单词**
```python
import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews  # 导入数据的库

def extract_feature(word_list):  # 提取特征的函数
	return dict([(word, True) for word in word_list])  # 以单词和bool值生成字典

if __name__ == "__main__":
	# 加载积极和消极的评论
	pos_fileids = movie_reviews.fileids('pos')
	neg_fileids = movie_reviews.fileids('neg')

	# 提取积极和消极特征
	feature_pos = [(extract_feature(movie_reviews.words(fileids=[f])), 'Positive') for f in pos_fileids]
	feature_neg = [(extract_feature(movie_reviews.words(fileids=[f])), 'Negative') for f in neg_fileids]

	# 分割训练集和测试集
	threshold_factor = 0.8
	threshold_pos = int(threshold_factor * len(feature_pos))
	threshold_neg = int(threshold_factor * len(feature_neg))
	feature_train = feature_pos[:threshold_pos] + feature_neg[:threshold_neg]
	feature_test = feature_pos[threshold_pos:] + feature_neg[threshold_neg:]

	# 训练朴素贝叶斯分类器：NLTK已经调教好的分类器，直接进行使用
	nb_classifier = NaiveBayesClassifier.train(feature_train)

	# 计算正确率（使用测试集）、提取10个最有信息量的单词（在积极和消极中）
	accuracy = nltk.classify.util.accuracy(nb_classifier, feature_test)
	most_infomative_words = [item for item in nb_classifier.most_informative_features()[:10]]

	# 测试部分
	# 输入句子
	inputsentence = ["It is an amazing movie",
					 "This is a dull movie. I would never recommend it to anyone.",
					 "The cinematography is pretty great in this movie",
					 "The direction was terrible and the story was all over the place"]
	for sentence in inputsentence:
		print(sentence)
		probdist = nb_classifier.prob_classify(extract_feature(sentence.split()))  # 判断输入句子的词性
		pred_sentiment = probdist.max()
		probability = probdist.prob(pred_sentiment)  # 计算判断的概率大小

		print("Predicted sentiment: ", pred_sentiment)
		print("Probability: ", round(probability, 2))
```
## 6.7 示例：使用主题建模识别文本模式
主题建模指识别文本数据隐藏模式的过程，其目的是**发现一组文档的隐藏主题结构**，其通过识别文档中最有意义、最能表征主题的词来确定主题内容、实现主题分类

示例中使用的方法说明：
* 预处理
	* 正则表达式：去除含标点或标记的单词
	* 停用词去除：减少常用词（be动词或连词）的噪声干扰
	* 词干提取：获得单词原型
* 本例使用**隐含狄利克雷分布（Latent Dirichlet Allocation,  LDA）**技术来构建主题模型，其可将文档表示为不同主题的混合，这些主题可以有概率地输出一些单词

```python
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from gensim import models, corpora
from nltk.corpus import stopwords

# 文本数据预处理类
class Preprocessor:
    # 对各类操作进行初始化（创建各类“器”）
    def __init__(self):
        self.tokenizer = RegexpTokenizer(r'\w+')  # 创建正则表达式解析器
        self.stop_words_english = stopwords.words('english')  # 创建停用词列表
        self.stemmer = SnowballStemmer('english')  # 创建词干提取器

    # 标记解析、移除停用词、词干提取
    def process(self, input_text):
        # 标记解析
        tokens = self.tokenizer.tokenize(input_text.lower())  # 将单词均还原为小写形式
        # 移除停用词
        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]
        # 词干提取
        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]
        return tokens_stemmed

if __name__ == "__main__":
    inputdata = []  # 输入文件，两层列表，一层为一行文本

    # 创建预处理对象，并创建一组经过预处理的文档
    preprocessor = Preprocessor()
    processed_tokens = [preprocessor.process(x) for x in data]

    # 创建基于标记文档的词典
    dict_tokens = corpora.Dictionary(processed_tokens)

    # 创建文档-词矩阵
    corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]

    # 使用隐含狄利克雷分布进行主题建模
    # 定义相关参数
    num_topics = 2  # 两个主题
    num_words = 4  # 每个主题提取4个单词
    # 初始化LDA对象
    ldamodel = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dict_tokens, passes=25)
    # 计算获取两个主题中具有最大贡献的词
    # words的第一项是贡献度，第二项是对应的词
    words = ldamodel.print_topics(num_topics=num_topics, num_words=num_words)
```