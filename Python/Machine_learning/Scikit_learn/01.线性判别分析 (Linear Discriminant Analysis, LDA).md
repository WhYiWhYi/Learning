## 1. LDA概念

LDA通过将输入的数据投影到由**最大化类间分离**所组成的线性子空间来进行有监督的降维。其在模型训练时可提高计算效率以及避免过拟合

基本思想：给定训练集，将训练集样例投影到**一条直线**（线性判别提供线性边界，若为二次判别则提供二次边界）上，使**同类样例投影点尽可能接近，异类样例投影点尽可能远离**。在对测试集进行分类时，将其投影到该直线上，依据投影点的位置来确定新样本的类别

缺点：LDA不适合对非高斯分布样本进行降维；LDA在样本分类信息依赖方差而非均值时，降维效果较差；LDA可能导致过拟合

## 2.LDA内部逻辑
1.标准化d维（即有d个特征）数据集

2.计算每个类别的d维均值向量

3.计算类内散度矩阵S~W~（within-class scatter matrix）和类间散度矩阵S~B~（between-class scatter matrix）

4.线性判别式及特征计算

5.按特征值降序排列，与对应的特征向量成对排序

6.选择最具线性判别性的前k个特征，构建变换矩阵W~d×k~

7.通过变换矩阵将源数据投影至k维子空间

## 3.LDA实现
基于wine数据集的自我实现（https://www.cnblogs.com/chenzhenhong/p/13504526.html）

```Python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# 加载数据
dt_wine = pd.read_csv("wine.data", header=None)

# 分割训练集和测试集
x, y = dt_wine.iloc[:, 1:].values, dt_wine.iloc[:, 0].values
	# iloc: pandas中用于提取数据的函数，其可提取行也可提取列，此处为提取列
	# values: 将提取的数据以对应的二维NumPy数组形式返回
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, random_state=0)

# 将数据集（x）标准化：标准化单位方差
sc = StandardScaler()
x_train_std = sc.fit_transform(x_train)
x_test_std = sc.fit_transform(x_test)

# 计算均值向量（用于Sw和Sb的计算）
np.set_printoptions(precision=4)
	# set_printoptions: 控制打印输出：保留4位小数
mean_vecs = []
for label in range(1, 4):  # wine数据集共三类数据
	mean_vecs.append(np.mean(x_train_std[y_train == label], axis=0))
>>> Mean Vector
>>> [[0.9066, -0.3497, 0.3201, ...],  # 类别1中13个特征的均值向量
>>> [-0.8749, -0.2848, -0.3755, ...],  # 类别2中13个特征的均值向量
>>> [0.1992, 0.866, 0.1682]]  # 类别3中13个特征的均值向量

# 计算类内散度矩阵：为每个样本的散布矩阵之和
k = 13  # wine数据集共13个特征
Sw = np.zeros((k, k))  # 类内散度矩阵的规模为13*13
for label in range(1, 4):
	Si = np.cov(x_train_std[y_train == label].T)
		# cov: 可直接求得矩阵的协方差矩阵
	Sw += Si

# 计算类间散度矩阵
mean_all = np.mean(x_train_std, axis=0)
Sb = np.zeros((k, k))
for i, col_mv in enumerate(mean_vecs):
	n = x_train[y_train == i+1, :].shape[0]
	col_mv = col_mv.reshape(k, 1)  # 列均值向量
	mean_all = mean_all.reshape(k, 1)  # 所有样本总均值向量（不分类情况下计算特征平均值）
	Sb += n * (col_mv - mean_all).dot((col_mv - mean_all).T)
		# dot: 处理一维数组得到数组的内积；处理二维数组值得到矩阵积：第一个矩阵中与该元素行号相同的元素与第二个矩阵与该元素列号相同的元素，两两相乘后再求和

# 计算广义特征值（广义瑞利商，generalized Rayleigh qeuotient）
eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))  # 计算广义特征值
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]
eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)

for eigen_vals in eigen_pairs:  # 输出每个特征的广义特征值
	print(eigen_vals[0])
>>> 349.617808905994  # 依据数字可得前两个特征占据了大部分数据集特征
>>> 172.76152218979385
>>> 3.192705060156757e-14
>>> ...

# 构建变换矩阵（一列一个特征）
trans_matrix = np.hstack((eigen_pairs[0][1][:, np.newaxis].real, eigen_pairs[1][1][:, np.newaxis].real))
	# hstack：将数组按列水平合并
	# [0]、[1]: 提取前两个特征对应的数组
	# [1]、[1]: 提取前两个特征数组对应的信息
	# newaxis: 插入新维度；[np.newaxis,:]：在行上增加维度；[:,np.newaxis]：在列上增加维度
	# real: 返回数组中复杂参数的实部，若变量具有复杂元素，则返回类型为float；eg. 2+3j→2.

# 通过变换矩阵将样本数据投影到低维空间
x_train_lda = x_train_std.dot(trans_matrix)

colors = ['r', 'g', 'b']
marks = ['s', 'x', 'o']
for l, c, m in zip(np.unique(y_train), colors, marks):  # 绘制点
	plt.scatter(x_train_lda[y_train == l, 0],
				x_train_lda[y_train == l, 1] * -1,
				c=c, label=l, marker=m)
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.show()
```

基于sklean的实现
link: https://scikit-learn.org/stable/modules/lda_qda.html#dimensionality-reduction-using-linear-discriminant-analysis
link: https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis

LinearDiscriminantAnalysis

* Parameters
    * solver: 求解LDA超平面特征矩阵使用的方法
        * **"svd"**: 奇异值分解（默认设置）。多用于具有大量特征的数据集
        * "lsqr": 最小二乘分解。可以与“shrinkage”或自定义“covariance estimator”联合使用
        * "eigen": 特征值分解。特征数不多时可使用，可以与“shrinkage”或自定义“covariance estimator”联合使用
    * shrinkage: 正则化参数，可增加LDA分类的泛化能力，仅用于降维时可不关注
        * 只能与"lsqr"或"eigen"一起使用；若使用了“covariance estimator”，则需设置为None
        * **None**: 无shrinkage（默认设置）
        * "auto": 基于Ledoit-Wolf lemma自动设置的值
        * **[0, 1](folat)**: 固定设置数字（可进行不同的设置以进行调参）
    * priors: 类别权重，可在分类模型中指定不同类别的权重，仅用于降维时可不关注
        * **None**: 默认设置
    * n_components: 进行LDA降维时的目标维数，降维时需输入该参数，此参数仅影响transform方法
        * 范围为[1, n_classes-1 or n_feature]之间的整数
        * **None**: 默认设置
    * store_covariance: 计算协方差矩阵，用于"svd"
        * **False**: 不计算协方差矩阵（默认设置）
    * tol: 运用"svd"方法进行求解时，用于秩估计的阈值，基于该阈值舍弃奇异值不显著的维度
        * **10^-4^**: 默认设置
    * covariance_estimator: 协方差矩阵估计器，用于"lsqr"和"eigen"
        * **None**: 默认设置
    * random_state: 随机数种子
* Attributes
	* **coef_**
	* intercept_
	* covariance_
	* explained_variance_ratio_
	* means_
	* priors_
	* scalings_
	* xbar_
	* classes_
	* n_features_in_
	* feature_names_in_
* Methods
	* decision_function
	* fit
	* fit_transform
	* get_params
	* predict
	* predict_log_proba
	* predict_proba
	* score
	* set_params
	* transform

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda = LDA()  # 创建LDA分类器

lda.fit(x_train, y_train)  # 使用训练集数据进行拟合
lda.score(x_train, y_train)  # 计算得分（平均正确率）
>>> 0.9732142857142857

y_test = lda.predict(x_test)  # 对测试集进行预测
>>> array([1, 2, 0, 1, 2, 1, 1, 1, ...])  # 测试集结果
lda.score(x_test, y_test)  # 计算得分（平均正确率）
>>> 1.0
```