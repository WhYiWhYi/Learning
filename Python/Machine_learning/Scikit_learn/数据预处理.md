## 1 数据预处理技术

真实世界中的原始数据是机器学习算法无法理解的，为了使机器学习理解算法原始数据，需要对数据进行预处理，接下来将以示例数据为例，介绍几种常见的数据预处理技术
```python
# 示例数据：一行一个样本，一列一个特征
data = np.array([[3, -1.5, 2, -5.4], [0, 4, -0.3, 2.1], [1, 3.3, -1.9, -4.3]])
```
### 1.1 均值移除 (Mean removal)
在某些模型中（如回归算法、KNN算法等），各个特征的**基准值**和**分散度**不同可能会大大影响模型的预测效果，因此在应用这些模型的时候，要确保**每一个特征列的数值都在类似的数据范围之间，防止某一个特征列数据数值过大而影响结果**（特征的方差如果比其他特征大几个数量级，该特征就会在学习算法中占据主导位置）。均值移除即可做到**消除特征彼此间的偏差**，使每一个特征的**均值为0，方差为1**

均值移除
* 以**特征**为单位进行操作
* 标准化的一种，为**z-score标准化 (zero-mean normalization)**
* 原理：均值化为0，标准差化为1（符合标准正态分布）
* 数学方法：对于特征数据集X=[a1, a2, a3, ...]，其均值为μ，标准差为σ。则每一个值均值移除的方法为：**(ai-μ)/σ**
```python
# 普通方法（依据原理实现）
for col in data.T:  # 因为特征在列上，因此处理需要进行array的转置
	col_mean = col.mean()
	col_std = col.std()
	col -= col_mean
	col /= col_std
```
一般使用调用sklearn库方法实现均值移除
```python
from sklearn import preprocessing

data_standardized = preprocessing.scale(data)  # 传入均值移除目标array
```
### 1.2 范围缩放 (Scaling)
范围缩放用于**统一数据矩阵中不同特征的最大值和最小值范围（通常为1-0）**。其数据处理效果与均值移除类似，也是使**每一个特征列的数值都在类似的数据范围之间**。将不同特征缩放到一个指定的最大、最小值间可以对方差非常小的属性增强其稳定性，也可维持稀疏矩阵中为0的条目

范围缩放
* 以**特征**为单位进行操作
* 标准化的一种，为**0-1标准化 (0-1 normalizzation)**
* 原理：离差标准化，进行线性变换使范围变为指定范围，通常为0-1
* 数学方法：对于特征数据集X=[a1, a2, a3, ...]，其最大值为amax，最小值为amin。则每一个值范围缩放的方法为：**(ai-amin)/(amax-amin)**
```python
from sklearn import preprocessing

data_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))  # 创建范围缩放对象（器），缩放范围为0-1
data_scaled = data_scaler.fit_transform(data)  # 进行范围缩放
```
### 1.3 归一化 (Normalization)

数据归一化用于需要对样本的**特征向量（向量：指一个样本所含的多个特征）**的值进行调整时，以**保证每个特征向量的值都缩放到相同的数值范围**。这一方法常用于确保数据没有因为特征的基本性质而产生较大差异，确保**所有特征的数据处于同一数量级**。提高不同特征数据的可比性

归一化：
* 以**样本**为单位进行操作
* 原理：用特征占比表示特征大小，调整方式有两种：调整为**L1范数**或**L2范数**，机器学习常用方法为调整为**L1范数**
* 数学方法：对于样本数据集（包含多个特征）Y=[f1, f2, f3, ...]。分子为fi，在L1范数中，分母为：Σ|fi|；L2范数中，分母为：√(Σ|fi|)2
```python
from sklearn import preprocessing

data_normalized = preprocessing.normalize(data, norm="l1")  # 传入归一化目标array，调整方式为l1
```
### 1.4 二值化 (Binarization)
二值化为将数值特征向量转换为布尔类型向量，也即只有True和False两类。二值化方法对于所有的特征值使用同一个阈值标准，因此对于特征值差异较大的特征，不适合使用二值化的方法。此外，注意**二值化方法不可逆**，若希望可逆，则可考虑使用独热编码

二值化
* 对每一个特征值进行操作
* 原理：设定阈值，用0和1来表示各个特征值相对于某个给定阈值高于或低于它的元素
```python
from sklearn import preprocessing

data_binarizer = preprocessing.Binarizer(threshold=1.4)  # 创建二值化对象，设定阈值为1.4
data_binarized = data_binarizer.transform(data)
```
### 1.5 标签编码 (Label Encoding)
监督学习中需要处理各种各样的标签，可能是数字、可能是单词。大多数情况下，为了方便人们理解，标签多采用单词的形式。标签编码就是**把单词转化为数值形式**，使算法懂得如何操作标记

标签编码
* 以**特征**为单位进行操作
* 将离散型变量转化为连续的数值型变量，即对不连续的数字或文本进行编号
* 注意：**不同的特征编码表不同且相互独立**，**编码和解码要使用对应特征的编码表**
* 标签编码有时候会产生一些限制，如有[dog,cat,dog,mouse,cat]可转换为[1,2,1,3,2]。这里就产生了一个奇怪的现象：dog和mouse的平均值是cat（算法中要对特征数值进行一定计算的）
```python
import numpy as np
from sklearn import preprocessing

label_classes = np.array(['audi', 'ford', 'ford', 'bmw', 'toyota', 'ford', 'audi'])  # 待编码特征

# 创建标签编码对象：编码和解码都需要使用
# 如果对不同的特征进行编码，需要使用不同的编码器
label_encoder = preprocessing.LabelEncoder()

# 传入目标array进行编码，同时构建标签编码表字典（训练时推荐使用）
label_encoded_fit = label_encoder.fit_transform(label_classes)
# 传入目标array进行编码，使用已有的编码表字典（预测试使用：使用训练时编码好的字典进行对应）
label_encoded = label_encoder.transform(label_classes)

# 逆解码：获取原始数据
# inverse_transform()方法中不能传入空数组，最好在使用前进行判断，判断array不为空
label_inv = label_encoder.inverse_transform(label_encoded_fit)
```
### 1.6 独热编码 (One-Hot Encoding)

独热编码又称一位有效编码，其使用N位状态寄存器来对特征的**N个状态**进行编码。每个状态都有自己独立的寄存器位，并且在任意时候，其中只有一位有效。即**有多少个状态就有多少个bit，而且只有一个bit为1，其他全为0**的一种编码。这种编码方式把特征向量的每个特征与特征的非重复总数相对应，通过one-of-k（k指的是某一特征的非重复计数值，也即在该特征中有多少种类型的特征值）的形式对每个值进行编码。按照这种方式编码**可以更加有效地表示空间**（以空间表示特征）
* 将编码看作向量，则每个特征的每个值都是一个指向K维空间中的独特变量（在特征方向上长度固定为1，其他方向为0）
* 这种编码方式使特征长度固定为1，则同一特征中不同特征值的取值是一致的
```python
sex: {male, female}  # feature1
class: {class1, class2, class3}  # feature2

# 进行独热编码：有几个状态就有几位，每个状态码只有一个独特位是1，其他位置均为0
# 如果有多个特征，则多个特征单独进行编码：第i列特征不能使用第j列特征相同的数字的编码来编码
sex: {10, 01}  # 可看作指向空间的二维向量：特征值所在的一维是1，其余维度为0
class: {100, 010, 001}  # 可看作指向空间的三维向量：特征值所在的一维是1，其余维度为0

# 多个特征的编码拼接起来即为完整的独热编码
sample1: {male, class2}  -->  10010 (10 + 010)
sample2: {female, class1}  -->  01100 (01 + 100)
```

独热编码：
* 作用：对于离散型的数据（如上述示例的sex和class），如果单纯使用 {0,1,2} 进行编码（即标签编码），在模型训练中**不同的值可能会使同一特征在样本中的权重发生变化**（当以数字进行表示时，该特征中越靠后被编码的特征值将会具有更大的数字，也即更大的权重）。独热编码的作用在于**对离散型的分类型数据数字化，解决分类器不好处理属性数据的问题，且在一定程度上起到扩充特征的作用**，如将文本分类属性的性别进行数字化
* 为什么使用独热编码
	* 大部分算法是**基于向量空间中的度量**进行计算，为了使**非偏序关系**的变量取值不具有**偏序性**，且到原点的距离等距，需要使用独热编码
	* 使用独热编码后，可以将离散特征的取值扩展到欧氏空间（任意维度，取决于特征种类），离散特征的某个值就对应欧式空间的某个点，使特征之间的距离计算更加合理
		* 为什么要映射到欧氏空间：在回归，分类，聚类等机器学习算法中，特征之间常用的**距离的计算或相似度的计算**都是在欧氏空间的计算（余弦相似性，基于欧氏空间）
	* 编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化**（编码后的特征可适用于前述的各类标准化中）**
* 优缺点
	* 优点
		* 能够很好地处理非连续型数值特征
		* 在一定程度上扩充了特征（如性别特征扩充为“男”、“女”两个特征，且二者向量值相同，储存在不同方向）
	* 缺点
		* 当特征的特征值种类很多时，特征空间会变得非常大，稀疏矩阵会很稀，占内存空间大
		* 占空间大时可使用PCA来减少维度（One-Hot Encoding+PCA组合在实际中也非常有用）
* 独热编码的使用情景
	* 使用
		* 独热编码用于解决类别型数据的离散值问题
		* 基于**参数**、基于**距离**的模型，需要进行特征的归一化，因此需要对类别型特征进行独热编码以适用归一化
	* 不使用
		* 离散型特征无需独热编码即可很合理地计算出距离，则没必要使用独热编码
		* 基于**树**的方法无需进行特征的归一化，如随机森林、bagging、boosting等。对于决策树来说，独热编码会增加数的深度
```python
from sklearn import preprocessing

# 创建独热编码对象
# sparse：是否采用压缩格式
# dtype：元素类型
encoder = preprocessing.OneHotEncoder(sparse=False, dtype=int)  # 创建独热编码对象

# 传入目标array进行独热编码，同时构建编码表字典（推荐使用）
data_encoded_fit = encoder.fit_transform(data)
# 传入目标array进行独热编码，使用已有的编码表字典
# 使用已构建过的独热编码字典进行编码,前提是特征中的状态必须是已有编码字典里的状态，如果存在未出现过的状态，则编码会出现错误
data_encoded = encoder.transform(data) 
```