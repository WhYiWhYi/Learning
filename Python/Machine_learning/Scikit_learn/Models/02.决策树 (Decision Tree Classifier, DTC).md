## 1 DTC概念及原理

### 1.1 基本概念
决策树基于**树结构**来进行决策，这十分接近人类在面临决策问题时的处理机制

一般地，一棵决策树包含一个根结点、若干个内部结点及若干个叶结点。叶结点对应**决策结果**，其他每个结点都对应一个**属性测试**：每个结点包含的样本集合可根据属性测试的结果被划分到子结点中。根节点包含样本全集，从根节点到每个叶结点的路径即为一个判定测试序列

决策树的生成是一个递归过程。共有三种情况可导致递归返回：当前结点包含的样本全部属于同一类别；当前属性集为空或所有样本在所有属性上取值相同；当前结点包含的样本集合为空
### 1.2 划分选择
1. 信息熵
	* 信息熵是度量样本集合纯度最常用的指标
	* 假设当前样本集合D中第k类样本所占比例为p~k~，则信息熵为：**Ent(D)=-Σ~k=1~P~k~log~2~(p~k~)**
2. 信息增益（ID3）
	* 假设离散属性a有V个可能的取值（a^1^, a^2^, ..., a^V^）。
	* 基于a对样本集合D进行划分，则会产生V个分支节点，其中第v个分支结点上包含了样本集合D中在属性a上取值为a~v~的样本，记为D~v~
	* 通过计算D~v~的信息熵，并考虑在样本属性a下不同分支结点所包含样本数的不同，对其赋予权重（|D~v~|/|D|）
	* 基于此，可计算出用属性a对样本集合D进行划分所获得的信息增益：**Gain(D,a)=Ent(D)-Σ~v=1~|D~v~|/|D|Ent(D^v^)**
	* 一般而言，**信息增益越大，使用属性a进行划分所获得的纯度提升越大**（Gain(D,a)越大越好）
3. 增益率（C4.5）
	* 信息增益可能对**可取值数目较多的属性**有所偏好，为减少此类偏好带来的不良影响，可使用增益率
	* 属性的固有值：**IV(a)=-Σ~v=1~|D~v~|/|D|log2(|D~v~|/|D|)**在属性a可取值的数目越多时值越大
	* 增益率：**Gain_ratio(D,a)=Gain(D,a)/IV(a)**
	* 需注意的是，增益率准则对**可取值数目较少的属性**有所偏好，因此其主要目的是：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的**
4. 基尼指数（CART）
	* 基尼值用于数据集D的纯度：**Gini(D)=Σ~k=1~Σ~k≠k'~(p~k~p~k‘~)**，直观来说，其反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率
	* 基尼指数：**Gini_index(D,a)=Σ~v=1~|D~v~|/|D|Gini(D^v^)**
	* 一般而言，选择可使**划分后基尼指数最小的属性作为最优划分属性**

## 2.DTC优缺点
（https://sklearn.apachecn.org/docs/master/11.html）
1. 特点
	* 由于训练时数据点的数量，决策树的使用开销水平呈指数分布（训练树模型的时间复杂度是参与训练数据点的对数值）
	* 即使模型假设的结果与这是模型所提供的数据有所相悖，其表现依旧良好

2. 优势
	* 便于理解和解释（使用**白盒**模型，所有的判断都是基于布尔逻辑来解释），树的结构可以可视化
	* 训练所需数据较少
	* 能处理数值型数据和分类数据
	* 能够处理多路输出的问题
	* 可以通过数值统计测试来验证该模型以实际验证模型的可靠性

3. 劣势
	* **容易产生过于复杂的模型，对数据的泛化性能会很差**，因此**剪枝**在决策树模型中十分重要
	* 决策树模型的生成可能不稳定，数据的变化可能会导致完全不同的树生成。这个问题可以通过**集成**来缓解
	* 有些概念很难被决策树学习到，因为决策树很难清楚的表述这些概念
	* 占据主导地位的类会使创建的决策树出现偏差，因此拟合前最好先对数据进行平衡
## 3.DTC实现
### 3.1 基本实现
基于sklearn实现
link: https://scikit-learn.org/stable/modules/tree.html
link: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier

DecisionTreeClassifier
* parameters
    * criterion: 进行决策分类的算法
        * **"gini"**: 基尼系数（默认设置），使用CART算法
        * "entropy": 信息熵，类似ID3、C4.5的最优特征选择方法
    * splitter: 寻找结点的方法
        * **"best"**: 在所有特征中寻找最优特征作为切分点（默认设置），适合样本量较小时
        * "random": 当样本量很大时，推荐使用随机切分
    * max_features: 在寻找最佳切分点时，考虑的特征数目
        * **None**: 不做设置（默认设置），max_features=n_features
        * int: 设置考虑的特征数目，max_features=max_features
        * float: 设置考虑的特征数目，int(max_features * n_features)
        * "auto": 随机考虑特征数，考虑的特征数为max_features=sqrt(n_features)
        * "sqrt": 随机考虑特征数，考虑的特征数为max_features=sqrt(n_features)
        * "log2": 随机考虑特征数，考虑的特征数为max_features=log2(n_features)
    * class_weight: 指定样本各类别的权重
        * **None**: 不做设置（默认设置）
        * "balanced": 算法自行计算权重，样本量较少的类别所对应的权重会增加
        * dict or list of dict: 通过字典指定样本的权重，eg. [{1:1}, {2:5}, {3:1}, {4:1}]
    * max_depth: 剪枝相关。设置创建子结点的最大深度，在数据量大、特征数目多时可加以限制以改善过拟合问题。可先设置较小的深度进行拟合，观察拟合情况后，再决定是否要加大深度
        * **None**: 默认设置
    * min_samples_split: 剪枝相关。当结点的样本数少于该设置时，将不再进一步创建子结点，数据量大时可适当增大该值的设置
        * **2**: 默认设置
    * min_samples_leaf: 剪枝相关。限制叶结点的最少样本数，若某叶结点数目小于该设置时，则会和兄弟结点一起被剪枝，数据量大时可不考虑此值
        * **1**: 默认设置
    * min_weight_fraction_leaf: 剪枝相关。限制叶结点所有样本权重和的最小值，若小于该值，则会和兄弟结点一起被剪枝，如果样本缺失值较多或样本的分布类别偏差很大，则需引入该参数
        * **0**: 默认设置，即不考虑权重问题
    * max_leaf_nodes: 剪枝相关。限制最大叶结点数，若加入限制，则算法会在最大叶结点数内构建最优的决策树，如果特征不多可不考虑该值，若特征较多可加以限制（通过交叉验证判断设置值）
        * **None**: 默认设置
    * min_impurity_decrease: 剪枝相关。通过限制不纯度（基尼系数、信息增益等）限制决策树的增长。若结点的不纯度小于该阈值则不再创建子结点
        * **"0.0"**: 默认设置
    * ccp_alpha: 用于最小成本修剪度修剪的复杂度参数，默认不执行修剪
    * random_state: 随机数种子，用于对寻找最佳切分点（max_features）时的随机设置
		* **None**: 默认设置
* Atributes
	* classes_
	* **feature_importances_**
	* max_features_
	* n_classes_
	* n_features_
	* n_features_in_
	* feature_names_in_
	* n_outputs_
	* tree_
* Methods
	* apply
	* cost_complexity_pruning_path
	* decision_path
	* fit
	* get_depth
	* get_n_leaves
	* get_params
	* predict
	* predict_log_proba
	* predict_proba
	* score
	* set_params

```python
from sklearn.tree import DecisionTreeClassifier as DTC

dtc = DTC()  # 此处可自行设置树深度及剪枝相关参数

dtc.fit(x_train, y_train)

dtc.feature_importances_  # 输出特征的相对重要性

dtc.predict(x_test)  # 预测样本类别
dtc.predict_prob(x_test)  # 预测每个类别的概率
```
### 3.2 可视化
基于sklearn的决策树可视化有如下几种方式
```python
#  基于matplotlib中plot_tree可视化
from sklearn import tree
import matplotlib.pyplot as plt

tree.plot_tree(dtc) 
plt.show()


# 基于Graphviz可视化（将决策树转化为dot文件后使用graphviz可视化）
from sklearn import tree
import pydotplus
import graphviz

dot_data = tree.export_graphviz(dtc,
								out_file="",
								feature_names=,
								class_names=,
								filleed=True,
								rounded=True,
								special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data)
graph.write_pdf("iris.pdf")  # 保存文件


# 将结果以text的结果展示（以iris数据集为例）
from sklearn import tree

iris = load_iris()
dtc = DTC()
dtc = dtc.fit(x_train, y_train)
text = tree.export_text(dtc, feature_names=iris['feature_names'])
print(text)
```