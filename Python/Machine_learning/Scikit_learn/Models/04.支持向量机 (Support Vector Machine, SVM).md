## 1.基本概念

支持向量机（SVM）模型是一种监督学习算法，用于对二值和分类响应数据进行分类

数学化描述SVM：
* 训练样本数据：n个样本（x~1~, x~2~, ..., x~n~），p个输入特征（对于样本i有：X~i~ = (x~i1~, x~i2~, ..., x~ip~)^T^），一个输出 y
* 目的：以训练样本为研究对象，在样本的特征空间中找到一个超平面 **W^T^X+b = 0** (W = ω~1~,ω ~2~, ...,ω ~p~，针对每一个特征，都有对应的W^T^X)，将不同类别的样本分开
	* X是样本的向量表示，以二维数据（具有两个输入特征）为例：X^T^ =  (x1, x2)。由于向量一般为列向量，当以行向量表示时，就加上转置**T**
	* W^T^是一组行向量，是未知参数，可以将其中的每一项 ω~i~ 理解为 x~i~ 的系数
	* 行向量和列向量相乘（W^T^X）得到一个**1\*1**的矩阵，即一个实数

以两个特征为例说明如何寻找最佳线性分类器（原理介绍）：
* 在理想状态下，二维中的两类点是**完全线性可分**的，但此时可以有多条线能够分割数据，此时就需要确定哪一条是最佳的
* SVM中，对最好分类器的定义是**最大边界超平面，即距两个类别的边界观测点最远的超平面**
![](E:\SynologyDrive\READING\CS\note\机器学习\img\3.2.1.SVMtheory.png)
* 从上图可见，在寻找最佳分类器时，只会考虑A类中离B类最近的点和B类中离A类最近的点，即1、2、3；a、b、c这些点。因为**最大边界超平面仅取决于两类别的边界点**，这些边界点即称**支持向量**
* 详细原理可见：https://zhuanlan.zhihu.com/p/74484361

SVM的优势
* SVM的优势在于解决小样本、非线性和高维的回归和二分类问题
* **小样本**：与问题的复杂度相比，SVM要求的样本数相对较少
* **非线性**：SVM擅长应对样本数据线性不可分的情况，主要通过**核函数和松弛变量**实现（SVM精髓）
* **高维**：即便数据为高维数据，但SVM产生的分类器很简洁，使用的样本信息很少（仅使用支持向量），可以**有效避免过度拟合**
## 2.SVM实现
基于sklearn的实现
link: https://scikit-learn.org/stable/modules/svm.html#classification
link: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC

SVC
* Parameters
    * C: 正则化参数，正则化的强度与设置值成反比。**该值越大，对误分类的惩罚增大，趋向于对训练集全分对的情况，即对训练集测试时准确率要求高，但泛化能力弱；该值小，对误分类的惩罚减小，容错率高，泛化能力强**
        * **1.0**: 默认设置
    * kernel: 支持向量机中使用的核函数类型
        * "linear": 线性函数
        * "poly": 多项式函数
        * **"rbf"**: RBF函数
        * "sigmoid"
        * "precomputed"
    * degree: 多项式函数的维度，**kernel为"poly"时设置才有效，其余核函数会被忽略**
        * **3**: 默认设置
    * gamma: 核函数参数，**kernel为"poly"、"rbf"、"sigmoid"时设置才有效，其余核函数会被忽略**。**值越大，训练集拟合越好，泛化能力越差；值越小则反之**
        * **"scale"**: 计算方式为1/(n_features * X.var())（默认设置）
        * "auto": 计算方式为1/n_features
    * coef0: 核函数的常数项，**kernel为"poly"、sigmoid"时设置才有效**
        * **0.0**: 默认设置
    * shrinking: 是否采用shrinking heuristic方法
        * **True**: 默认设置
    * probability: 是否采用概率估计
        * **False**: 默认设置
    * tol: 自动停止训练的误差值大小
        * **1e-3**: 默认设置
    * cache_size: 核函数的缓存大小
        * "200.0": 单位为MB（默认设置）
    * class_weight: 针对不同类别的C的参数权重：对于第i类，权重为class_weight[i]*C
        * **None**: 不做设置（默认设置）
        * "balanced": 算法自动调整权重
        * dict: 传入字典设置权重
    *  max_iter: 迭代的次数
        * **-1**: 不做限制（默认设置）
        * int: 手动进行迭代次数限制
    * decision_function_shape: 是否像其他分类器一样返回决策函数
        * **"ovr"**: 形状为(n_samples, n_classes)的一对一决策函数（默认设置）
        * "ovo": 形状为(n_samples, n_classes\*(n_classes - 1)/2)的决策函数
    * break_ties: 0.22版本的新设置（可暂时忽略）
        * **False**: 默认设置
    * verbose: 控制在训练过程中返回信息的详细程度，设置太大会影响运行速度
        * **0**: 训练时不返回任何信息（默认设置）
        * 1: 训练时返回部分信息
        * 2: 训练时返回寻览步骤都详细信息
    * random_state: 随机数种子
* Attributes
	* class_weight_
	* classes_
	* **coef_**
	* dua_coef_
	* fit_status_
	* intercept_
	* n_features_in_
	* feature_names_in_
	* support_
	* support_vectors_
	* n_support_
	* probA_
	* probB_
	* shape_fit_
* Methods
	* decision_function
	* fit
	* get_params
	* predict
	* predict_log_proba
	* predict_proba
	* score
	* set_params

```python
from sklearn.svm import SVC

# 线性
params = {'kernel': 'linear'}
svc = SVC(**params)

# 非线性
# 使用多项式函数初始化SVM对象
# degree：表明使用一个n次多项式方程。增加方程的次数可以使曲线更加弯曲，但训练时间也会更长
params = {'kernel': 'poly', 'degree': 3}

# 使用径向基函数初始化SVM对象
params = {'kernel': 'rbf'}

svc.fit(x_train, y_train)
y_predict = svc.predict(x_test)
```