## 1 集成方法

集成方法的目标是把多个使用**给定学习算法**构建的**基估计器**的预测结果结合起来，从而获得比单个估计器更好的泛化能力/稳健性

集成方法分类

1. 平均方法（averaging methods）
	* 原理：构建多个**独立的**估计器，然后取**它们的预测结果的平均**。一般来说组合后的估计器是会比单个估计器要好，因为**方差减小**了
	* eg. Bagging方法、随机森林
		* Bagging方法：在原始训练集中**随机抽取训练子集**，并在子集上构建一系列**黑盒估计器**的多个实例，然后把这些估计器的预测结果结合起来。其在构建模型的时候**引入随机性**来**减少估计器的方差**。Bagging方法使用极为简单的方式对模型进行改进而**无需修改背后的算法**，其在**强分类器和复杂模型**上表现良好（如完全生长的决策树）。Bagging方法分为多种，其主要区别在于**随机抽取训练子集方法的不同**
		* 随机森林：见下
2. Boosting方法（boosting methods）
	* 原理：**依次构建**多个基估计器，**每一个基估计器都在此前估计器组合的基础上尝试减少组合估计器的偏差**，这一方法**结合了多个弱估计器**，使集成的模型更为强大
	* eg. AdaBoost、梯度提升分类器
		* AdaBoost：核心原理为通过**反复修正样本数据的权重**来训练一系列弱估计器，由这些弱估计器的预测结果通过**加权投票（或加权求和）**的方式组合，得到最终的预测结果。初始化时，所有弱学习器的权重都设置为**1/N**，第一次迭代仅训练出一个弱学习器，而接下来的迭代则**将样本的权重逐个修改**，学习算法也将**重新应用修改的权重**。**随着迭代次数的增加，难以预测的样本（此前迭代中预测为错误结果的样本）权重会增加，而被预测为正确结果的样本权重会被降低**
		* 梯度提升分类器：见下

## 2.随机森林

### 2.1 基本概念
随机森林算法为**基于树**设计的扰动和组合技术，其通过在构造分类器的过程中**引入随机性**创建一组不同的分类器。在构建的过程中的随机性会产生**具有不同预测错误**的决策树，集成分类器的预测结果就是单个分类器预测结果的平均值，通过此可消除部分错误。随机森林虽然能够通过组合不同的树**降低方差**，但是有时会**略微增加偏差**。在实际问题中，**方差的降低通常更加显著**，所以随机森林能够取得更好地效果

在随机森林中，集成模型中的每棵树构建时的**样本**都是经**有放回抽样**得到的。在构建树的过程中进行结点分割时，所选的分割点是**所有特征的最佳分割点**
### 2.2 随机森林实现
基于sklearn实现
link: https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees
link: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier

RandomForestClassifier
* Parameters
    * n_estimators: 构建随机森林时的基估计器（即基础决策树）数量
        * **100**: 默认设置
    * criterion: 基估计器进行决策分类的算法
        * **"gini"**: 基尼系数（默认设置），使用CART算法
        * "entropy": 信息熵，类似ID3、C4.5的最优特征选择方法
    * bootstrap: 在构建树时是否使用随机采样
        * **True**: 默认设置
    * max_samples: 进行随机采样时，随机采样样本的数目上限（仅当bootstrap为"True"时可用）
        * **None**: 使用全部样本
        * int: 提取设置个数样本
        * folat: 提取全部样本的比例
    * oob_score: 是否使用未采样样本进行泛化得分评估（仅当bootstrap为"True"时可用）
        * "False": 默认设置
    * max_features: 在寻找最佳切分点时，考虑的特征数目
        * None: 不做设置，max_features=n_features
        * int: 设置考虑的特征数目，max_features=max_features
        * float: 设置考虑的特征数目，int(max_features * n_features)
        * **"auto"**: 随机考虑特征数（默认设置），考虑的特征数为max_features=sqrt(n_features)
        * "sqrt": 随机考虑特征数，考虑的特征数为max_features=sqrt(n_features)
        * "log2": 随机考虑特征数，考虑的特征数为max_features=log2(n_features)
    * class_weight: 指定样本各类别的权重
        * **None**: 不做设置（默认设置）
        * "balanced": 算法自行计算权重，样本量较少的类别所对应的权重会增加
        * "balanced_subsample"
        * dict or list of dict: 通过字典指定样本的权重，eg. [{1:1}, {2:5}, {3:1}, {4:1}]
    * n_jobs: 是否使用多核进行模型构建（用于fit、predict、dicision_path、apply等）
        * **None**: 不做设置（默认设置）
        * int: 使用设置核心数
        * -1: 使用所有核
    * max_depth: 剪枝相关。设置创建子结点的最大深度，在数据量大、特征数目多时可加以限制以改善过拟合问题。可先设置较小的深度进行拟合，观察拟合情况后，再决定是否要加大深度
        * **None**: 默认设置
    * min_samples_split: 剪枝相关。当结点的样本数少于该设置时，将不再进一步创建子结点，数据量大时可适当增大该值的设置
        * **2**: 默认设置
    * min_samples_leaf: 剪枝相关。限制叶结点的最少样本数，若某叶结点数目小于该设置时，则会和兄弟结点一起被剪枝，数据量大时可不考虑此值
        * **1**: 默认设置
    * min_weight_fraction_leaf: 剪枝相关。限制叶结点所有样本权重和的最小值，若小于该值，则会和兄弟结点一起被剪枝，如果样本缺失值较多或样本的分布类别偏差很大，则需引入该参数
        * **0**: 默认设置，即不考虑权重问题
    * max_leaf_nodes: 剪枝相关。限制最大叶结点数，若加入限制，则算法会在最大叶结点数内构建最优的决策树，如果特征不多可不考虑该值，若特征较多可加以限制（通过交叉验证判断设置值）
        * **None**: 默认设置
    * min_impurity_decrease: 剪枝相关。通过限制不纯度（基尼系数、信息增益等）限制决策树的增长。若结点的不纯度小于该阈值则不再创建子结点
        * **0.0**: 默认设置
    * verbose: 控制在训练过程中返回信息的详细程度，设置太大会影响运行速度
        * **0**: 训练时不返回任何信息（默认设置）
        * 1: 训练时返回部分信息
        * 2: 训练时返回寻览步骤都详细信息
    * warm_start: 在训练模型时是否在前一个阶段训练结果的基础上继续训练（增加基估计器数量），在进行训练时勾选此项可有效减少时间
        * **False**: 训练新的模型（默认设置）
        * True: 在前一阶段的训练结果上继续训练
    * ccp_alpha: 用于最小成本修剪度修剪的复杂度参数，默认不执行修剪
    * random_state: 随机数种子，用于对**随机样本采样**和**寻找最佳切分点**时的随机设置
        * **None**: 随机设置
* Attributes
	* base_estimator_
	* estimators_
	* **feature_importances_**
	* classes_
	* n_classes_
	* n_features_
	* n_features_in_
	* feature_names_in_
	* n_outputs_
	* oob_score_
	* oob_decision_function_
* Methods
	* apply
	* decision_path
	* fit
	* get_params
	* predict
	* predict_log_proba
	* predict_proba
	* score
	* set_params

```python
from sklearn.ensemble import RandomForestClassifier as RFC

rfc = RFC(n_estimators=200)  # 此处可自行设置相关参数

rfc.fit(x_train, y_train)

rfc.feature_importances_  # 输出特征的相对重要性

rfc.predict(x_test)  # 预测样本类别
rfc.predict_prob(x_test)  # 预测每个类别的概率
```
## 3.梯度提升分类器
### 3.1 基本概念
梯度提升分类器是对于任意**可微损失函数**的提升算法的泛化

优点
* 对混合型数据的自然处理（异构特征）
* 强大的预测能力
* 在输出空间中对异常数据的稳健性（通过具有文件下的损失函数实现）
* 支持自定义的损失函数

缺点
* 针对更大规模数据集、复杂度更高的模型的可扩展性较差
* 由于算法的**有序性**（下一步依赖于上一步的结果），因此并行性较差
### 3.3 梯度提升分类器实现
基于sklearn实现
link: https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting
link: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier

GradientBoostingClasifier
* Parameters
    * loss: 算法所使用的损失函数类型
        * **"deviance"**: 对数似然损失（默认设置），推荐使用此类算法
        * "exponential" : 指数损失函数，当应用此时，相当于AdaBoost模型
    * learning_rate: 每个基学习器的权重缩减系数，也称步长。越小的系数说明需要更多的基学习器迭代，该参数通常和n_estimators一起调参，二者之间存在权衡
        * **0.1**: 默认设置
    * n_estimators: 构建随机森林时的基估计器（即基础决策树）数量
        * **100**: 默认设置
    * subsample: 利用决策树基模型进行boosting的数据集采样比例，默认在全体数据集上进行。此处的采样与随机森林不同，此处为不放回采样，因此默认值代表所有数据用于boosting。使用小于1的值可获得更小的方差，但会得到更大的偏差
        * **1.0**: 使用全体数据集进行boosting（默认设置）
    * criterion: 用于评估切分点质量的方法设置
        * **"friedman_mse"**: fridman最小平方误差，为最小平方误差的近似（默认设置）
        * "squared_error": 平方误
        * "mse": 最小平方误差
        * "mae": 平均绝对值误差
    * init: 用于计算初始预测的估计器对象，一般在对数据有先验知识，或事先进行过拟合时才使用该参数
        * **None**: 不进行设置（默认设置），使用 DummyEstimator 预测类先验
        * estimator: 估计器对象
        * "zero": 初始原始预测设置为0
    * max_features: 在寻找最佳切分点时，考虑的特征数目
        * None: 不做设置，max_features=n_features
        * int: 设置考虑的特征数目，max_features=max_features
        * float: 设置考虑的特征数目，int(max_features * n_features)
        * **"auto"**: 随机考虑特征数（默认设置），考虑的特征数为max_features=sqrt(n_features)
        * "sqrt": 随机考虑特征数，考虑的特征数为max_features=sqrt(n_features)
        * "log2": 随机考虑特征数，考虑的特征数为max_features=log2(n_features)
    * max_depth: 剪枝相关。设置创建子结点的最大深度
        * **3**: 默认设置
    * min_sample_split: 剪枝相关。当结点的样本数少于该设置时，将不再进一步创建子结点，数据量大时可适当增大该值的设置
        * **2**: 默认设置
    * min_sample_leaf: 剪枝相关。限制叶结点的最少样本数，若某叶结点数目小于该设置时，则会和兄弟结点一起被剪枝，数据量大时可不考虑此值
        * **1**: 默认设置
    * min_weight_fraction_leaf: 剪枝相关。限制叶结点所有样本权重和的最小值，若小于该值，则会和兄弟结点一起被剪枝，如果样本缺失值较多或样本的分布类别偏差很大，则需引入该参数
        * **0**: 默认设置，即不考虑权重问题
    * max_leaf_nodes: 剪枝相关。限制最大叶结点数，若加入限制，则算法会在最大叶结点数内构建最优的决策树，如果特征不多可不考虑该值，若特征较多可加以限制（通过交叉验证判断设置值）
        * **None**: 默认设置
    * min_impurity_decrease: 剪枝相关。通过限制不纯度限制决策树的增长。若结点的不纯度小于该阈值则不再创建子结点
        * **0.0**: 默认设置
    * verbose: 控制在训练过程中返回信息的详细程度，设置太大会影响运行速度
        * **0**: 训练时不返回任何信息（默认设置）
        * 1: 训练时返回部分信息
        * 2: 训练时返回寻览步骤都详细信息
    * warm_start: 在训练模型时是否在前一个阶段训练结果的基础上继续训练（增加基估计器数量），在进行训练时勾选此项可有效减少时间
        * **False**: 训练新的模型（默认设置）
        * True: 在前一阶段的训练结果上继续训练
    * ccp_alpha: 用于最小成本修剪度修剪的复杂度参数，默认不执行修剪
    * n_iter_no_change: 决定在**验证分数没有提升**时是否**提前终止训练**
        * **None**: 不进行提前终止
        * int: 基于validation_fraction留出的验证机验证结果，在之前的所有 n_iter_no_change 迭代次数中验证分数没有提高时终止训练
    * validation_fraction: 提取训练集数据设置为验证集的比例，范围为(0,1])
        * **0.1**: 默认设置
    * tol: 提前终止训练的loss阈值设置。当n_iter_no_change迭代时loss值未改善至设置值时，将终止训练
        * **1e-4**: 默认设置
    * random_state: 随机数种子，用于对**每次迭代时的基估计器**和**切分时特征排列**的随机设置，若n_iter_no_change 不是 None ，它还控制训练数据的随机分裂以获得验证集
        * **None**: 不进行设置（默认设置）
* Attributes
	* n_estimators_
	* **feature_importances_**
	* oob_improvement_
	* train_score_
	* loss_
	* init_
	* estimators_
	* classes_
	* n_features_
	* n_features_in_
	* feature_names_in_
	* n_classes_
	* max_features_
* Methods
	* apply
	* decision_function
	* fit
	* get_params
	* predict
	* predict_log_proba
	* predict_proba
	* score
	* set_params
	* staged_decision_function
	* staged_predict
	* staged_predict_proba

```python
from sklearn.ensemble import GradientBoostingClassifier as GBC

gbc = GBC(n_extimators=200)  # 此处可自行设置相关参数

gbc.fit(x_train, y_train)

gbc.feature_importances_  # 输出特征的相对重要性

gbc.predict(x_test)  # 预测样本类别
gbc.predict_prob(x_test)  # 预测每个类别的概率
```