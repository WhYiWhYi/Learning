## 1 基本概念

最近邻模型是一个通用算法类，其目的是依据训练数据集中最近邻数量来决定对策。需要注意的是，“基于邻居的”（Neighbors-based）方法被称为**非泛化**机器学习方法，因为它们只是简单地记住了所有的训练程序

k近邻算法（k-Nearest Neighbor, k-NN）采用测量不同特征值之间的距离方法进行分类，其是一种监督学习的分类器算法
* 工作原理：将未知标签的数据的每个特征与已有标签数据的特征进行比较，由算法提取样本数据集中**前k个（k一般小于20）**最相似数据（**最相似数据=经过距离计算后得到的结果最小**）的分类标签，在这k个数据中**出现次数最多**的分类即作为新数据的分类
* 常用距离计算公式
	* 欧式距离公式：√(Σ(ai-bi)²)
	* 曼哈顿距离公式：Σ|ai-bi|
* 优点：算法简单易于实现、精度高、对异常值不敏感、无数据输入假定
* 缺点：计算复杂度高、空间复杂度高、**结果受k值设定影响**（较大的k可能抑制噪声的影响从而使分类不明确）、**结果受训练样本集不平衡影响**（如果某个分类数量极大，则前k个很可能就会接近该分类而不是目标分类）
* 适用数据范围：数值型、标称型

## 2.k-NN实现
自我实现
```python
import numpy as np
import operator

# 数据的特征和目标变量要分别存放
group = np.array([[1, 1.1], [1, 1], [0, 0], [1, 0.1]])  # 训练数据的特征，储存在array中
lable = ['A', 'A', 'B', 'B']  # 训练数据的目标变量，储存在列表中

def classify(inX, dataSet, labels, k):
	"""
	inX：待分类的测试集数据
	dataSet：储存训练数据特征的array
	labels：训练数据的分类list
	k：设置的k值
	"""
	# shape获取的是一个元组，第一项为array中元素的项数（行数），第二项为每个元素中元素的项数（列数）
	dataSetR = dataSet.shape[0]  # 获取训练数据集的行数、列数
	# np.tile可将输入的列表（para1）复制扩展为指定的形状（para2，元组中为行数、1）
	# 输入列表列数已经与dataSet相同，无需更改
	# 在np的array中，两个形状相同（行、列数相同）的array可以直接相减（相加也可），结果为对应位置的相减值
	diffMat = np.tile(inX, (dataSetR, 1)) - dataSet
	
	# 欧式距离计算
	# 同理，对array进行简单数值的乘除法时，也是对array中每一个元素执行
	sqdiffMat = diffMat ** 2  # 对array中每个元素求平方值
	# 对二维array元素求和时，axis=0代表对每一列分别求和，axis=1代表对每一行分别求和
	# 补充：axis越大，计算跨越的维度越低（=1时在子列表中求和，=0时跨一维列表求同位置元素的和）
	# 每次求和都会将原始array降维一层
	sqDistances = sqdiffMat.sum(axis=1)  # 对array中每一行求和
	oDistances = sqDistances ** 0.5
	
	# 对计算后的元素进行排序
	sortedOIndex = oDistances.argsort()  # argsort：排序后返回元素排序前的index，默认由小到大
	
	# 获取距离最近的前k个元素：label-前k次中出现的频次
	classCount = {}
	for i in range(k):
		# 此处为按sort顺序依次提取对应元素的label
		# 由于原始label顺序与array中元素顺序一致，可据此获取排序后各元素的label
		sortedLabel = labels[sortedOIndex[i]]
		# 字典中的特殊计数方法，不用使用Counter方法
		# Dict.get(key, default)：如果Dict中有key则返回key，如果没有则返回default
		# default默认为None值，但可自行指定
		# 新值：此处对没记录过的key赋值为0后+1，然后将新值作为新key储存到字典中
		# 已存在的值：获取key的计数后，+1，将新值赋值给原key
		classCount[sortedLabel] = classCount.get(sortedLabel,0)+1  # 对前k个label计数
	
	# 对距离最近的前k个元素进行排序：依据出现频次由大到小（reverse=True），返回值为排序后的字典
	# key=operator.itemgetter()：取出索引位置处的数据，作为排序的依据
	sortedclassCount = sorted(classCount.items(), key=operator.itemgetter(1),reverse=True)
	return sortedclassCount[0][0]
```
sklearn实现
link: https://scikit-learn.org/stable/modules/neighbors.html
link: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier

KNeighborsClassifier
* Parameters
    * n_neighbors: 设置k值
        * **5**: 默认设置
    * weights: 用于预测时设置“邻居”的权重
        * **"uniform"**: 所有“邻居”的权重相同（默认设置）
        * "distance": “邻居”越近，权重越重
        * callable variable: 用户自定义的权重，接受距离的数组后，返回相同形状的权重数组
    * algorithm: 计算最近邻使用的算法
        * **"auto"**: 自动决定最合适的算法（默认设置）
        * "ball_tree": 基于Ball树结构的算法，其在高维下性能较kd_tree较好，但其实际性能高度依赖于训练数据的结构
        * "kd_tree": 基于KD树结构的算法，复杂度为OPDNlog(N)。在低维（<20）时效率很高，但D增长到很大时效率变低
        * "brute": 暴力算法，对于D维度N个样本，复杂度为O[DN^2^]
    * leaf_size: 在**算法选择“ball_tree”或“kd_tree”时**，影响树构建和查询的速度、储存树所需的内存
        * **30**: 默认设置
    * metric: 用于**树**的距离度量
        * **"minkowski"**: 默认设置
    * p: Minkowski度量的参数
        * **2**: 欧几里得距离
        * 1: 曼哈顿距离
        * 任意值: 闵可夫斯基距离
    * metric_params: metric参数的额外关键参数
        * **None**: 默认设置
        * dict
    * n_jobs: 是否使用多核进行模型构建（用于fit、predict、dicision_path、apply等）
        * **None**: 不做设置（默认设置）
        * int: 使用设置核心数
        * * -1: 使用所有核
* Attributes
	* classes_
	* effective_metric_
	* effective_metric_params_
	* n_features_in_
	* feature_names_in_
	* n_samples_fit_
	* outputs_2d_
* Methods
	* fit
	* get_params
	* kneighbors
	* kneighbors_graph
	* pedict
	* predict_proba
	* score
	* set_params

```python
import numpy as np
from sklearn.neighbors import KNeighborsClassifier as KNC

knc = KNC(n_neighbors=20, weights="distance")  # 此处可自行设置相关参数
knc.fit(x_train, y_train)
y_predict = knc.predict(x_test)
```