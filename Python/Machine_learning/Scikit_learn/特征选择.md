## 1 基于相关系数计算的特征选择

### 1.1 常见相关系数
Pearson correlation
* 用于衡量两个变量X、Y之间的相关性（**线性相关**），取值范围为**[-1, 1]**，正负号代表正负相关，0代表不相关，1代表完全相关
* Pearson相关系数要求两个变量呈**联合双变量正态分布**，且需**方差齐**（需检验方差齐性）
* Pearson相关系数只适用于**线性相关**的**连续数据**，对于曲线或更复杂的场景，其大小不能代表相关性强弱
* 极端值/异常值对Pearson相关系数的影响极大，需进行筛选后剔除
```python
from scipy.stats import pearsonr

pearson = pearsonr(x, y)[0]
```

Spearman correlation
* Spearman相关系数又称Spearman秩相关系数，”秩“可理解为排序或顺序。在计算时，首先对X、Y变量的数据进行排序，记下排序的位置（X', Y'），该（X', Y'）即为”秩次”，Spearman相关系数计算即基于此，取值范围为**[-1, 1]**，正负号代表正负相关，0代表不相关，1代表完全相关
* Spearman相关系数是一种**非参数统计方法**，其**对原始变量的分布无要求**，数据可以是**连续数据**，也可以是**分类数据**。相比Pearson相关系数适用范围更广，但统计效能会稍低
* 样本容量最好能够大
* 对极端值/异常值的容忍度较高
```python
from scipy.stats import spearmanr

spearman = spearmanr(x, y)[0]
```

Kendall correlation
* Kendall相关系数也是一种秩相关系数，取值范围为**[-1, 1]**，正负号代表正负相关，0代表不相关，1代表完全相关
* Kendall相关系数的数据应用范围与Spearman相关系数一致，其多用于**分类数据**
```python
from scipy.stats import kendalltau

kendall = kendalltau(x, y)[0]
```
## 2.基于sklearn的特征选择
```sklearn.feature_selection``` 模块中的类可以对样本集进行**特征选择**或**降维**，可提高估计器的准确度、增强它们在高维数据集上的性能
## 2.1 移除低方差特征
```VarianceThreshold``` 是一种基本的特征选择方法，其通过**计算特定特征在数据集中所有样本中的方差**进行特征选择。当某一特征在所有样本上的取值使方差较小时，我们可认为该特征在所有的样本上表现出的差异不大，当方差**不满足设置的阈值时，即将该特征移除**

* Parameters
	* threshold: 训练集中移除特征的方差阈值，低于该值即被分解
		* **0**: 移除所有零方差特征，即在所有样本中保持不变的的特征（默认设置）
* Attributes
	* variances\_
	* n_features_in_
	* feature_names_in_（当输入值为pandas中的DataFrame时可使用，后同）
		```python
		import pandas as pd
		from sklearn.feature_selection import VarianceThreshold as VT
		
		x = pd.DataFrame(data,
						 columns = [f'V{i}' for i in range(10)])  # 特征名称
		
		select_vt = VT(threshold=0.2)
		x_selection = select_vt.fit(x)
		x_selection.feature_names_in_
		>>> ['V0', 'V1', 'V2, ...]
		```
* Methods
	* fit: 使用fit时输出的是VarianceThreshold对象
	* fit_transform: 使用fit_transform时输出的是经过筛选后的特征结果
	* get_feature_names_out
	* get_params
	* get_support
	* inverse_transform
	* set_params
	* transform

```python
# 移除数据集中特征为0或1的比例超过80%的特征
from sklearn.feature_selection import VarianceThreshold as VT

x = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
selection_vt = VT(thresthold=0.2)
x_selection = selection_vt.fit_transform(x)  # 仅使用特征
```
## 2.2 单变量特征选择
单变量特征选择是通过基于单变量的统计测试来选择最好的特征
```SelectKBest```：**按指定K值保留评分最高的特征**，移除其他所有特征
* Parameters
	* score_func: 得分函数，返回单特征的得分和p值
		* **f_classif**: 分类估计器（默认设置）
		* chi2: 分类估计器
		* mutual_info_classif: 分类估计器。PS.Mutual information：互信息，主用于评价**类别变量之间的相关性**。互信息能计算任何种类的统计相关性，但其作为非参数方法，需要更多样本。此外，互信息用于特征选择实际不太方便，理由有二：它不属于度量方式，也没法归一化，在不同数据集上的结果无法比较；对于连续变量的计算不方便，其需要将变量离散化，而互信息的结果对离散化的方式很敏感。最大信息系数（maximal inforation coefficient, MIC）能在一定程度上克服这一缺陷，其会先选择一种最优的离散化方式，再将互信息取值转化为一种取值为[0,1]的度量方式
		* f_regression: 回归估计器
		* mutual_info_regression: 回归估计器
	* k: 最终保留的特征数目
		* **10**: 默认设置
		* "all": 不进行特征选择，只计算得分
* Attributes
	* scores_
	* pvalues_
	* n_features_in_
	* feature_names_in_
* Methods
	* fit
	* fit_transform
	* get_feature_names_out
	* get_params
	* get_support
	* inverse_transform
	* set_params
	* transform

```SelectPercentile```：**按指定百分比保留评分最高的特征**，移除其他所有特征
* Parameters
	* score_func: 得分函数，返回单特征的得分和p值
		* **f_classif**: 分类估计器（默认设置）
		* chi2: 分类估计器
		* mutual_info_classif: 分类估计器
		* f_regression: 回归估计器
		* mutual_info_regression: 回归估计器
	* percentile: 最终保留的特征数目百分比
		* **10**: 默认设置
* Attributes
	* scores_
	* pvalues_
	* n_features_in_
	* feature_names_in_
* Methods
	* fit
	* fit_transform
	* get_feature_names_out
	* get_params
	* get_support
	* inverse_transform
	* set_params
	* transform
```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

x_selection = SelectKBest(chi2, k=2).fit_transform(x, y)  # 设置选择特征数为2；使用了特征和标签
```
```GenericUnivariateSelect```：允许使用可配置方法来进行单变量特征选择。它允许超参数搜索评估器来选择最好的单变量特征

* Parameters
	* score_func: 得分函数
		* **f_classif**: 分类估计器（默认设置）
		* chi2: 分类估计器
		* mutual_info_classif: 分类估计器
		* f_regression: 回归估计器
		* mutual_info_regression: 回归估计器
	* mode: 特征选择模式
		* **"percentile"**: 基于百分比的方式保留特征（默认设置）
		* "k_best": 基于个数的方式保留特征
		* "fpr": 基于假阳性率选择特征
		* "fdr": 基于估计错误发现率（false discovery rate）选择特征
		* "fwe": 基于family-wise error rate选择特征
	* param: 与mode相关的参数设置，基于mode进行调整
		* **10^-5^**: 默认设置
* Attributes
	* scores_
	* pvalues_
	* n_features_in_
	* feaure_names_in_
* Methods
	* fit
	* fit_transform
	* get_feature_names_out
	* get_params
	* get_support
	* inverse_transform
	* set_params
	* transform
## 2.3 递归式特征消除（需coef\_或feature_importances_属性）

先设置一个基估计器，对其进行第一轮训练，针对每个特征有一个权重返回值 **coef\_**或**feature\_importances\_**，该算法基于返回的参数，消除权重较低的特征，再基于保留的特征进行下一轮训练，以递归的方式来考虑最小最优的特征集合
```recursive feature elimination (RFE)```：通过考虑越来越小的特征集合来递归选择特征
* Parameters
	* estimator: 包含fit方法、能提供特征重要性信息，如coef\_、feature_importances_的监督学习估计器
	* n_features_to_select: 要选择的特征数量
		* **None**: 保留一半特征（默认设置）
		* int: 保留指定数量的特征
		* float: 保留指定比例数量的特征
	* step: 每次循环中移除特征数量
		* **1**: 每次循环移除1个特征（默认设置）
		* int: 每次循环移除的特征数目（需>1）
		* float: 每次循环移除的特征百分比（0< <1）
	* verbose: 控制输出信息的详细程度
		* **0**: 默认设置
	* importance_getter: RFE获取特征重要性信息的来源
		* **"auto"**: 使用coef_和feature_importances_的信息（默认设置）
		* str: 使用指定属性信息（coef_或feature_importances_）
		* callable
* Atrributes
	* classes_
	* estimator_
	* n_features_
	* n_features_in_
	* feature_names_in_
	* ranking_
	* support_
* Methods
	* decision_function
	* fit
	* fit_transform
	* get_feature_names_out
	* get_params
	* get_support
	* inverse_transform
	* predict
	* predict_log_proba
	* predict_proba
	* score
	* set_params
	* transform
```python
from sklearn.svm import SVC
from sklearn.feature_selection import RFE

svc = SVC(kernel="linear", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit_transform(x, y)
```

```recursive feature elimination with cross-validation(RFECV)```：在交叉验证的循环中执行RFE，基于RFE对特征重要性的判断选择最优的特征数量
* Parameters
	* estimator: 包含fit方法、能提供特征重要性信息，如coef\_、feature\_importances\_的监督学习估计器
	* min_features_to_select: 最少保留的特征数（选出的最优特征数必然大于该值）
		* **1**: 默认设置
	* step: 每次循环中移除特征数量
		* **1**: 每次循环移除1个特征（默认设置）
		* int: 每次循环移除的特征数目（需>1）
		* float: 每次循环移除的特征百分比（0< <1）
	* cv: 决定交叉验证所使用的分割策略。若输入值为None或整数，如果分类为二元或多分类，则会使用StratifiedKFold（分层的交叉验证），否则（非二元或多分类）将会使用KFold进行交叉验证
		* **None**: 使用默认5-fold交叉验证（默认设置）
		* int: 使用n-fold交叉验证
		* cross-validation generator or iterable
	* scoring: 用于模型评估的指标（详见https://scikit-learn.org/stable/modules/model_evaluation.html）
		* 分类器
			* "accuracy"（常规使用）
			* "balanced_accuracy"
			* "top_k_accuracy"
			* "average_precision"
			* "neg_brier_score"
			* "f1" etc: average: "bainary", "micro", "macro", "weighted", "samples"
			* "neg_log_loss"
			* "precision" etc: average: "bainary", "micro", "macro", "weighted", "samples"
			* "recall" etc: average: "bainary", "micro", "macro", "weighted", "samples"
			* "jaccard" etc: average: "bainary", "micro", "macro", "weighted", "samples"
			* "roc_auc" etc: average: "bainary", "micro", "macro", "weighted", "samples"; multi_class: "ovr", "ovo"
		* 回归器
			* "explained_variance"
			* "max_error"
			* "neg_mean_absolute_error"
			* "neg_mean_squared_error"
			* "neg_root_mean_squared_error"
			* "neg_mean_squared_log_error"
			* "neg_median_absolute_error"
			* "r2"
			* "neg_mean_poisson_deviance"
			* "neg_mean_gamma_deviance"
			* "neg_mean_absolute_percentage_error" 
	* verbose: 控制输出信息的详细程度
		* **0**: 默认设置
	* n_jobs: 使用的核心数
		* **None**: 默认设置（具体含义没搞明白）
		* int: 使用指定数量的核心
		* -1: 使用全部核心
	* importance_getter: RFE获取特征重要性信息的来源
		* **"auto"**: 使用coef\_和feature_importances_的信息（默认设置）
		* str: 使用指定属性信息（coef\_或feature_importances_）
		* callable
* Attributes
	* classes_
	* estimator_
	* grid_scores_
	* cv_results_
	* n_features_
	* n_features_in_
	* feature_names_in_
	* ranking_: 每个特征的重要性排序，输出列表的顺序对应特征顺序。1代表被选中的特征，其余按重要性依次排序
	* support_
* Methods
	* decision_function
	* fit
	* fit_transform
	* get_feature_names_out
	* get_params
	* get_support
	* inverse_transform
	* predict
	* predct_log_proba
	* predict_proba
	* score
	* set_params
	* transform
```pyton
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV

svc = SVC(kernal="linear")
rfecv = RFECV(estimator=svc,
			  step=1,
			  cv=StratifiedKFold(2),
			  scoring="accuracy",
			  min_features_to_select=2)
rfecv.fit(x, y)
rfecv.n_features_
>>> feature name
```
## 2.4 使用SelectFromModel选取特征
```SelectFromModel```：一个元转换器，可依据设置的阈值处理能提供特征重要性信息，如coef\_、feature\_importances\_的监督学习估计器。如果coef\_、feature\_importances\_低于预先设置的阈值，将被认为不重要并移除。除了指定数字设定阈值外，还可通过给定的字符串参数来使用内置的启发式方法找到合适的阈值，内置的启发式方法包括：mean、median等及其与浮点数的乘积
* 看起来其与RFE类似，均为依据coef\_或feature\_importances\_进行特征选择，二者的区别在于，**RFE是基于迭代进行特征选择，每次迭代都从最不重要的特征开始删除，而SelectFromModel则是依据一个参数直接进行特征删除，不涉及迭代，其稳健性较RFE较差**
* Parameters
	* estimator: 包含fit方法、能提供特征重要性信息，如coef\_、feature\_importances\_的监督学习估计器
	* threshold: 用于特征选择的阈值设置
		* **None**: 若估计器将参数惩罚设置为l1范数，阈值将为10^-5^，否则将默认使用"mean"（默认设置）
		* int: 大于等于该设置值的特征将保留
		* "mean": 特征重要性大于等于特征重要性均值的特征将被保留（与浮点数的乘积同理）
		* "median": 特征重要性大于等于特征重要性中位数的特征将被保留（与浮点数的乘积同理）
	* prefit: bool值，决定此前的训练模型是否直接传递给构造函数
		* **False**: 使用fit训练模型，并在转换后进行特征选择
		* True: 必须直接调用转换。且SelectFromModel不能与cross_val_score、GridSearchCV等一同使用
			```python
			from sklearn.svm import LinearSVC
			from sklearn.feature_selection import SelectFromModel
			
			lsvc = LinearSVC().fit(x, y)  # 估计器需要进行拟合
			model = SelectFromModel(lsvc, prefit=True)  # 将预先拟合的训练模型传递给构造函数
			x_new = model.transform(x)  # 直接调用transform进行特征选择
			```
	* norm_order: 在估计器coef\_维度为2时，用于过滤低于阈值的系数向量的范数的顺序
		* **1**: 默认设置
		* non-zero int、inf、-inf
	* max_features: 保留的特征最大数目
		* **None**: 默认设置
		* int: 按设置值保留特征的最大数目
	* importance_getter: RFE获取特征重要性信息的来源
		* **"auto"**: 使用coef_和feature_importances_的信息
		* str: 使用指定属性信息（coef_或feature_importances_）
		* callable
* Attributes
	* estimator_
	* n_features_in_
	* feature_names_in_
	* threshold_
* Methods
	* fit
	* fit_transform
	* get_feature_names_out
	* get_params
	* get_support
	* inverse_transform
	* partial_fit
	* set_params
	* transform





